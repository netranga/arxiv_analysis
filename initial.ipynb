{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new virtual environment, you can use the following command in the terminal:\n",
    "\n",
    "For Python 3 (recommended):\n",
    "python3 -m venv myenv\n",
    "\n",
    "Or, if 'python3' doesn't work, try:\n",
    "python -m venv myenv\n",
    "\n",
    "Replace 'myenv' with your desired environment name\n",
    "\n",
    "After creating the environment, activate it with:\n",
    "On Windows:\n",
    "myenv\\Scripts\\activate\n",
    "On macOS and Linux:\n",
    "source myenv/bin/activate\n",
    "\n",
    "To deactivate the environment when you're done:\n",
    "deactivate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import dspy\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "from dotenv import load_dotenv\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('/Users/netraranga/Desktop/Projects/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed content length: 42282 words\n",
      "First 500 characters:\n",
      "1 a survey on knowledge distillation of large language models xiaohan xu1, ming li2, chongyang tao3, tao shen4, reynold cheng1, jinyang li1, can xu5, dacheng tao6, tianyi zhou2 1the university of hong kong2university of maryland3microsoft 4university of technology sydney5peking university6the university of sydney {shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk jl0725@connect.hku.hk abstract —in the era of large language models (llms), knowledge distillation (kd) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary llms, such as gpt -4, to their open-source counterparts like llama and mistral. additionally, as open-source llms flourish, kd plays a crucial role in both compressing these models, and facilitating their self- improvement by employing themselves as teachers. this paper presents a comprehensive survey of kd’s role within the realm of llm, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self- improvement. our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization – providing a comprehensive examination of kd mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. crucially, the survey navigates the intricate interplay between data augmentation (da) and kd, illustrating how da emerges as a powerfu\n"
     ]
    }
   ],
   "source": [
    "def process_arxiv_paper(pdf_path):\n",
    "    # Load the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Combine all text into one string\n",
    "    full_text = ' '.join([page.page_content for page in pages])\n",
    "    \n",
    "    # Lowercase the text\n",
    "    full_text = full_text.lower()\n",
    "    \n",
    "    # Find the index of the reference marker\n",
    "    ref_marker = 'references\\n[1'\n",
    "    ref_index = full_text.find(ref_marker)\n",
    "    \n",
    "    if ref_index != -1:\n",
    "        # Remove all text after the reference marker\n",
    "        full_text = full_text[:ref_index]\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    full_text = ' '.join(full_text.split())\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "# Example usage\n",
    "#arxiv_url = \"https://arxiv.org/pdf/2402.06196\"\n",
    "#arxiv_url = 'https://arxiv.org/pdf/2302.07459'\n",
    "#arxiv_url = \"https://arxiv.org/pdf/2204.05862\"\n",
    "#arxiv_url = \"https://arxiv.org/pdf/2303.08774\"\n",
    "arxiv_url = 'https://arxiv.org/pdf/2402.13116'\n",
    "processed_content = process_arxiv_paper(arxiv_url)\n",
    "print(f\"Processed content length: {len(processed_content.split())} words\")\n",
    "print(\"First 500 characters:\")\n",
    "print(processed_content[:1500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_oai = dspy.AzureOpenAI(api_base='https://fsodnaopenai2.openai.azure.com/', api_version='2023-05-15',model='gpt-4o', max_tokens=4000)\n",
    "dspy.configure(lm=azure_oai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SectionHeaderExtraction(dspy.Signature):\n",
    "    \"\"\"Extract section headers from a research paper\"\"\"\n",
    "\n",
    "    input_text = dspy.InputField(desc=\"The full text of a research paper\")\n",
    "    section_headers = dspy.OutputField(desc=\"A list of only main section headers found in the paper, do not include subheaders. Main headers typical lead with an integer and do not contains decimals. Return the first sentence after each main section header in the following format: Number. header: first sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "produce the section_headers. We will start by identifying the main section headers in the text. These headers typically lead with an integer and do not contain decimals. We will then extract the first sentence following each main section header.\n",
      "Section Headers Output:\n",
      "1. introduction: in the evolving landscape of artificial intelligence (ai), proprietary large language models (llms) such as gpt-3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude have emerged as groundbreaking technologies, reshaping our understanding of natural language processing (nlp).\n",
      "2. overview: the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021).\n",
      "3. knowledge distillation algorithms: this section navigates through the process of knowledge distillation.\n",
      "4. skill distillation: building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms.\n",
      "5. domain-specified vertical distillation: this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc.\n",
      "6. open problems: further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions.\n",
      "7. conclusion and discussion: this survey has traversed the expansive domain of knowledge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field.\n"
     ]
    }
   ],
   "source": [
    "class SectionHeaderExtractor(dspy.Module):\n",
    "    'Custom module, need to initialize with a prompting method  '\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.extractor = dspy.ChainOfThought(SectionHeaderExtraction)\n",
    "\n",
    "    def forward(self, input_text):\n",
    "        'Parameter is input field from Signature'\n",
    "        result = self.extractor(input_text=input_text)\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "extractor = SectionHeaderExtractor()\n",
    "section_results = extractor(input_text=processed_content)\n",
    "\n",
    "print(section_results.rationale)\n",
    "print(\"Section Headers Output:\")\n",
    "print(section_results.section_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('1. introduction', 'in the evolving landscape of artificial intelligence (ai), proprietary large language models (llms) such as gpt-3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude have emerged as groundbreaking technologies, reshaping our understanding of natural language processing (nlp).'), ('2. overview', 'the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021).'), ('3. knowledge distillation algorithms', 'this section navigates through the process of knowledge distillation.'), ('4. skill distillation', 'building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms.'), ('5. domain-specified vertical distillation', 'this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc.'), ('6. open problems', 'further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions.'), ('7. conclusion and discussion', 'this survey has traversed the expansive domain of knowledge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field.')])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def parse_section_headers(section_results):\n",
    "    section_headers = {}\n",
    "    for header in section_results.section_headers.split('\\n'):\n",
    "        if ':' in header:\n",
    "            key, value = header.split(':', 1)\n",
    "            section_headers[key.strip().lower()] = value.strip().lower()\n",
    "    return section_headers\n",
    "\n",
    "section_headers = parse_section_headers(section_results)\n",
    "\n",
    "section_headers.items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1. introduction': 'in the evolving landscape of artificial intelligence (ai), proprietary large language models (llms) such as gpt-3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude have emerged as groundbreaking technologies, reshaping our understanding of natural language processing (nlp).',\n",
       " '2. overview': 'the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021).',\n",
       " '3. knowledge distillation algorithms': 'this section navigates through the process of knowledge distillation.',\n",
       " '4. skill distillation': 'building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms.',\n",
       " '5. domain-specified vertical distillation': 'this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc.',\n",
       " '6. open problems': 'further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions.',\n",
       " '7. conclusion and discussion': 'this survey has traversed the expansive domain of knowledge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field.'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_chunks(output_text, section_headers):\n",
    "    # Convert the text to lowercase for case-insensitive matching\n",
    "    output_text_lower = output_text.lower()\n",
    "    \n",
    "    chunks = {}\n",
    "    \n",
    "    for header, content in section_headers.items():\n",
    "        current_header = content.lower()\n",
    "        \n",
    "        # Try to find the full first sentence\n",
    "        start_index = output_text_lower.find(current_header)\n",
    "        \n",
    "        # If full sentence not found, try to find 6 consecutive words\n",
    "        if start_index == -1:\n",
    "            words = current_header.split()\n",
    "            for j in range(len(words) - 5):\n",
    "                six_word_phrase = ' '.join(words[j:j+6])\n",
    "                start_index = output_text_lower.find(six_word_phrase)\n",
    "                if start_index != -1:\n",
    "                    break\n",
    "        \n",
    "        if start_index == -1:\n",
    "            print(f\"Warning: Couldn't find the content for header: {header}\")\n",
    "            continue\n",
    "        \n",
    "        # Find the start of the next section\n",
    "        next_header = None\n",
    "        for next_h, next_c in section_headers.items():\n",
    "            if next_h > header:\n",
    "                next_header = next_c.lower()\n",
    "                break\n",
    "        \n",
    "        if next_header:\n",
    "            end_index = output_text_lower.find(next_header)\n",
    "            \n",
    "            # If full next header not found, try to find 6 consecutive words\n",
    "            if end_index == -1:\n",
    "                words = next_header.split()\n",
    "                for j in range(len(words) - 5):\n",
    "                    six_word_phrase = ' '.join(words[j:j+6])\n",
    "                    end_index = output_text_lower.find(six_word_phrase)\n",
    "                    if end_index != -1:\n",
    "                        break\n",
    "        else:\n",
    "            end_index = len(output_text)\n",
    "        \n",
    "        # Extract the chunk\n",
    "        if end_index != -1:\n",
    "            chunk = output_text[start_index:end_index].strip()\n",
    "            chunks[header] = chunk\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1. introduction': 'in the evolving landscape of artificial intelligence (ai), proprietary1large language models (llms) such as gpt- 3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude2have emerged as groundbreaking technologies, reshaping our understand- ing of natural language processing (nlp). these models, characterized by their vast scale and complexity, have un- locked new realms of possibility, from generating human- like text to offering sophisticated problem-solving capa- bilities. the core significance of these llms lies in their emergent abilities (wei et al., 2022a,b; xu et al., 2024a), a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. their deep understanding of context, nuance, and the intrica- cies of human language enables them to excel in a wide array of applications, from creative content generation to 1. for simplicity, we use ‘proprietary’ to represent both versatile yet close-source llms like gpt-4 and open-source yet huge llms like llama-2-70b, which encapsulate rich knowledge with a large number of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond current applications, promising to revolutionize industries, augment human creativity, and redefine our interaction with technology. despite the remarkable capabilities of proprietary llms like gpt-4 and gemini, they are not without their shortcom- ings, particularly when viewed in light of the advantages offered by open-source models. a significant drawback is their limited accessibility and higher cost (openai et al., 2023). these proprietary models often come with substantial usage fees and restricted access, making them less attain- able for individuals and smaller organizations. in terms of data privacy and security (wu et al., 2023a), using these proprietary llms frequently entails sending sensitive data to external servers, which raises concerns about data pri- vacy and security. this aspect is especially critical for users handling confidential information. moreover, the general- purpose design of proprietary llms, while powerful, may not always align with the specific needs of niche applica- tions. the constraints of accessibility, cost, and adaptability thus present significant challenges in leveraging the full potential of proprietary llms. in contrast to proprietary llms, open-source modelsarxiv:2402.13116v3 [cs.cl] 8 mar 2024 2 like llama (touvron et al., 2023) and mistral (jiang et al., 2023a) bring several notable advantages. one of the primary benefits of open-source models is their accessibility and adaptability. without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader range of users, from individual re- searchers to smaller organizations. this openness fosters a more collaborative and inclusive ai research environment, encouraging innovation and diverse applications. addition- ally, the customizable nature of open-source llms allows for more tailored solutions, addressing specific needs that generic, large-scale models may not meet. however, the open-source llms also have their own set of drawbacks, primarily stemming from their relatively limited scale and resources compared to their proprietary counterparts. one of the most significant limitations is the smaller model scale, which often results in lower per- formance on real-world tasks with a bunch of instruc- tions (zheng et al., 2023a). these models, with fewer pa- rameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like gpt-4. ad- ditionally, the pre-training investment in these open-source models is typically less substantial. this reduced investment can lead to a narrower range of pre-training data, poten- tially limiting the models’ understanding and handling of diverse or specialized topics (liang et al., 2022; sun et al., 2024a). moreover, open-source models often undergo fewer fine-tuning steps due to resource constraints. fine-tuning is crucial for optimizing a model’s performance for spe- cific tasks or industries, and the lack thereof can hinder the model’s effectiveness in specialized applications. this limitation becomes particularly evident when these models are compared to the highly fine-tuned proprietary llms, which are often tailored to excel in a wide array of complex scenarios (openai et al., 2023). primarily, recognizing the disparities between propri- etary and open-source llms, kd techniques have surged as a means to bridge the performance gap between these models (gou et al., 2021; gupta and agrawal, 2022). knowl- edge distillation, in this context, involves leveraging the more advanced capabilities of leading proprietary models like gpt-4 or gemini as a guiding framework to enhance the competencies of open-source llms. this process is akin to transferring the ‘knowledge’ of a highly skilled teacher to a student, wherein the student (e.g., open-source llm) learns to mimic the performance characteristics of the teacher (e.g., proprietary llm). compared to traditional knowledge distillation algorithms (gou et al., 2021), data augmentation (da) (feng et al., 2021) has emerged as a prevalent paradigm to achieve knowledge distillation of llms, where a small seed of knowledge is used to prompt the llm to generate more data with respect to a specific skill or domain (taori et al., 2023). secondly, kd still retains its fundamental role in compressing llms, making them more efficient without significant loss in performance. (gu et al., 2024; agarwal et al., 2024). more recently, the strategy of employing open-source llms as teachers for their own self-improvement has emerged as a promising approach, enhancing their capabilities significantly (yuan et al., 2024a; chen et al., 2024a). figure 1 provides an illustration of these three key roles played by kd in the context of llms. closed-sourcellmsopen-sourcellmssmallerlmsadvancecompressself-improvement directionofkd ①②③fig. 1: kd plays three key roles in llms: 1) primarily enhancing capabilities, 2) offering traditional compression for efficiency, and 3) an emerging trend of self-improvement via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- struction following (taori et al., 2023)), improved align- ment with user intents (e.g., human values/principles (cui et al., 2023a), and thinking patterns like chain-of-thought (cot) (mukherjee et al., 2023)), and nlp task specialization (e.g., semantic understanding (ding et al., 2023a), and code generation (chaudhary, 2023)). these skills are crucial for the wide array of applications that llms are expected to perform, ranging from casual conversations to com- plex problem-solving in specialized domains. for instance, in vertical domains like healthcare (wang et al., 2023a), law (law, 2023), or science (zhang et al., 2024), where accuracy and context-specific knowledge are paramount, knowledge distillation allows open-source models to sig- nificantly improve their performance by learning from the proprietary models that have been extensively trained and fine-tuned in these areas. the benefits of knowledge distillation in the era of llms are multifaceted and transformative (gu et al., 2024). through a suite of distillation techniques, the gap between proprietary and open-source models is significantly nar- rowed (chiang et al., 2023; xu et al., 2023a) and even filled (zhao et al., 2023a). this process not only streamlines computational requirements but also enhances the environ- mental sustainability of ai operations, as open-source mod- els become more proficient with lesser computational over- head. furthermore, knowledge distillation fosters a more accessible and equitable ai landscape, where smaller enti- ties and individual researchers gain access to state-of-the-art capabilities, encouraging wider participation and diversity in ai advancements. this democratization of technology leads to more robust, versatile, and accessible ai solutions, catalyzing innovation and growth across various industries and research domains. the escalating need for a comprehensive survey on the knowledge distillation of llms stems from the rapidly evolving landscape of ai (openai et al., 2023; team et al., 2023) and the increasing complexity of these models. as ai continues to penetrate various sectors, the ability to effi- ciently and effectively distill knowledge from proprietary llms to open-source ones becomes not just a technical aspiration but a practical necessity. this need is driven by 3 studentmodelllamagptvicunaopt…… seedknowledgesteerdrivegeneratedknowledgedataset demonstrationsrawdatainput set context followingalignmentagentnlp task specializationmulti-modalityskills lawmedical&healthcarefinancesciencemisc.verticaldomains teacherllm gpt-4 claude llama gemini instructions skill domain knowledgeelicitationdistillationalgorithmtraindivergenceandsimilarity feature featureguide reinforcementlearningoutputsreward rm!(·)distill supervisedfine-tuningx,y preferencerankoptimizationy,1y,2y3y1y2y3≻≻rank…… datacuration x,yrawdatasynthesizefeedbackfeedback input outputself-knowledge outputinputinput ylabellabelingexpansion x,ydemonstrationsexpandfeature featureinput,outputextractsec.4sec.5 sec.3.1sec.3.2 fig. 2: an overview of this survey on knowledge distillation of large language models. note that ‘section’ is abbreviated as ‘sec.’ in this figure. rm s(·)denotes the student reward model. the growing demand for more accessible, cost-effective, and adaptable ai solutions that can cater to a diverse range of applications and users. a survey in this field is vital for synthesizing the current methodologies, challenges, and breakthroughs in knowledge distillation. it may serve as a beacon for researchers and practitioners alike, guiding them through the intricate process of distilling complex ai capabilities into more manageable and accessible forms. moreover, such a survey can illuminate the path forward, identifying gaps in current techniques and proposing direc- tions for future research. survey organization. the remainder of this survey is orga- nized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm ofllms. following this intro- duction, §2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of llms and highlighting the role of data augmentation (da) in this context. §3 delves into the approaches to elicit knowledge from teacher llms and core distillation algorithms, examining methods from supervised fine-tuning to more complex strategies involving divergence and similarity, reinforcement learning, and ranking opti- mization. then, §4 focuses on skill distillation, exploring how student models can be enhanced to improve context understanding, alignment with user intentions, and perfor- mance across a variety of nlp tasks. this includes discus- sions on natural language understanding (nlu), genera- tion (nlg), information retrieval, recommendation systems, and the evaluation of text generation. in §5, we ventureinto domain-specific vertical distillation, showcasing how knowledge distillation techniques are applied within spe- cialized fields such as law, healthcare, finance, and science, illustrating the practical implications and transformative impact of these approaches. the survey suggests open problems in §6, identifying current challenges and gaps in knowledge distillation research that offer opportunities for future work. finally, the conclusion and discussion in §7 synthesize the insights gained, reflecting on the implica- tions for the broader ai and nlp research community and proposing directions for future research. figure 2 shows an overview of this survey. 2 o verview 2.1 comparing traditional recipe',\n",
       " '2. overview': 'the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021). this technique is pivotal in mitigating the challenges posed by the computational demands and resource constraints of deploying large-scale models in practical applications. historically, knowledge distillation techniques, prior to the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh et al., 2019; kim and rush, 2016). this process was largely driven by the need to deploy machine learning models in resource-constrained environments, such as mobile devices or edge computing platforms, where the computational 4knowledge distillation of llmskd algorithmsknowledgelabelingannollm (he et al., 2023a), pandalm (wang et al., 2023b), cot-distill (hsieh et al., 2023) orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), baize (xu et al., 2023b), mammoth (yue et al., 2023a), mixed distill (chenglin et al., 2023) expansionself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), code alpaca (chaudhary, 2023) self-align (sun et al., 2024b), wizardlm (xu et al., 2023a), wizardcoder (luo et al., 2023a), wizardmath (luo et al., 2023b), auggpt (dai et al., 2023a), tdg (he et al., 2023b) curationultrachat (ding et al., 2023b), phi-1 (gunasekar et al., 2023), phi-1.5 (li et al., 2023a), phi-2 (mar, 2023), magicoder (wei et al., 2023), wavecoder (yu et al., 2024) zerogen (ye et al., 2022), sungen (gao et al., 2023a), inpars (bonifacio et al., 2022) featurebabyllama (timiryasov and tastet, 2023), minillm (gu et al., 2024), gkd (agarwal et al., 2024), quantgpt (tao et al., 2022a), llm-qat (liu et al., 2023a), feedbackcai (bai et al., 2022a), wizardmath (luo et al., 2023b), ultrafeedback (cui et al., 2023a), zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), rlaif (lee et al., 2023a), lion (jiang et al., 2023b), persd (chen et al., 2023a), gkd (agarwal et al., 2024) self-knowledgeself-instruct (wang et al., 2022a), self-align (sun et al., 2024b), rlcd (yang et al., 2024a), impdistill (jung et al., 2023), lmsi (huang et al., 2023a), rest (gulcehre et al., 2023), self-rewarding (yuan et al., 2024a), baize (xu et al., 2023b), star (zelikman et al., 2022) distillationsupervised fine-tuningalpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), self-instruct (wang et al., 2022a), baize (xu et al., 2023b), star (zelikman et al., 2022), divergence and similaritydistilgpt (sanh et al., 2019), f-distill (wen et al., 2023), minillm (gu et al., 2024) ted (liang et al., 2023a), gkd (agarwal et al., 2024),babyllama(timiryasov and tastet, 2023) reinforcement learningcai (bai et al., 2022a), ultrafeedback (cui et al., 2023a), wizardmath (luo et al., 2023b), minillm (gu et al., 2024), gkd (agarwal et al., 2024), gpt3 reward (kwon et al., 2023) rank optimization zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), skill distillationcontext followinginstruction followingself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), wizardmath (luo et al., 2023b), llama-gpt4 (peng et al., 2023a), multi-turn dialoguevicuna (chiang et al., 2023), baize (xu et al., 2023b), ultrallama (ding et al., 2023b), camel (li et al., 2023b), openchat (wang et al., 2023c), zephyr (tunstall et al., 2023), rag capbility kard (kang et al., 2023a), sail (luo et al., 2023c), self-rag (asai et al., 2023), alignmentthinking patternselfee (ye et al., 2023), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), aft (wang et al., 2023d), adaptllm (cheng et al., 2023), knowpat (zhang et al., 2023a), preferencecai (bai et al., 2022a), gpt-3 reward (kwon et al., 2023), ilf (scheurer et al., 2023), almost (kim et al., 2023a), rlef (roit et al., 2023), rlaif (lee et al., 2023a), zephy (tunstall et al., 2023), ultrafeedback (cui et al., 2023a), valuecai (bai et al., 2022a), align honesty (yang et al., 2023a), sandbox (liu et al., 2023b), self-align (sun et al., 2024b), ultrafeedback (cui et al., 2023a), rlcd (yang et al., 2024a) agenttool usingtoolformer (schick et al., 2023), graph-toolformer (zhang, 2023), gorilla (patil et al., 2023), toolalpaca (tang et al., 2023a), toolllm (qin et al., 2023a), craft (yuan et al., 2023a), confucius (gao et al., 2023b), mllm-tool (wang et al., 2024), α-umi (shen et al., 2024), planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task specializationnluauggpt (dai et al., 2023a), gpt annotation (gilardi et al., 2023), (ding et al., 2023a), tdg (he et al., 2023b), sungen (gao et al., 2023a), mix distill (chenglin et al., 2023), annollm (he et al., 2023a), udg (wang et al., 2021a), zerogen (ye et al., 2022), nlginheritsumm (xu et al., 2023c), recomp (xu et al., 2024b), mario (ramnath et al., 2023), id (jung et al., 2023), gpt-3 labeling (wang et al., 2021b), biogpt (guo et al., 2023a), chatgpt nmt (yang and nicolai, 2023), information retrievalquill (srinivasan et al., 2022), promptgator (dai et al., 2023b), inpars (bonifacio et al., 2022), augtriever (meng et al., 2023), (sun et al., 2023a), rankvicuna (pradeep et al., 2023a), rankzephyr (pradeep et al., 2023b), exaranker (ferraretto et al., 2023), recommendation ndr (mysore et al., 2023), instrcutrec (zhang et al., 2023b), once (liu et al., 2023c), text generation evaluationpandalm (wang et al., 2023b), prometheus (kim et al., 2024), instructscore (xu et al., 2023d), tigerscore (jiang et al., 2023c), auto-j (li et al., 2024a), codecodealpaca (chaudhary, 2023), codellama (rozi `ere et al., 2023), magicoder (wei et al., 2023) phi-1 (gunasekar et al., 2023), persd (chen et al., 2023a), mftcoder (liu et al., 2023d), wavecoder (yu et al., 2024), code clean (jain et al., 2023), multi-modalityllava (liu et al., 2023e), svit (zhao et al., 2023b), lvis-instruct4v (wang et al., 2023e), shikra (chen et al., 2023c), lskd (park et al., 2023), detgpt (pi et al., 2023; zhao et al., 2023c), lrv (liu et al., 2023f), next-gpt (wu et al., 2023b), valley (luo et al., 2023d), iluvui (jiang et al., 2023d), stablellava (li et al., 2023c), pointllm (xu et al., 2023e), verticalization distillationlaw (huang et al., 2023b; cui et al., 2023b); medical & healthcare (zhang et al., 2023c; chen et al., 2023d); finance (zhang and yang, 2023); science (xie et al., 2023a; zhang et al., 2024) and misc. (dan et al., 2023; guo et al., 2023b) fig. 3: taxonomy of knowledge distillation of large language models. the detailed taxonomy of verticalization distillation is shown in figure 7. 5 power and memory are limited. the focus was predomi- nantly on ad-hoc neural architecture selection and training objectives tailored for single tasks. these earlier methods involved training a smaller student network to mimic the output of a larger teacher network, often through techniques like soft target training, where the student learns from the softened softmax output of the teacher. please refer to the survey (gou et al., 2021) for more details on general knowledge distillation techniques in ai and dl. in contrast, the advent of llms has revolutionized the knowledge distillation landscape. the current era of knowledge distillation in llms shifts the focus from mere architecture compression to the more nuanced process of knowledge elicitation and transfer (taori et al., 2023; chaud- hary, 2023; tunstall et al., 2023). this paradigm change is largely due to the expansive and deep-seated knowledge that llms like gpt-4 and gemini possess. and the inacces- sible parameters of llms make it hard to compress them by using pruning (han et al., 2016) or quantization (liu et al., 2023a) techniques. unlike the earlier era, where the goal was to replicate the output behavior of the teacher model or reduce the model size , the current focus in llm-based knowledge distillation is to extract and transfer the rich, nuanced understanding that these models have developed. the key to this modern approach lies in heuristic and carefully designed prompts, which are used to elicit specific knowledge (ding et al., 2023b) or capabilities (chaudhary, 2023) from the llms. these prompts are crafted to tap into the llm’s understanding and capabilities in various domains, ranging from natural language understanding (he et al., 2023a) to more complex cognitive tasks like reason- ing (hsieh et al., 2023) and problem-solving (qiao et al., 2024). the use of prompts as a means of knowledge elici- tation offers a more flexible and dynamic approach to dis- tillation. it allows for a more targeted extraction of knowl- edge, focusing on specific skills or domains of interest. this method is particularly effective in harnessing the emergent abilities of llms, where the models exhibit capabilities beyond their explicit training objectives. furthermore, this era of knowledge distillation also em- phasizes the transfer of more abstract qualities such as reasoning patterns (mitra et al., 2023), preference align- ment (cui et al., 2023a), and value alignment (sun et al., 2024b). this is in stark contrast to the earlier focus on output replication (taori et al., 2023), indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. the current techniques involve not just the replication of outputs, but also the emulation of the thought processes (mitra et al., 2023) and decision-making (asai et al., 2023) patterns of the teacher model. this involves complex strategies like chain-of-thought prompting, where the student model is trained to learn the reasoning process of the teacher, thereby enhancing its problem-solving and decision-making capabilities. 2.2 relation to data augmentation (da) in the era of llms, data augmentation (da) (wang et al., 2022a; ye et al., 2022) emerges as a critical paradigm integral to the process of knowledge distillation. unlike traditional da techniques such as paraphrasing (gangal et al., 2022) orback-translation (longpre et al., 2019), which primarily aim at expanding the training dataset in a somewhat mechanical manner. da within the context of llms focuses on the generation of novel, context-rich training data tailored to specific domains and skills. this innovation is driven by the unique capabilities of llms to generate coherent, diverse, and intricate data samples that closely mimic the nuanced understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed knowledge, kd employs da to prompt llms to produce explicit data that encapsulates specific skills or domain expertise (chaudhary, 2023; west et al., 2022). this method stands out as a potent mechanism for bridging the knowl- edge and capability gap between proprietary and open- source models. through da, llms are prompted to create targeted, high-quality datasets that are not merely larger in volume but are also rich in diversity and specificity. this approach enables the distillation process to be more effec- tive, ensuring that the distilled models not only replicate the teacher model’s output behavior but also embody its deep-seated understanding and cognitive strategies. the significance and necessity of da for achieving kd in the llm era cannot be overstated. da acts as a force multiplier, enabling the distilled models to acquire and re- fine capabilities that would otherwise require exponentially larger datasets and computational resources. it facilitates a more nuanced and effective transfer of knowledge, fo- cusing on the qualitative aspects of learning rather than quantitative expansion. this strategic use of da within kd processes underscores a pivotal shift towards a more efficient, sustainable, and accessible approach to harnessing the power of llms. it empowers open-source models with the ability to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts, thereby democratizing access to advanced ai capabilities and fostering innovation across a broader spectrum of applications and users. 2.3 survey scope building on the discussions introduced earlier, this survey aims to comprehensively explore the landscape of knowl- edge distillation within the context of llms, following a meticulously structured taxonomy as in figure 3. the survey’s scope is delineated through three primary facets: kd algorithms, skill distillation, and verticalization dis- tillation. each facet encapsulates a range of subtopics and methodologies. it’s important to note that kd algorithms provide the technical foundations for skill distillation and verticalization distillation. kd algorithms. this segment focuses on the technical foundations and methodologies of knowledge distillation. it includes an in-depth exploration of the processes involved in constructing knowledge from teacher models (e.g., pro- prietary llms) and integrating this knowledge into student models (e.g., open-source llms). under the umbrella of ‘knowledge ’, we delve into strategies such as labeling (hsieh et al., 2023), expansion (taori et al., 2023), curation (gu- nasekar et al., 2023), feature understanding (agarwal et al., 6 2024), feedback mechanisms (tunstall et al., 2023), and self- knowledge generation (wang et al., 2022a). this exploration seeks to uncover the various ways in which knowledge can be identified, expanded, and curated for effective dis- tillation. the ‘ distillation ’ subsection examines learning ap- proaches like supervised fine-tuning (sft) (wang et al., 2022a), divergence minimization (agarwal et al., 2024), rein- forcement learning techniques (cui et al., 2023a), and rank optimization strategies (tunstall et al., 2023). this analysis aims to illuminate how these algorithms facilitate the trans- fer of knowledge, ensuring that open-source models can replicate and, in some cases, surpass the capabilities of their proprietary counterparts. skill distillation. this facet examines the specific compe- tencies and capabilities enhanced through kd. it encom- passes detailed discussions on context following (taori et al., 2023; luo et al., 2023c), with subtopics like instruction following and retrieval-augmented generation (rag) capa- bility. in the realm of alignment (mitra et al., 2023; tun- stall et al., 2023), the survey investigates thinking patterns, persona/preference modeling, and value alignment. the ‘agent’ category delves into skills such as tool using and planning. nlp task specialization (dai et al., 2023a; jung et al., 2023; chaudhary, 2023) is scrutinized through lenses like natural language understanding (nlu), natural lan- guage generation (nlg), information retrieval, recommen- dation systems, text generation evaluation, and code gen- eration. finally, the survey addresses multi-modality (liu et al., 2023e; zhao et al., 2023b), exploring how kd enhances llms’ ability to interpret and integrate multiple forms of input, enriching their utility and applicability across various contexts. verticalization distillation. this section assesses the ap- plication of kd across diverse vertical domains, offering insights into how distilled llms can be tailored for spe- cialized fields such as law (law, 2023), medical & health- care (wang et al., 2023a), finance (zhang and yang, 2023), science (zhang et al., 2024), among others. this exploration not only showcases the practical implications of kd tech- niques but also highlights their transformative impact on domain-specific ai solutions. through detailed analysis and examples, this part aims to demonstrate the versatility and efficacy of kd in adapting llms to meet the nuanced demands of different industries, thus contributing to the broader ai and ml ecosystem. by navigating through these facets, this survey en- deavors to provide an extensive and nuanced analysis of knowledge distillation in the era of llms. it serves as a guide for researchers, practitioners, and enthusiasts in the field, shedding light on current methodologies, challenges, and opportunities for innovation in this rapidly evolving domain. declaration. this survey represents our earnest effort to provide a comprehensive and insightful overview of knowl- edge distillation techniques applied to llms, focusing on algorithms, skill enhancement, and domain-specific appli- cations. given the vast and rapidly evolving nature of this field, especially with the prevalent practice of elic- iting knowledge from training data across academia, weacknowledge that this manuscript may not encompass every pertinent study or development. nonetheless, it endeavors to introduce the foundational paradigms of knowledge dis- tillation, highlighting key methodologies and their impacts across a range of applications. 2.4 distillation pipeline in llm era seedknowledgeskill/domain teacherllmknowledgeelicitationstudentmodeldistillationalgorithmsteer drivegeneratedknowledgelearningobjectivetrain fig. 4: an illustration of a general pipeline to distill knowl- edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge from a sophisticated teacher model to a less complex student model. this pipeline is integral for leveraging the advanced capabilities of models like gpt-4 or gemini in more acces- sible and efficient open-source counterparts. the outline of this pipeline can be broadly categorized into four distinct stages, each playing a crucial role in the successful distilla- tion of knowledge. an illustration is shown in figure 4. the detailed pipeline could also be seen in figure 2. i. target skill or domain steering teacher llm. the first stage involves directing the teacher llm towards a specific target skill or domain. this is achieved through care- fully crafted instructions or templates that guide the llm’s focus. these instructions are designed to elicit responses that demonstrate the llm’s proficiency in a particular area, be it a specialized domain like healthcare or law, or a skill such as reasoning or language understanding. the objective here is to utilize the teacher llm’s extensive training and nuanced capabilities to generate outputs that are rich in the specific knowledge or skills desired for the student model. ii. seed knowledge as input. once the target area is defined, the next step is to feed the teacher llm with seed knowledge. this seed knowledge typically comprises a small dataset or specific data clues relevant to the elicit skill or domain knowledge from the teacher llm. it acts as a catalyst, prompting the teacher llm to generate more elaborate and detailed outputs based on this initial infor- mation. the seed knowledge is crucial as it provides a foundation upon which the teacher model can build and expand, thereby creating more comprehensive and in-depth knowledge examples. iii. generation of distillation knowledge. in response to the seed knowledge and steering instructions, the teacher llm generates knowledge examples. these examples are predominantly in the form of question-and-answer (qa) dialogues or narrative explanations, aligning with the nat- ural language processing/understanding capabilities of the 7 llm. in certain specialized cases, the outputs may also in- clude logits or hidden features, although this is less common due to the complexity and specific requirements of such data forms. the generated knowledge examples constitute the core of the distillation knowledge, encapsulating the advanced understanding and skills of the teacher llm. iv . training the student model with a specific learn- ing objective. the final stage involves the utilization of the generated knowledge examples to train the student model. this training is guided by a loss function that aligns with the learning objectives. the loss function quantifies the student model’s performance in replicating or adapting the knowledge from the teacher model. by minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. the process involves iteratively adjusting the student model’s parameters to reduce the discrepancy be- tween its outputs and those of the teacher model, ensuring the effective transfer of knowledge. in essential, the above four stages can be abstracted as two formulations. the first formulation represents the process of eliciting knowledge: d(kd) i={parse( o, s)|o∼pt(o|i⊕s),∀s∼ s} , (1) where ⊕denotes fusing two pieces of text, idenotes an instruction or a template for a task, skill, or domain to steer the llm and elicit knowledge, s∼ s denotes an example of the seed knowledge, upon which the llm can explore to generate novel knowledge, parse( o, s)stands for to parse the distillation example ( e.g., (x, y)) from the teacher llm’s output o(plus the input sin some cases), andptrepresents the teacher llm with parameters θt. given the datasets d(kd) ibuilt for distillation, we then define a learning objective as l=x ili(d(kd) i;θs), (2) wherep idenotes there could be multiple tasks or skills being distilled into one student model, li(·;·)stands for a specific learning objective, and θsparameterizes the student model. following our exploration of the distillation pipeline and the foundational concepts underlying knowledge distilla- tion in the llm era, we now turn our focus to the specific algorithms that have gained prominence in this era. 3 k nowledge distillation algorithms',\n",
       " '3. knowledge distillation algorithms': 'this section navigates through the process of knowledge distillation. according to section 2.4, it is categorized into two principal steps: ‘knowledge,’ focusing on eliciting knowledge from teacher llms (eq.1), and ‘distillation,’ centered on injecting this knowledge into student models (eq.2). we will elaborate on these two processes in the subsequent sections. 3.1 knowledge this section focuses on the approaches to elicit knowledge from teacher llms. according to the manners to acquire knowledge, we divided them into labeling ,expansion ,datacuration ,feature ,feedback , and self-knowledge . figure 5 shows an illustration of these knowledge elicitation meth- ods. 3.1.1 labeling labeling knowledge refers to using a teacher llm to label the output yfor a given input xas the seed knowledge, according to the instruction ior demonstrations c, where c= (x1, y1), . . . , (xn, yn). this method of eliciting knowl- edge from teacher llms is straightforward yet effective and has been widely applied across various tasks and appli- cations. it requires only the collection of an input dataset and feeding it into llms to obtain the desired generations. moreover, the generation of yis controllable through the predefined iandc. this process can be formulated as follows: d(lab)={x, y|x∼ x, y∼pt(y|i⊕c⊕x)}. (3) input xcould be sourced from existing nlp task datasets, which serve as typical reservoirs for distillation efforts. numerous works have sought to harness the capa- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- egorize text (gilardi et al., 2023; ding et al., 2023a; he et al., 2023a), while in natural language generation, llms assist in generating sequences for outputs (hsieh et al., 2023; jung et al., 2023; wang et al., 2021b). text generation evaluation tasks leverage llms to label evaluated results (li et al., 2024b; wang et al., 2023b), and reasoning tasks utilize llms for labeling chains of thought (cot) explanations (hsieh et al., 2023; li et al., 2022; ho et al., 2023; magister et al., 2023; fu et al., 2023; ramnath et al., 2023; li et al., 2023d; liu et al., 2023g), among others. rather than concentrating on specific tasks, many current works focus on labeling outputs based on instructions, thereby teaching student models to solve tasks in a more flexible way by following in- structions. collections of various nlp tasks, complemented by instructional templates, serve as valuable input sources forx. for instance, flan-v2 collections (longpre et al., 2023) offers extensive publicly available sets of tasks with instructions, which are labeled with responses generated by teacher llms in orca (mukherjee et al., 2023; mitra et al., 2023). the instructions from these nlp tasks are built from predefined templates, which lack diversity and may have gaps between human’s natural query. the real conversations between humans and chat models provide large-scale data with real queries and generations labeled by powerful llms, like sharegpt. additionally, xu et al. (2023b) and anand et al. (2023) label the real questions sampled from forums like quora and stack overflow. moreover, the process of labeling could be guided by instructions ior demonstrations c. a commonly used in- struction type for guiding labeling is chain-of-thought (cot) prompt (hsieh et al., 2023; fu et al., 2023; magister et al., 2023). mukherjee et al. (2023) add multiple system messages (e.g. “you must generate a detailed and long answer.” or “explain like i’m five, think step-by-step”) to elicit rich signals. yue et al. (2023a) and chenglin et al. (2023) la- bel a hybrid of knowledge of chain-of-thought (cot) and 8 𝑐𝐼labelingexpansion𝑥𝐼𝑦𝑥𝑦expandcompleteupdatedata curation 𝑚meta sources 𝐼𝑥𝑦 input set completecreatesamplegenerate 𝑚meta-information𝑐demonstrations𝑥𝐼 𝑦 filterfeedback extractfeature𝑥𝑦 distributionintermediatefeature 𝑥input𝑦output𝐼instruction𝑦! 𝑦\" 𝑦# 𝑥guidefeedback𝑦#∗ 𝑦# feedback self-knowledge studentteacher generate≻≻𝑦\" 𝑦! 𝑦# 𝑥 𝑥& correctexpand𝑐 fig. 5: an illustration of different knowledge elicitation methods from teacher llms. labeling : the teacher generates the output from the input; expansion : the teacher generates samples similar to the given demonstrations through in- context learning; data curation : the teacher synthesizes data according to meta-information, such as a topic or an entity; feature : feed the data into the teacher and extract its internal knowledge, such as logits and features; feedback : the teacher provides feedback on the student’s generations, such as preferences, corrections, expansions of challenging samples, etc; self-knowledge : the student first generates outputs, which is then filtered for high quality or evaluated by the student itself. program-of-thought (pot) rationales. xu et al. (2023b) pro- pose a self-chat technique that two teacher llms simulate the real conversational to generate multi-turn dialogues for a question from quora and stack overflow. 3.1.2 expansion while the labeling approach is simple and effective, it faces certain limitations. primarily, it is constrained by the scale and variety of the input data. in real-world applications, especially those involving user conversations, there are also concerns regarding the privacy of the data involved. to address these limitations, various expansion methods have been proposed (wang et al., 2022a; taori et al., 2023; chaud- hary, 2023; si et al., 2023; ji et al., 2023a; luo et al., 2023b,a; wu et al., 2023c; sun et al., 2024b; xu et al., 2023a; guo et al., 2023c; rozi `ere et al., 2023; west et al., 2022). these methods take the demonstrations as seed knowledge and aim to expand a large scale and various data by in-context learning. a key characteristic of these expansion methods is the utilization of the in-context learning ability of llms to gen- erate data similar to the provided demonstrations c. unlike in the labeling approach, where the input xis sampled from the existing dataset, in the expansion approach, both x andyare generated by teacher llms. this process can be formulated as follows: d(exp)={(x, y)|x∼pt(x|i⊕c), y∼pt(y|i⊕x)}.(4) in this formulation, xand yrepresent the new input- output pairs generated by the teacher llm. the input x is generated based on a set of input-output demonstrations c. the output yis then generated in response to the new input xunder the guidance of an instruction i. note thatthe demonstrations could be predefined or dynamically updated by adding the newly generated samples. expansion techniques have been widely utilized to extract extensive instruction-following knowledge from teacher llms. wang et al. (2022a) first introduces an iter- ative bootstrapping method, self-instruct, to utilize llms to generate a wide array of instructions based on sev- eral demonstrations sampled from 175 manually-written in- structions. the newly generated instructions are then added back to the initial pool, benefiting subsequent expansion iterations. subsequently, taori et al. (2023) applies this ex- pansion method to a more powerful teacher llm, text- davinci-003, to distill 52k high-quality data. to improve the diversity and coverage during expansion, wu et al. (2023c) and (sun et al., 2024b) prompt the teacher llm to generate instructions corresponding to some specific topics. xu et al. (2023a) propose an evol-instruct method to ex- pand the instructions from two dimensions: difficulty (e.g. rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to expand the distillation of coding (luo et al., 2023a) and math (luo et al., 2023b). additionally, expansion methods can significantly augment nlp task datasets with similar samples, thereby enhancing task performance. for instance, auggpt (dai et al., 2023a) leverages a teacher llm to rephrase each sentence in the training samples into multi- ple conceptually similar, but semantically varied, samples to improve classification performance. similarly, tdg (he et al., 2023b) proposes the targeted data generation (tdg) framework, which automatically identifies challenging sub- groups within data and generates new samples for these subgroups using llms through in-context learning. in summary, the expansion method leverages the in- 9 context learning strengths of llms to produce more var- ied and extensive datasets with both inputs and outputs. however, the quality and diversity of the generated data are heavily reliant on the teacher llms and the initial seed demonstrations. this dependence can lead to a dataset with inherent bias from llms (yu et al., 2023a; wei et al., 2023) and a homogeneity issue where the generations may be prone to similarity ultimately, limiting the diversity this method seeks to achieve (ding et al., 2023b). moreover, the expansion process may inadvertently amplify any biases present in the seed data. 3.1.3 data curation the pursuit of high-quality and scalable data generation in knowledge distillation from llms has led to the emergence of the data curation approach. this method arises in re- sponse to the limitations observed in both the labeling and expansion approaches. these methods often yield data of variable quality and face constraints in quantity. in labeling, the seed knowledge is sourced from task datasets, leading to potential noise and dirty data. meanwhile, in expansion, the input xis derived from seed demonstrations, which can result in homogeneous data when generated in large quantities. to overcome these challenges, the data curation method curates high-quality or large-scale data by extensive meta-information as seed knowledge (ding et al., 2023b; gunasekar et al., 2023; li et al., 2023a; mar, 2023; liu et al., 2023d; wei et al., 2023; yu et al., 2024; ye et al., 2022; gao et al., 2023a; yang and nicolai, 2023). a distinct feature of data curation is its approach to synthesize data from scratch. numerous diverse meta- information, such as topics or knowledge points, could be incorporated into this process to generate controllable x andy. thus, this process can be meticulously controlled to yield datasets that are not only large in scale but also of high quality. the formulation for data curation can be represented as: d(cur)={(x, y)|x∼pt(x|i⊕m), y∼pt(y|i⊕x)}.(5) in this formulation, mrepresents the diverse meta- information used to guide the synthesis of x, and iis the instruction guiding teacher llms to generate xory. different studies primarily vary in their source and method of leveraging meta-information. ultrachat (ding et al., 2023b) effectively demonstrates the process of curating both high-quality and diverse data by distilled knowledge. they collect extensive meta-information across three do- mains: questions about the world, creation and generation , and assistance on existing materials . for example, under questions about the world , they explore 30 meta-topics like ”technology” and ”food and drink.” the teacher llms then use this meta-information to distill a broad array of instructions and conversations, achieving a substantial scale of 1.5 million instances. ultrachat stands out with its lexical and topical diversity. the ultrallama model, fine- tuned on this data, consistently surpasses other open-source models. another notable series, phi(gunasekar et al., 2023; li et al., 2023a; mar, 2023), focuses on distilling smaller, high-quality datasets akin to ”textbooks.” phi-1 (gunasekar et al., 2023) experiments with synthesizing ”textbook qual- ity” data in the coding domain. their approach involvesdistilling clear, self-contained, instructive, and balanced con- tent from llms, guided by random topics or function names to enhance diversity. the distilled data is a synthesis of 1 billion tokens of python textbooks, complete with natural language explanations and code snippets, as well as 180 mil- lion tokens of python exercises with solutions. remarkably, thephi-1 model, despite its smaller size, outperforms nearly all open-source models on coding benchmarks like hu- maneval and mbpp while being 10 times smaller in model size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in contrast, magicoder (wei et al., 2023) and wavecoder (yu et al., 2024) get raw code collections from open-source code datasets, using this as meta-information for generating instructional data. in the context of nlu tasks, certain studies (ye et al., 2022; gao et al., 2023a; wang et al., 2021a) explore the use of labels as meta-information to synthesize corresponding samples for data augmentation. similarly, in information retrieval tasks, there are efforts to utilize docu- ments as meta-information for generating potential queries, thereby constructing large-scale retrieval pairs (bonifacio et al., 2022; meng et al., 2023). in conclusion, data curation through teacher llms has emerged as a promising technique for synthesizing datasets that are not only high-quality and diverse but also large in scale. the success of models like phi-1 in specialized domains underscores the efficacy of this method. the ability to create synthetic datasets will become a crucial technical skill and a key area of focus in ai (li et al., 2023a). 3.1.4 feature the previously discussed knowledge elicitation methods are typically applied to powerful black-box models, which are expensive and somewhat unreproducible due to calling api. in contrast, white-box distillation offers a more trans- parent and accessible approach for researchers. it involves leveraging the output distributions, intermediate features, or activations from teacher llms, which we collectively refer to as feature knowledge. white-box kd approaches have predominantly been studied for smaller encoder-based lms, typically those with fewer than 1 billion parameters (cf. gou et al. (2021) for detail). however, recent research has begun to explore white-box distillation in the context of generative llms (timiryasov and tastet, 2023; liang et al., 2023a; gu et al., 2024; agarwal et al., 2024; liu et al., 2023a; wen et al., 2023; wan et al., 2024a; zhao and zhu, 2023; qin et al., 2023b; boizard et al., 2024; zhong et al., 2024). the typical method for acquiring this feature knowledge involves teacher llms annotating the output sequence y with its internal representations. these annotations are then distilled into the student model using methods such as kullback-leibler divergence (kld). the process of eliciting feature knowledge can be formulated as follows: d(feat)={(x, y, ϕ feat(x, y;θt))|x∼ x, y∼ y} . (6) in this formulation, yis the output set, which can be generated by teacher llms, the student model, or directly sourced from the dataset. ϕfeat(·;θt)represents the opera- tion of extracting feature knowledge (such as output distri- bution) from the teacher llm. 10 the most straightforward method to elicit feature knowl- edge of teacher is to label a fixed dataset of sequences with token-level probability distributions (sanh et al., 2019; wen et al., 2023). to leverage the rich semantic and syntactic knowledge in intermediate layers of the teacher model, ted (liang et al., 2023a) designs task-aware layer-wise distillation. they align the student’s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task. gu et al. (2024) and agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, termed ‘self- generated sequences.’ the student then learns by using feedback (i.e. output distribution) from teacher on these sequences. this method is particularly beneficial when the student model lacks the capacity to mimic teacher’s distri- bution. moreover, various llm-quantization methods with distilling feature knowledge from teacher llms have been proposed (tao et al., 2022a; liu et al., 2023a; kim et al., 2023b). these methods aim to preserve the original output distribution when quantizing the llms, ensuring minimal loss of performance. additionally, feature knowledge could serve as a potent source for multi-teacher knowledge distil- lation. timiryasov and tastet (2023) leverages an ensemble of gpt-2 and llama as teacher models to extract output distributions. similarly, fusellm (wan et al., 2024a) inno- vatively combines the capabilities of various llms through a weighted fusion of their output distributions, integrating them into a singular llm. this approach has the potential to significantly enhance the student model’s capabilities, surpassing those of any individual teacher llm. in summary, feature knowledge offers a more transpar- ent alternative to black-box methods, allowing for deeper insight into and control over the distillation process. by utilizing feature knowledge from teacher llms, such as output distributions and intermediate layer features, white- box approaches enable a more nuanced transfer of informa- tion. while showing promise, especially in smaller models, its application is not suitable for black-box llms where internal parameters are inaccessible. furthermore, student models distilled from white-box llms may underperform compared to their black-box counterparts, as the black-box teacher llms (e.g. gpt-4) tend to be more powerful. 3.1.5 feedback most previous works predominantly focus on one-way knowledge transfer from the teacher to the student for imitation, without considering feedback from the teacher on the student’s generation. the feedback from the teacher typically offers guidance on student-generated outputs by providing preferences, assessments, or corrective informa- tion. for example, a common form of feedback involves teacher ranking the student’s generations and distilling this preference into the student model through reinforcement learning from ai feedback (rlaif) (bai et al., 2022a). here is a generalized formulation for eliciting feedback knowledge: d(fb)={(x, y, ϕ fb(x, y;θt))|x∼ x, y∼ps(y|x)}, (7) where ydenotes the output generated by the student model in response to x, and ϕfb(·;θt))represents providing feedback from teacher llms. this operation evaluates thestudent’s output ygiven the input x, by offering assess- ment, corrective information, or other forms of guidance. this feedback knowledge can not only be distilled into the student to also generate feedback (such as creating a student preference model) but, more importantly, enable the student to refine its responses based on the feedback. various methods have been explored to elicit this advanced knowledge (bai et al., 2022a; luo et al., 2023b; cui et al., 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). preference, as previously discussed, represents a notable form of feedback knowledge from teacher models. various knowledge of preferences could be distilled from teachers by prompting it with specific criteria. bai et al. (2022a) in- troduce rlaif for distilling harmlessness preferences from llms. this involves using an sft-trained llm to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. this dataset is distilled into a preference model (pm), which then guides the rl training of a more harmless llm policy. wizard- math (luo et al., 2023b) places emphasis on mathematical reasoning. they employ chatgpt as teacher to directly provide process supervision and evaluate the correctness of each step in the generated solutions. to scale up high- quality distilled preference data, cui et al. (2023a) develop a large-scale preference dataset for distilling better preference models, ultrafeedback. it compiles various instructions and models to produce comparative data. then, gpt-4 is used to score candidates from various aspects of preference, including instruction-following, truthfulness, honesty and helpfulness. beyond merely assessing student generations, teachers can also furnish extensive feedback on instances where students underperform. in lion (jiang et al., 2023b), teacher model pinpoints instructions that pose challenges to the student model, generating new, more difficult instructions aimed at bolstering the student’s abilities. persd (chen et al., 2023a) showcases a method where teacher offers tailored refinement feedback on incorrect code snippets gen- erated by students, guided by the specific execution errors encountered. similarly, selfee (ye et al., 2023) leverages chatgpt to generate feedback and revise the student’s answer based on the feedback. in contrast, figa (guo et al., 2024) revises the student’s response by comparing it to the ground-truth response. furthermore, teacher model’s distribution over the student’s generations can itself act as a form of feedback. minillm (gu et al., 2024) and gkd (agarwal et al., 2024) present an innovative strategy wherein the student model initially generates sequences, followed by teacher model producing an output distribution as feedback. this method leverages the teacher’s insight to directly inform and refine the student model’s learning process. 3.1.6 self-knowledge the knowledge could also be elicited from the student itself, which we refer to as self-knowledge . in this setting, the same model acts both as the teacher and the student, iteratively improving itself by distilling and refining its own previously 11 generated outputs. this knowledge uniquely circumvents the need for an external, potentially proprietary, powerful teacher model, such as gpt-series llms. furthermore, it allows the model to surpass the limitations or “ceiling” inherent in traditional teacher-student methods. eliciting self-knowledge could be formulated as: d(sk)={(x, y, ϕ sk(x, y))|x∼ s, y∼ps(y|i⊕x)},(8) where ϕsk(·)is a generalized function that represents an additional process to the self-generated outputs y, which could include but is not limited to filtering, rewarding, or any other mechanisms for enhancing or evaluating y. it could be governed by external tools or the student itself θs. recent research in this area has proposed various innovative methodologies to elicit self-knowledge, demonstrating its potential for creating more efficient and autonomous learn- ing systems. (allen-zhu and li, 2020; wang et al., 2022a; sun et al., 2024b; yang et al., 2024a; jung et al., 2023; huang et al., 2023a; gulcehre et al., 2023; yuan et al., 2024a; xu et al., 2023b; zelikman et al., 2022; chen et al., 2024a; zheng et al., 2024; li et al., 2024c; zhao et al., 2024; singh et al., 2023; chen et al., 2024c; hosseini et al., 2024) a notable example of this methodology is self- instruct (wang et al., 2022a), which utilizes gpt-3 for data augmentation through the expansion approach, gen- erating additional data samples to enhance the dataset. this enriched dataset subsequently fine-tunes the original model. other methods aim to elicit targeted knowledge from student models by modifying prompts, and leveraging these data for further refinement. in self-align (sun et al., 2024b), they find that models fine-tuned by self-instruct data tend to generate short or indirect responses. they prompt this model with verbose instruction to produce in- depth and detailed responses. then, they employ context- distillation (askell et al., 2021) to distill these responses paired with non-verbose instructions back to the model. similarly, rlcd (yang et al., 2024a) introduces the use of contrasting prompts to generate preference pairs from an unaligned llm, encompassing both superior and inferior examples. a preference model trained on these pairs then guides the enhancement of the unaligned model through reinforcement learning. several other approaches employ filtering methods to refine self-generated data. for exam- ple, impossible distillation (jung et al., 2023) targets sen- tence summarization tasks, implementing filters based on entailment, length, and diversity to screen self-generated summaries. lmsi (huang et al., 2023a) generates multiple cot reasoning paths and answers for each question, and then retains only those paths that lead to the most consistent answer. note that refined self-knowledge can be iteratively ac- quired as the student model continuously improves, further enhancing the student’s capabilities. this is gulcehre et al. (2023) introduces a reinforced self-training (rest) frame- work that cyclically alternates between grow andimprove stages to progressively obtain better self-knowledge and refine the student model. during the grow stage, the student model generates multiple output predictions. then, in the improve stage, these self-generated outputs are ranked and filtered using a scoring function. subsequently, the lan- guage model undergoes fine-tuning on this curated dataset,employing an offline rl objective. self-play (chen et al., 2024a) introduces a framework resembling iterative dpo, where the language model is fine-tuned to differentiate the self-generated responses from the human-annotated data. these self-generated responses could be seen as “negative knowledge” to promote the student to better align with the target distribution. self-rewarding (yuan et al., 2024a) explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for the self-generated responses. the entire process can then be iterated, improving instruction following and reward modeling capabilities. 3.2 distillation this section focuses on the methodologies for effectively transferring the elicited knowledge from teacher llms into student models. we explore a range of distillation tech- niques, from the strategies that enhance imitation by su- pervised fine-tuning ,divergence and similarity , to advanced methods like reinforcement learning and rank optimization , as shown in figure 3. 3.2.1 supervised fine-tuning supervised fine-tuning (sft), or called sequence-level kd (seqkd) (kim and rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-box llms. sft finetunes student model by maximizing the like- lihood of sequences generated by the teacher llms, aligning the student’s predictions with those of the teacher. this process can be mathematically formulated as minimizing the objective function: lsft=ex∼x,y∼pt(y|x)[−logps(y|x)], (9) where yis the output sequence produced by the teacher model. this simple yet highly effective technique forms the basis of numerous studies in the field. numerous re- searchers have successfully employed sft to train student models using sequences generated by teacher llms (taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; luo et al., 2023b). additionally, sft has been ex- plored in many self-distillation works (wang et al., 2022a; huang et al., 2023c; xu et al., 2023b; zelikman et al., 2022). due to the large number of kd works applying sft, we only list representative ones here. more detailed works can be found in §4. 3.2.2 divergence and similarity this section mainly concentrates on algorithms designed for distilling feature knowledge from white-box teacher llms, including distributions and hidden state features. these algorithms can be broadly categorized into two groups: those minimizing divergence in probability distributions and those aimed at enhancing the similarity of hidden states. divergence. divergence-based methods minimize diver- gence between the probability distributions of the teacher 12 divergence type d(p, q)function forward kldpp(t) logp(t) q(t) reverse kldpq(t) logq(t) p(t) js divergence1 2\\x10pp(t) log2p(t) p(t)+q(t)+pq(t) log2q(t) p(t)+q(t)\\x11 table 1: functional forms of dfor various divergence types. p: reference similarity function lf expression l2-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥2 l1-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥1 cross-entropy loss −pφt(ft(x, y)) log(φ s(fs(x, y))) maximum mean discrepancy mmd (φt(ft(x, y)),φs(fs(x, y))) table 2: summary of similarity functions in knowledge distillation. and student models, represented by a general divergence function d: ldiv= e x∼x,y∼y[d(pt(y|x), ps(y|x))], (10) the specific form of dvaries depending on the type of divergence employed. table 1 outlines the functional forms ofdfor different divergence measures. the commonly-used standard kd objectives essentially minimize the approxi- mated forward kullback-leibler divergence (kld) between the teacher and the student distribution (sanh et al., 2019; wen et al., 2023; timiryasov and tastet, 2023; liang et al., 2023a; chen et al., 2024d) , which forces psto cover all the modes of pt. however, when a student model is unable to learn all modes of a highly complex teacher, the re- sultant “mode-covering” behavior might cause the student to assign probability mass to tokens with low probability under the teacher’s distribution (cf. figure 6 blue curve). this mode-covering phenomenon can potentially lead to hallucinations and low-quality generations. alternatively, mode-seeking divergences like reverse kl prioritize tokens where the teacher assigns high probabilities (cf. figure 6 green curve). this approach can mitigate the risk of low- quality outputs, fostering more accurate generations. how- ever, it often does so at the cost of reduced diversity. gu et al. (2024) adopt reverse kl divergence to prevent students from overestimating low-probability regions of the teacher’s distribution, employing policy gradient methods for opti- mization. both agarwal et al. (2024) and sason and verd ´u (2016) assess the efficacy of different divergence functions in llm distillation, finding the optimal divergence to be task-dependent. for instance, forward kl divergence is more suitable for tasks like machine translation, where the output has fewer modes or variations, while reverse kl divergence is preferable for tasks like dialogue generation and instruction tuning, which involve multiple modes and a wider range of potential responses. thus, the nature of the task significantly influences the selection of the divergence function for optimal performance. similarity. similarity-based methods in knowledge distilla- tion aim to align the hidden states or features of the student pargminqkl(p||q)argminqkl(q||p)fig. 6: comparison of forward and reverse kl diver- gences in approximating a target distribution . forward kl divergence approach tends to cover all modes of the target distribution but is less precise, i.e. “mode-covering” behavior. reverse kl divergence method focuses predom- inantly on the most prominent mode, thereby exhibiting a “mode-seeking” behavior. model with those of the teacher. these methods use various similarity metrics to measure and optimize the congruence of internal representations between the two models. the objective is to ensure that the student model not only produces similar outputs to the teacher but also processes information in a comparable manner. the formulation for a similarity-based objective might look like this: lsim= e x∼x,y∼y[lf(φt(ft(x, y)),φs(fs(x, y)))],(11) where ft(x, y)andfs(x, y)are the feature maps of the teacher and student models, respectively. the transforma- tion functions φtandφsare applied to these feature maps to ensure they are in the same shape, facilitating direct comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based methods in the kd of llms. among them, liang et al. (2023a) propose task-aware layer-wise distillation (ted), a method that utilizes task-aware filters. these filters are designed to selectively capture the most pertinent informa- tion for a specific task from the teacher model. the key objective is to minimize the discrepancy between the filtered representations in both teacher and student models. while similarity-based approaches are common in encoder-based lms (sun et al., 2019, 2020; jiao et al., 2020; hou et al., 2020; zuo et al., 2022; liang et al., 2021), their application in llm knowledge distillation is not as widespread. however, considering their effectiveness, we anticipate an increase in research exploring these methods for llm distillation in the near future. 3.2.3 reinforcement learning this section explores advanced methods of distilling knowl- edge into student models using reinforcement learning (rl). this approach is especially relevant for leveraging the feed- back from teacher to train student models (bai et al., 2022a; cui et al., 2023a; luo et al., 2023b; agarwal et al., 2024; chen et al., 2024b; ma et al., 2023a; pang et al., 2023; du et al., 2023a). the rl-based distillation process typically involves two main stages: 13 distilled reward model training. the first stage involves training a reward model rϕusing the feedback data d(fd) generated by teacher llms. preference data, as one of the typical feedback, is employed to train the student reward model (bai et al., 2022a; cui et al., 2023a; lee et al., 2023a; kim et al., 2023a). they usually consist of input-output pairs (x, yw, yl). here, ywandylrepresent “winning” and “losing” outputs relative to the teacher’s preferences. the loss function for the reward model is defined as: lrm(rϕ,d(fd)) =− e (x,yw,yl)∼d(fd)[logσ(rϕ(x, yw)−rϕ(x, yl))] (12) this formulation guides the reward model to correctly distinguish between more and less preferable outputs based on the teacher’s criteria. instead of learning the instance- level rewards, rlmec (chen et al., 2024b) adopts a dif- ferent approach by training a generative reward model. it is trained on an erroneous solution rewriting data distilled from a teacher llm. this distilled reward model can pro- duce token-level rewards for rl training. reinforcement learning optimization. in the second stage, the student model, represented by a policy πθ, is optimized to maximize the expected reward as per the trained reward model. simultaneously, it minimizes the divergence from a reference policy πref, typically the initial policy of the student model trained by sft, controlled by a factor β. the rl objective is given by: max πθe x∼x,y∼πθ(y|x)[rϕ(x, y)]−βdkl[πθ(y|x)∥πref(y|x)] (13) this rl framework not only ensures that the student model learns the explicit content from the teacher but also effec- tively adopts the teacher’s preference patterns. the use of rl, particularly with the ppo (schulman et al., 2017) algo- rithm, offers a robust mechanism for aligning the student model’s outputs with the teacher. alternatively, the teacher llm can also serve as the reward model to directly assign rewards during rl, circumventing the need for training a reward model (lee et al., 2023a; kwon et al., 2023). while this approach may exhibit superior performance, it comes at a higher computational cost compared to employing a smaller distilled reward model. 3.2.4 ranking optimization ranking optimization presents a stable and computationally efficient alternative to rl for injecting preference feedback into language models (rafailov et al., 2023; song et al., 2023a; yuan et al., 2023b). this method, diverging from traditional rl approaches, directly incorporates ranking information into language models from a fixed preference dataset during fine-tuning. intuitively, it directly updates policy to increase the relative likelihood of preferred over less favored responses. this direct optimization of prefer- ences, without the need for sampling outputs, makes the process more stable and efficient. recently, some works have been proposed to explore using ranking optimization todistill teacher’s preferences into student models (tunstall et al., 2023; hong et al., 2023; yuan et al., 2024a). zephyr (tunstall et al., 2023) utilizes direct preference optimization (dpo) (rafailov et al., 2023) to distill the preference alignment in teacher llms. dpo streamlines the objective of reinforcement learning (as in eq. 13), which involves reward maximization with a kl-divergence constraint, into a single-stage policy training. specifically, dpo’s training goal is to maximize the following expecta- tion: e (x,yw,yl)∼d(fd)\\x14 logσ\\x12 βlogπθ(yw|x) πref(yw|x)−βlogπθ(yl|x) πref(yl|x)\\x13\\x15 , (14) where ywis preferred over ylaccording to the teacher llm. hong et al. (2023) (hong et al., 2023) adopt two ranking-based optimization objectives, rank responses to align human feedback (rrhf) (yuan et al., 2023b) and preference ranking optimization (pro) (song et al., 2023a), for preference distillation. rrhf (yuan et al., 2023b) focuses on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj are their corresponding conditional log probabilities under the policy πθ. this approach emphasizes direct comparison and ranking of responses based on the teacher’s preferences. pro (song et al., 2023a) expands the concept of pairwise comparison to handle preference rankings of any length. for a given instruction xand a sequence of responses ordered by teacher preference as y1≻y2≻...≻yn, the rpo training objective is: lpro=−n−1x k=1logexp (pk)pn i=kexp (pi), (16) where pkrepresents the conditional log probabilities for ykunder the student policy πθ. by iteratively contrasting the likelihood of generating responses, pro optimizes the student lm to prioritize the most preferred response while progressively ranking the rest in the order of diminishing preference. 4 s kill distillation',\n",
       " '4. skill distillation': 'building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms. our exploration will encompass a diverse range of skills exhibited by llms, including context following ,alignment ,agent ,nlp task specializa- tion and multi-modality .context following focuses on the student’s ability to comprehend and respond effectively to input information. alignment delves into the student’s capability to align its output with the teacher’s responses. moving forward, agent underscores the autonomous nature of language models. nlp task specialization highlights the llm’s versatility in specializing across various natural language processing tasks, demonstrating its adaptability. 14 methods skill seed knowledge teacher llm student model knowledge elicitation objective context following self-instruct (wang et al., 2022a) if 175 human-curated tasks gpt3 gpt3 expansion + self-knowledge sft alpaca (taori et al., 2023) if 175 human-curated tasks gpt3 llama expansion + self-knowledge sft lamini-lm (wu et al., 2023c) if3.5k wikipedia categories + mixed datasetchatgpt various models expansion sft wizardlm (xu et al., 2023a) if alpaca data chatgpt llama expansion sft lion (jiang et al., 2023b) if alpaca cata chatgpt llama labeling + expansion + feedback - babyllama (timiryasov and tastet, 2023) if 10m-word babylm dataset gpt-2 + small llama 58m-parameter llama feature d&s minillm (gu et al., 2024) if dolly dataset gpt2 + opt + llama gpt2 + opt + llama feature d&s self-align (sun et al., 2024b) if human-written principles llama llama expansion + self-knowledge sft self-rewarding (yuan et al., 2024a) if human-written samples llama llama self-knowledge sft + rl star (zelikman et al., 2022) if arithmetic + commonsenseqa + gsm8k gpt-j gpt-j self-knowledge sft llama-gpt4 (peng et al., 2023a) if alpaca dataset gpt4 llama labeling sft reflection-tuning (li et al., 2023e) if alpaca/wizardlm dataset chatgpt llama labeling sft selective reflection-tuning (li et al., 2024d) if alpaca/wizardlm dataset chatgpt llama labeling sft vicuna (chiang et al., 2023) if/md human conversation chatgpt + gpt4 llama labeling sft koala (geng et al., 2023) if/md human conversation chatgpt llama labeling sft baize (xu et al., 2023b) if/md quora + stack overflow chatgpt llama expansion + self-knowledge sft ultrachat (ding et al., 2023b) if/md wikidata + text material + c4 chatgpt llama curation sft orca (mukherjee et al., 2023) if/tp flan-v2 chatgpt + gpt4 llama labeling sft orca2 (mitra et al., 2023) if/tp flan-v2 + few-shot/math/synthetic gpt4 llama labeling sft selfee (ye et al., 2023) if/tp human conv, flan/code/math collection chatgpt llama labeling sft cot-distill (hsieh et al., 2023) if/tp e-snli + anli + cqa + svamp palm t5 labeling sft knowpat (zhang et al., 2023a) if/tp cpkg + qa data chatgpt + chatglm + vicuna-7b llama labeling sft debatune (li et al., 2024e) if/tp controversial topics chatgpt llama labeling sft phi-1 (gunasekar et al., 2023) if/code - gpt3.5 phi-1 curation sft phi-1.5 (li et al., 2023a) if/code 20k topics from web gpt3.5 phi-1 curation + labeling sft sail (luo et al., 2023c) if/rag alpaca data + web content gpt4 llama label sft kard (kang et al., 2023b) if/rag medqausmle chatgpt t5 + opt label sft + d&s self-rag (asai et al., 2023) if/rag open-instruct gpt4 llama labeling sft alignment openchat (wang et al., 2023c) if/preference human conversation chatgpt + gpt4 llama labeling sft + rl zephyr (tunstall et al., 2023) if/preference mixed datasets gpt4 mistral labeling + feedback sft + ro almost (kim et al., 2023a) if/preference human-written prompts llama llama expansion + labeling sft + rl rlcd (yang et al., 2024a) if/preference human-written prompts llama llama labeling sft + rl rlaif (lee et al., 2023a) if/preference human-written prompts palm 2 palm 2 labeling + feedback rl gpt3 reward (kwon et al., 2023) preference human-written prompts gpt3 gpt3 labeling rl ilf (scheurer et al., 2023) preference task-specific datasets gpt3 + feedme gpt3 labeling rl ultrafeedback (cui et al., 2023a) preference mixed datasets gpt4 llama labeling rl constitutional ai (bai et al., 2022a) preference/value human-written prompts self-defined student model self-defined model labeling + expansion + feedback sft + rl sandbox (liu et al., 2023b) value simulationtext-davinci-002/-003 + gpt4 + chatgptllama data curation sft + rl agent toolformer (schick et al., 2023) tool ccnet gpt-j gpt-j labeling sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft gpt4tools (yang et al., 2023b) tool image content chatgpt llama curation + expansion sft toolalpaca (tang et al., 2023a) tool public-apis repository chatgpt llama curation sft toolllm (qin et al., 2023a) tool real-world apis chatgpt llama curation sft mllm-tool (wang et al., 2024) tool huggingface model cards gpt4 llama curation sft fireact (chen et al., 2023b) planning mixed qa dataset gpt4 llama labeling sft agenttuning (zeng et al., 2023a) planning 6 agent tasks gpt4 + chatgpt llama labeling + expansion sft lumos (yin et al., 2023a) planning mixed interactive tasks gpt4 llama labeling sft autoact (qiao et al., 2024) planning mixed qa tasks llama llama labeling sft nlp task specialization auggpt (dai et al., 2023a) nlu amazon/symptoms/pubmed20k dataset chatgpt bert label sft tdg (he et al., 2023b) nlu sst + qqp + mnli gpt3 bert expansion sft sungen (gao et al., 2023a) nlu text classification tasks gpt2 distilbert curation sft udg (wang et al., 2021a) nlu nlu tasks gpt3 bert expansion sft inheritsumm (xu et al., 2023c) nlg pile + arxiv + cnn/dm + wikihow gpt3.5 zcode++ label sft dimsum+ (jung et al., 2023) nlg none gpt2 + ctrl + biogpt t5 curation + self-knowledge sft genie (yehudai et al., 2024) nlg eli5 + asqa + nq + cnn/dm falcon + llama flan + llama label sft gkd (agarwal et al., 2024) nlg/nlu/if xsum+wmt14 en-de+gsm8k+flan2021 t5-xl t5 feature + feedback d&s + rl quill (srinivasan et al., 2022) ir ir datasets t5 4-layer transformer internal knowledge d&s rankvicuna (pradeep et al., 2023a) ir ir datasets chatgpt llama labeling sft rankzephyr (pradeep et al., 2023b) ir ir datasets chatgpt + gpt4 mistral labeling sft ndr (mysore et al., 2023) recommendation recommendation datasets gpt3 mpnet-110m labeling sft instrcutrec (zhang et al., 2023b) recommendation 39 instruction templates chatgpt flan-t5 expansion + self-knowledge sft once (liu et al., 2023c) recommendation recommendation dataset chatgpt llama labeling sft pandalm (wang et al., 2023b) evaluation alpaca data chatgpt llama labeling sft prometheus (kim et al., 2024) evaluation 50 seed rubrics gpt4 llama labeling sft instructscore (xu et al., 2023d) evaluation mixed dataset gpt4 llama labeling sft wizardmath (luo et al., 2023b) math gsm8k + math chatgpt llama expansion + feedback sft + rl mammoth (yue et al., 2023a) math/tp mixed math dataset gpt4 llama labeling sft mixed distill (chenglin et al., 2023) math/tp svamp + gsm8k + asdiv + strategyqa chatgpt llama labeling sft wizardcoder (luo et al., 2023a) code code alpaca data chatgpt starcoder expansion sft magicoder (wei et al., 2023) code existing source codes chatgpt llama curation sft wavecoder (yu et al., 2024) code existing source codes gpt4 llama curation sft code alpaca (chaudhary, 2023) code code instructions chatgpt llama expansion + self-knowledge sft code llama (rozi `ere et al., 2023) code human-written instructions llama llama expansion + self-knowledge sft code clean (jain et al., 2023) code code datasets chatgpt llama labeling sft multi-modality llava (liu et al., 2023e) vision-language coco gpt4 llama labeling sft svit (zhao et al., 2023b) vision-language visual genome + coco gpt4 llama labeling sft lvis-instruct4v (wang et al., 2023e) vision-language lvis gpt4v llama labeling sft llavar (zhang et al., 2023d) vision-language laion gpt4 llama labeling sft macaw-llm (lyu et al., 2023) multiple modalities image/video with caption chatgpt llama labeling sft mimic-it (li et al., 2023f) multiple modalities image/video dataset chatgpt llama labeling sft chatbridge (zhao et al., 2023d) multiple modalities task-specific/multimodal-chat data gpt4 + chatgpt llama labeling sft table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, rag: retrieval-augmented generation, nlu: natural language understanding, nlg: natural language generation, ir: information retrieval, sft: supervised fine-tuning, d&s: divergence and similarity, rl: reinforcement learning, ro: ranking optimization. finally, multi-modality encompasses the knowledge trans- fer from teacher llms to multi-modal models. table 3 summarizes the representative works, encompassing details such as the skills involved, seed knowledge, teacher llm, student model, knowledge elicitation method, and training objectives.4.1 context following this part concentrates on the distillation of context follow- ing skills from llms. this process involves transferring the ability of llms to handle a variety of complex contexts — such as few-shot demonstrations, intricate instructions, dia- logue history, and retrieval-augmented information — into smaller models. many research efforts in this domain aim to imbue smaller models with these sophisticated, context- 15 following capabilities. our discussion here will dissect this facet of skill distillation, categorizing it based on different types of context and elaborating on how each is distilled and incorporated into smaller, efficient models. 4.1.1 instruction following instruction-following capacity enables llms to understand and follow user-given instructions. this ability significantly enhances human-ai interaction, allowing for seamless un- derstanding and execution of tasks as directed by users. a primary method for acquiring this skill involves construct- ing instruction-like prompt-response pairs and employing supervised fine tuning (sft) for model training. data for this purpose can be manually curated by human experts or transformed from existing nlp tasks into instructional formats with templates, such as prefacing machine transla- tion data with ”translate this sentence to spanish:” . however, these approaches have limitations. manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input. llms like gpt-4 offer an efficient alternative for creating diverse and controlled sft data by their capabil- ities of in-context learning and instruction following. most relevant works use openai’s gpt series models to generate prompt-response data pairs and then train the student llms by supervised fine-tuning (wang et al., 2022a; taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; mukherjee et al., 2023; mitra et al., 2023; luo et al., 2023b; peng et al., 2023a). basic instructions. self-instruct (wang et al., 2022a) lever- ages the in-context learning capability of gpt-3 to expand a seed pool of 175 tasks to 52k task-agnostic instructions, ensuring a broad spectrum of general instructions. addi- tionally, a filtering and post-processing stage is introduced to eliminate redundant or similar instructions. notably, through training with this enriched dataset, gpt-3 acquires the ability to follow instructions, enabling it to perform comparably to instructgpt in zero-shot instruction tasks and when provided with expert-written instructions for novel tasks. based on the self-instruct method, taori et al. (2023) train an alpaca model using the llama 7b model on 52k instruction-following demonstrations, generated in a similar style as self-instruct but utilizing the more robust text-davinci-003 model. to enhance the diversity of instruc- tional data, wu et al. (2023c) introduce a technique known astopic-guided instruction generation . this method involves gathering 3.5k common topics from wikipedia to serve as guidance during the generation process. complex instructions. some works promote students to solve more complex instructions (xu et al., 2023a; luo et al., 2023b,a; guo et al., 2023c). according to xu et al. (2023a), in- struction datasets derived from human-written seeds often exhibit low to moderate complexity. to enhance the com- plex instruction-following capabilities of smaller models, wizardlm (xu et al., 2023a) introduces evol-instruct . this method gradually transforms instructions into more com- plex forms through a multi-step evolution process, focusing on both increasing difficulty levels and expanding the di- versity of topics. they conducted four rounds of evolution using the openai chatgpt api, resulting in a dataset of250k complex instructions. subsequently, they trained the llama 7b model, referred to as wizardlm, on this dataset. in the high-difficulty section of test instructions, wizardlm even outperformed chatgpt, achieving a win rate 7.9% higher than chatgpt. zhao et al. (2023e) further conduct preliminary studies revealing the effectiveness of increasing instruction complexity. instruction fusion (guo et al., 2023c) further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to distill specific skills such as coding (luo et al., 2023a) and mathematics (luo et al., 2023b). human instructions. in contrast to works that rely on gener- ating instructions from chatgpt, which may lack diversity and have gaps with real human instructions, vicuna (chiang et al., 2023) and koala (geng et al., 2023) showcase impres- sive performance by using human conversations and natu- ral instructions from community-contributed conversations. these conversations, found in platforms like sharegpt, pro- vide a forum for users to share their interactions with chat- gpt. it’s important to note, however, that models trained on such natural conversations might mimic the style but may not fully capture the reasoning process of the original teacher (gudibande et al., 2023; mukherjee et al., 2023). system instructions. to encourage student models to learn the reasoning process, orca and orca 2 (mukherjee et al., 2023; mitra et al., 2023) enhance the prompt, response data pairs by introducing a system message (e.g., ”explain like i’m five, think step-by-step”) to encourage student mod- els to grasp the reasoning process. this system message prompts gpt-4 to provide explanation traces that eluci- date the teacher’s reasoning process. orca 2 (mitra et al., 2023) further trains the student model to identify the most effective solution strategy for each task, guided by orca’s performance. this approach significantly improves the abil- ity of smaller models to follow instructions that involve reasoning. high-quality instructions. as demonstrated in zhou et al. (2023a) and (li et al., 2024f), the data quality is crucial for instruction following training. ultrachat (ding et al., 2023b) distills large-scale data with high-quality and di- verse instructions from teacher llms by various meta- information. the ultrallama model, fine-tuned on this data, consistently surpasses other open-source models. the phi series models (gunasekar et al., 2023; li et al., 2023a; mar, 2023) prioritize data quality and employ synthetic methods to generate data of “textbook quality” to enhance the learning experience for smaller models. notably, phi exhibits the ability to follow instructions effectively even without specific instruction fine-tuning. what’s particularly remarkable is that phi-2, with just 2.7 billion parameters, outperforms mistral and llama-2 models with 7b and 13b parameters across various benchmark evaluations. improved instructions. another line of work focuses on improving the quality of existing instruction data, including both the improvement of instruction and corresponding response. selfee (ye et al., 2023) utilizes the chatgpt to iter- atively improve the quality of responses. expertllama (xu et al., 2023f) improves the quality of responses by augment- 16 ing vanilla instructions with specialized expert identity descriptions. reflection-tuning (li et al., 2023e) improves both the instruction and response sequentially by reflecting on specific criteria. deita (liu et al., 2023h) proposes to enhance and score instructions in three directions includ- ing complexity, quality, and diversity to get high-quality distillation data. muffin (lou et al., 2023) proposes to scale the instruction according to the input by diversifying these tasks with various input facets. selective reflection- tuning (li et al., 2024d) first involves the student model in the data improvement pipeline with a novel student- selection module, in which the student model is able to decide the data learn from. in summary, distilling instruction data from teachers presents a promising avenue for training cheap and re- producible instruction-following language models. cur- rent small models have made strides in enhancing var- ious aspects of instruction-following ability, like diver- sity, complexity and explanation. however, student mod- els trained on instruction data expanded by chatgpt of- ten mimic chatgpt’s style without replicating its factual accuracy (gudibande et al., 2023). achieving a more ca- pable instruction-following capability requires a stronger teacher llm (gudibande et al., 2023) and access to di- verse, high-quality instruction data, such as the one used in orca (mukherjee et al., 2023; mitra et al., 2023), which incorporates extensive task instructions from the flan 2022 collection (longpre et al., 2023). 4.1.2 multi-turn dialogue while instruction following focuses on single-instance com- mand execution, multi-turn dialogue extends this to com- prehend and maintain context through ongoing interactions. this skill is vital for models to engage meaningfully in human-like conversations and respond coherently over suc- cessive dialogue turns. some works have been dedicated to train to small chat models by distilling multi-turn knowl- edge from teacher llms (chiang et al., 2023; xu et al., 2023b; ding et al., 2023b; li et al., 2023b; wang et al., 2023c; tunstall et al., 2023). sharegpt serves as a platform for users to share their conversations with chatgpt, offering a vast repository of multi-turn conversations readily available. some small chat models are trained using this data to acquire the capability for engaging in multi-turn dialogues (chiang et al., 2023; ye et al., 2023; wang et al., 2023c). for example, vicuna (chiang et al., 2023) is a chat model exclusively trained on sharegpt data. despite its sole training source being sharegpt, vi- cuna achieves a high mt-bench (zheng et al., 2023a) score assigned by gpt-43. in the study conducted by wang et al. (2023c), gpt-3.5 and gpt-4 are employed to generate mixed responses using sharegpt data. they assign higher rewards to responses generated by gpt-4, aiming to incentivize student models to produce high-quality responses. addi- tionally, ye et al. (2023) enhance the quality of multi-turn data from sharegpt by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. 3. mt-bench: a multi-turn question set, where the generations of models are evaluated by llm, like gpt-4.to enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversa- tional datasets through self-chat and using them to train smaller models (xu et al., 2023b; ding et al., 2023b; tunstall et al., 2023). for instance, xu et al. (2023b) initiate their work by using questions sourced from quora and stack overflow as seeds, resulting in the collection of 111.5k dialogues through self-chat. subsequently, they employ parameter- efficient tuning to train a chat model named baize. ding et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions and dialogues from chatgpt. notably, ultrachat encom- passes a wide range of topics and instructions. building upon the ultrachat dataset, they fine-tune a llama model, resulting in the creation of a powerful chat model known as ultrallama. ultrallama consistently outperforms other open-source chat models, including vicuna and baize. fur- thermore, ultrachat is employed in conjunction with an ai preference-aligned chat model named zephyr (tunstall et al., 2023). zephyr enhances intent alignment through the application of distilled direct preference optimization (ddpo). 4.1.3 rag capbility llms are known to lack the ability to utilize up-to-date knowledge, and often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge. retrieval-augmented generation (rag) is a promising technique to decrease this issue. handling the augmented context of retrieved information is also a non- trivial skill of llms. several approaches to distill rag capabilities have been proposed (kang et al., 2023a; luo et al., 2023c; asai et al., 2023). sail (luo et al., 2023c) starts by retrieving search results for each training case using search apis, creating search- augmented instructions that include both the instruction and grounding information. to encourage the language model to prioritize informative retrieval results, they input each retrieved passage along with the ground truth response into the entailment model to label each retrieval result for relevance. subsequently, the search-augmented instructions and relevance labels are fed into teacher llms (like gpt- 4) for generating responses. following fine-tuning on this training set, the student model becomes proficient at de- noising search results and generating accurate responses. kard (kang et al., 2023b) distills rationales rfrom the teacher llm in response to questions x. these rationales are then utilized to train two models: a student lm and a reranker. for training the student lm, the rationales serve as a means to retrieve relevant knowledge d, and the student lm is subsequently fine-tuned using the rationales along- side questions and knowledge. however, during inference, only questions are available. to address this, the reranker is trained to mimic how the retriever scores passages with the rationale by minimizing the kl divergence between retriever (d|r)andreranker (d|x). however, the integra- tion of a fixed number of passages in language models, without considering their necessity or relevance, can reduce versatility and lead to the generation of unhelpful responses. to equip student lms with adaptive rag capabilities, self- 17 rag (asai et al., 2023) distills this adaptive ability from teacher llms into a small critic model. this critic model determines whether retrieval is necessary and evaluates the quality of the retrieved results by generating ‘reflection to- kens.’ for instance, self-rag initiates the retrieval operation when generating the reflection token retrieve . to distill this critic data, gpt-4 is prompted to assess the need for retrieval using few-shot demonstrations i, the task input x, and output yto predict a reflection token ras follows: p(r|i, x, y ). 4.2 alignment 4.2.1 thinking pattern most existing methods mainly focus on directly aligning the direct responses of the student models to the responses of teacher models (taori et al., 2023). though effective, these models might suffer the problems that they tend to learn to imitate the response style of the teacher models, but not the reasoning process (mukherjee et al., 2023). thus in order to better distill from the teacher models, methods are proposed that not only imitate the pure responses but some novel thinking patterns (ye et al., 2023; mukherjee et al., 2023; mitra et al., 2023; wang et al., 2023d; cheng et al., 2023; zhang et al., 2023a). motivated by the effectiveness of llms in generat- ing their own feedback without relying on external mod- els (schick et al., 2022; madaan et al., 2023; saunders et al., 2022), selfee (ye et al., 2023) proposes to train a model that has been fine-tuned to continuously revise its own answer until it provides a high-quality response in a single inference. during training, it utilizes both the final response and feedback chain as the fitting target. this pat- tern, response with the revision process, shows a promising performance gain. following selfee, reflection-tuning (li et al., 2023e, 2024d) also utilizes the reflection process as the learning pattern. noticing the lack of reasoning imitation of the previous methods, orca (mukherjee et al., 2023) first proposes explanation tuning, which aims to learn the reasoning steps, including explanation traces, step-by-step thought processes, and other complex instructions, from the teacher model, rather than just the vanilla styles. extensive experiments verify the effectiveness of distilling with this thinking pattern. the following orca2 (mitra et al., 2023) further presents to equip the student models with the ability to utilize different solution strategies for different tasks, mo- tivated by the capability discrepancies between the smaller and larger models. by employing this training pattern, the student models are able to gain a better reasoning ability. be- sides learning with the corresponding revision or reflection process, another thinking pattern that recently appeared is generating both responses and preferences. zhang et al. (2023a) propose to learn both the knowledge and corre- sponding preference for domain-specific qa with llms. recently, debatune (li et al., 2024e) proposes to improve the controllability of llms in generating statements on controversial topics. by engaging two agents in a structured multi-round debate on controversial topics, salient and in- depth statements can be obtained and further distilled into the student models.4.2.2 preference the previously mentioned methods primarily focus on the basic capability of student models to produce outcomes that are strictly accurate but may not align with human preferences, reaching alignment at this level enables these models to aid in various tasks without meeting higher-level demands. early methods mainly utilize human feedback for the alignment of human preferences (ziegler et al., 2019; stiennon et al., 2020; wu et al., 2021; ouyang et al., 2022; bai et al., 2022b; k ¨opf et al., 2023; yuan et al., 2023b). however, obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon et al., 2023; scheurer et al., 2023; kim et al., 2023a; roit et al., 2023; yang et al., 2024a; lee et al., 2023a; tunstall et al., 2023; cui et al., 2023a; wang et al., 2023f). the concept of rlaif, introduced by bai et al. (2022a), involves the integration of preferences labeled by llms with those labeled by humans. this approach is designed to simultaneously optimize two key objectives: ensuring the helpfulness of the output and minimizing any potential harm, making the responses of llms more aligned with human preferences. kwon et al. (2023) develop a proxy reward function using llms like gpt-3, which is created by first providing the llm with a description of the behaviors desired by the user, along with a small number of examples. the llm then produces rewards by evaluating how closely the outputs of a model align with the provided descrip- tions, essentially measuring their relevance to the estab- lished ground truth. scheurer et al. (2023) propose imitation learning from language feedback, in which a language model is utilized to improve various outputs generated by a model. this refinement is based on a reference provided by a human. following this process, the most effectively refined output is chosen to be used in further supervised fine-tuning. as outlined by kim et al. (2023a), almost in- volves condensing human preferences into a set of heuristic guidelines. an example of such a rule is the idea that larger llms that utilize more comprehensive and higher-quality prompts are likely to yield superior responses. based on these established guidelines, comparison data is generated using responses from llms of different sizes and with varying prompts. this data is then used to train a reward model. yang et al. (2024a) propose reinforcement learning from contrast distillation, which aims to align language models without relying on human feedback. this approach involves training a preference model using simulated pairs of preferences, including both high-quality and low-quality examples which are generated through contrasting prompts, positive and negative. lee et al. (2023a) further highlight the effectiveness of rlaif. this work proposes that rlaif not only matches but in some cases surpasses rlhf, and interestingly, rlaif can also enhance the performance of supervised fine-tuning. another notable discovery is that directly prompting the llm for reward scores during reinforcement learning can be more effective than the conventional approach of training a reward model based on llm preferences. wang et al. (2023f) propose conditioned-rlft, which treats different data sources as coarse-grained reward labels and develops 18 a class-conditioned policy to effectively utilize the varying qualities of data, which is a reinforcement learning-free supervised learning approach. cui et al. (2023a) propose a large-scale, high-quality, and diversified preference dataset labeled by gpt4 for comprehensive feedback. tunstall et al. (2023), by proposing distilled direct preference optimiza- tion (rafailov et al., 2023) on ultrafeedback, obtaining a small by powerful llm. 4.2.3 value attaining alignment with human preferences allows large models to optimize human satisfaction by operating in a manner that aligns with human preferences. however, to establish trustworthy llms, the notion of ’aligning llms with human values’ is proposed and the key principles of alignment are often summarized as the “hhh” criteria: helpful, harmless, honest (weidinger et al., 2021; askell et al., 2021). numerous methods have been undertaken for building trustworthy llms. however, due to the intrinsic difficulty of this aim, which is still an unsolved problem for proprietary models (sun et al., 2024a), most existing methods rely on constructing high-quality human prefer- ence datasets (ji et al., 2023b; solaiman and dennison, 2021; bai et al., 2022b; qiu et al., 2022; kiesel et al., 2022; liu et al., 2022a), utilizing human-written rules as constrains (glaese et al., 2022; sun et al., 2023b, 2024b), etc. for detailed progress on trustworthy llms, please further refer to yao et al. (2023a); liu et al. (2023i); sun et al. (2024a). though slightly under-explored, aligning llms with human values by distilling is still possible (bai et al., 2022a; cui et al., 2023a; yang et al., 2024a; sun et al., 2024b). for instance, bai et al. (2022a) propose rlaif, utilizing ai- generated labels to interactively improve both helpfulness and harmlessness. sun et al. (2024b) prompt the student model with 16 principles as guidelines for generating help- ful, ethical, and reliable responses. similarly, both harmless and harmful generations could be elicited by modifying the prompts, and then are used to train the preference model (yang et al., 2024a). cui et al. (2023a) utilize gpt- 4 to rank generations regarding helpfulness, truthfulness, and honesty. liu et al. (2023b) advance the alignment of llms with societal values by incorporating simulated social interactions into the training process. this approach encom- passes a range of elements, including demonstrations that are both in alignment and in conflict with social norms, as well as collective ratings, in-depth feedback, and responses that are revised iteratively. 4.3 agent 4.3.1 tool using while recent llms have shown proficiency in solving var- ious tasks, they still tend to make mistakes when handling large numerical values or executing intricate mathematical calculations (qian et al., 2022; she et al., 2023; manikandan et al., 2023; liang et al., 2023b; mialon et al., 2023). thus equipping llm agents with the capability to utilize tools has been increasingly focused on. commonly used methods mainly relied on human-curated data for training (parisi et al., 2022; nakano et al., 2022; qin et al., 2023c; song et al., 2023b) or prompt designing(cai et al., 2023; shenet al., 2023a; hao et al., 2024). recently, distillation-based methods are also proposed (schick et al., 2023; zhang, 2023; patil et al., 2023; tang et al., 2023a; qin et al., 2023a; yuan et al., 2023a; gao et al., 2023b; wang et al., 2024; shen et al., 2024; yuan et al., 2024b). toolformer (schick et al., 2023) utilizes a self-supervised manner, avoiding large human annotations, to obtain the most required apis to use and further distill this capability to the model itself. the performance of the gpt-j-based toolformer surpasses opt (66b) (zhang et al., 2022) and gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed to enhance llms with graph reasoning skills using exter- nal graph reasoning api tools by adopting chatgpt to annotate and augment a larger graph reasoning statement dataset for training. gorilla (patil et al., 2023) addresses the limitations of current llms in generating accurate input arguments and reduces the problem of ”hallucination” or generating incorrect api usage and it collects thousands of models from platforms like huggingface and torch hub as the api calls and utilizes gpt4 to generate synthetic instruction data for training. gpt4tools (yang et al., 2023b) introduces to enable open-source llms like llama and opt to use multimodal tools, a capability previously limited to advanced proprietary models like chatgpt and gpt-4. the approach involves generating an instruction-following dataset by prompting an advanced teacher model with mul- timodal contexts, using the low-rank adaptation optimiza- tion. toolalpaca (tang et al., 2023a) proposes a framework aimed at enhancing the tool-use capabilities of compact language models for embodied intelligence. it creates a dataset with 3938 instances from over 400 real-world tool apis across 50 categories and utilizes chatgpt to generate documentation for each prompt for later training. toolllm (qin et al., 2023a) proposes a comprehensive framework for enhancing llms with tool-use proficiency, focusing on data creation, model training, and evaluation by distilling from chatgpt. their toolllama shows impressive performance in executing complex instructions and handling new apis, rivaling chatgpt. craft (yuan et al., 2023a) builds a general tool creation and retrieval framework, which uti- lizes gpt4 to generate code snippets as the created tools. during the inference, other small llms could select and retrieve from the generated code snippets to execute or generate other methods conditioned on the given snippets. confucius (gao et al., 2023b) introduces a tiered training strategy for llms to master tool usage through a graduated curriculum and an innovative method called iterative self- instruction from introspective feedback (isif) for dynamic dataset enhancement to handle complex tools. mllm-tool (wang et al., 2024) is a multi-modal tool agent capable of interpreting instructions embedded in visual or audio content through the integration of multi-modal encoders with open-source large language models. as a trainable method, the initial instruction-answer pairs are generated by utilizing gpt4. shen et al. (2024) demonstrate that small llms are weak tool learners and proposes a multi-llm framework that decomposes the tool-use ability of a single model into a planner, caller, and summarizer for the tool using, leading to a supreme performance. the two-stage 19 training strategy introduced by this work is powered by chatgpt and gpt4 for collecting execution trajectories for the training set. yuan et al. (2024b) notice the potential issue of the current lengthy tool documentation, which hinders llms from understanding how to utilize a tool, thus proposing easytool to purify the important infor- mation from extensive documentation. the ground truth summarization of the training documents is obtained by using chatgpt. 4.3.2 planning another important aspect for llm agents is the ability to decompose high-level tasks to a chosen set of actionable steps (huang et al., 2022b), which is especially useful when acting in interactive environments. huang et al. (2022b) first demonstrate that llms can generate plausible goal-driven action plans without training, introduces non-invasive tools to enhance model executability, and assesses these methods through human evaluation to balance executability and semantic accuracy. most existing methods utilize prompting strategies for task planning (singh et al., 2022; zhou et al., 2023b; song et al., 2023c; wang et al., 2023g; yao et al., 2023b; liu et al., 2023j; hao et al., 2023; hu et al., 2023a), or building human-curated data for training (lin et al., 2023a; valmeekam et al., 2023). recently, there have also been some distilling methods emerging (chen et al., 2023b; zeng et al., 2023a; yin et al., 2023a; qiao et al., 2024; kong et al., 2023). fireact (chen et al., 2023b) introduces an innovative ap- proach for refining llms. this method involves fine-tuning smaller-scale llms using agent trajectories that are derived from a variety of tasks and prompting techniques. applying this method with trajectories generated by gpt4 has been shown to consistently enhance performance. agenttuning (zeng et al., 2023a) aims to enhance the performance of llms in executing agent tasks without sacrificing their wide-ranging capabilities. by utilizing a new dataset called agentinstruct, which includes high-quality interaction tra- jectories, it applies a hybrid instruction-tuning approach that merges these trajectories with general domain instruc- tions. lumos (yin et al., 2023a) pertains to a novel frame- work designed to train agents using a unified data format and modular architecture based on open-source llms. this system comprises three key modules: planning, grounding, and execution, enabling the decomposition of tasks into subgoals and actionable steps. tptu-v2 (kong et al., 2023) focuses on improving the task planning and tool usage abili- ties of llms in real-world scenarios, by utilizing data gener- ated by human experts or llms. it introduces a framework comprising three components: an api retriever, an llm finetuner, and a demo selector. autoact (qiao et al., 2024) proposes an agent learning framework that does not require large-scale annotated data or synthetic trajectories from high-resource models like gpt-4. instead, it uses a self- instruct method to generate its own planning trajectories with limited initial data. it then applies a division-of-labor strategy, creating sub-agents specialized in different aspects of the task completion process. distillation also works out for the training of embodied multi-modal agents (sumers et al., 2023; yang et al., 2023c; ma et al., 2023a; du et al., 2023a; sumers et al., 2023). for instance, sumers et al. (2023) aim to enhance the ability ofai agents to follow instructions by using pretrained vision- language models to provide supervision for understanding and acting upon language within their operational environ- ment, leveraging model distillation and hindsight experi- ence replay to teach them contextually relevant interactions in a simulated 3d setting. emma (yang et al., 2023c) evalu- ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using imitation learning, guided by an expert language model (like chatgpt), which operates in a corresponding text- based simulation, focusing on the same tasks. 4.4 nlp task specialization nlp tasks often grapple with challenges like data scarcity, interpretability issues, privacy concerns, and noisy data. the “knowledge” section of our survey illustrates various methods for distilling knowledge from llms, effectively setting the stage for student models to adapt to a range of nlp tasks. this knowledge provides supervision for the training of student models through information aug- mentation (e.g., cot and explanation), data augmentation, and semantic representation. by transferring the distilled knowledge from llms, student models can better handle diverse nlp challenges, improving task performance and addressing data limitations more robustly. 4.4.1 natural language understanding natural language understanding (nlu) is a fundamen- tal nlp task that involves comprehending and interpret- ing human language. the knowledge distilled from llms, such as through data labeling or augmentation, is typi- cally transferred into encoder-based language models like bert (vaswani et al., 2017) and roberta (liu et al., 2019). regarding the task of classification, certain studies have been noteworthy (dai et al., 2023a; gilardi et al., 2023; he et al., 2023b; gao et al., 2023a; chenglin et al., 2023; li et al., 2023g). auggpt (dai et al., 2023a) focuses on both general and clinical domain text classification. to address the limitations of small-scale clinical datasets, which often lack expert annotation and are subject to stringent privacy regulations, auggpt utilizes knowledge from teacher llms to rephrase each sentence in the training samples. this process creates multiple conceptually similar but seman- tically distinct samples, enhancing the dataset’s richness and diversity. another approach is demonstrated by gilardi et al. (2023), who employ chatgpt as an annotator to cate- gorize inputs. this method has been shown to outperform crowd-workers in several tasks, including relevance, stance, topics, and frame detection. furthermore, he et al. (2023b) propose targeted data generation (tdg), a novel approach for identifying challenging subgroups within a dataset. tdg leverages llms, along with human-in-the-loop, to generate new data specifically tailored for these subgroups, thereby enriching the dataset and improving model performance in sentiment analysis and natural language inference tasks. to facilitate the clinical information extraction task, tang et al. (2023b) elicit diverse samples from llms by providing examples and different seeds of clinical entities, i.e. the curation manner. 20 several studies have also focused on multiple nlu tasks (ding et al., 2023a; he et al., 2023a; wang et al., 2021a; he et al., 2022; ye et al., 2022; meng et al., 2022). for example, he et al. (2023a) utilize the knowledge in gpt-3.5 to annotate inputs with labels and explanations for various nlu tasks, including user input and keyword relevance assessment, boolq, and wic. wang et al. (2021a) employ few-shot prompts to expand high-quality training data using gpt-3, i.e. the expansion manner. beyond merely employing a single approach to elicit nlp task knowledge, ding et al. (2023a) explore a combination of labeling ,ex- pansion , and curation methods to extract knowledge from gpt-3 for distilling data for both sequence- and token-level nlp tasks. 4.4.2 natural language generation natural language generation (nlg) is a key aspect of eval- uating the capabilities of llms, encompassing tasks such as summarization, machine translation, and other open-ended text generation tasks. known for their potent generative abilities and creativity, llms excel in these areas, making them prime sources for distilling knowledge into student models tailored for nlg tasks (xu et al., 2023c, 2024b; ramnath et al., 2023; agarwal et al., 2024). additionally, the knowledge distilled from llms can be effectively used for nlg task-specific data augmentation (jung et al., 2023; wang et al., 2021b; guo et al., 2023a; yang and nicolai, 2023; wang et al., 2023h; yang et al., 2023d). while the previous sections have focused on the works about open- ended generation and multi-turn dialogue, this part will specifically highlight the distillation techniques relevant to other nlg tasks. although automatic metrics often favor smaller, fine- tuned models in summarization tasks, human evaluators tend to prefer the summaries generated by llms. address- ing this discrepancy, xu et al. (2023c) develop a student sum- marization model by distilling a gptsumm dataset, which comprises over 4 million paragraph-summary pairs gener- ated by querying gpt-3.5. in a different approach, jung et al. (2023) introduce ‘impossible distillation,’ a method that creates high-quality summarization-specific dataset from weak teacher llms. this method involves training a stu- dent model on the generated dataset and enhancing its capabilities through self-knowledge. turning to the task of machine translation, where creating parallel corpora is tra- ditionally expensive and time-consuming, yang and nicolai (2023) propose a three-step distillation process. this process involves generating seeds of verbs and nouns, forming sen- tences, and then translating these sentences. their findings suggest that while the distilled dataset may lack diversity, it effectively improves the translation signal for training student translation models. to distill high-quality content- grounded data automatically, genie (yehudai et al., 2024) proposes a general methodology containing three key steps: (a) preparation of the content, (b) distillation of responses from a teacher llm corresponding to the content, and (c) filtering mechanism to ensure the quality and faithfulness of the generated data. genie demonstrates that student models trained through this distilled data can match or even surpass models trained on human-generated data.4.4.3 information retrieval information retrieval (ir) represents a crucial branch of computer science, focused on efficiently retrieving infor- mation relevant to user queries from extensive reposito- ries (cai et al., 2022; liu et al., 2022b; feng et al., 2023; shen et al., 2023b). a typical ir system encompasses three main components: the query rewriter, the retriever, and the reranker. recent studies have highlighted the effective- ness of employing llms in ir systems, e.g. in enhancing the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in ir systems faces challenges, primarily due to their slower generation speed, which conflicts with the low-latency re- quirements of ir tasks (sun et al., 2023a). as a result, the kd of llms emerges as a more promising approach for ir, offering a way to infuse the distilled knowledge from llms into various stages of the ir pipeline without compromising on speed. there has been a significant body of work demonstrating how knowledge distilled from llms can benefit each component of the ir system, including the query rewriter (srinivasan et al., 2022; ma et al., 2023c), the retriever (dai et al., 2023b; sachan et al., 2022, 2023; schick and sch ¨utze, 2021; meng et al., 2023; peng et al., 2023b), and thereranker (bonifacio et al., 2022; sun et al., 2023a; pradeep et al., 2023a,b; saad-falcon et al., 2023; ferraretto et al., 2023; jeronymo et al., 2023; sun et al., 2023c). query rewriter. the query rewriter (qr) is a pivotal com- ponent in ir systems, tasked with enhancing the precision and expressiveness of user queries by refining or modifying the initial query to more accurately align with the user’s information needs. one notable approach is quill (srini- vasan et al., 2022), which introduces a two-stage distillation method for query intent understanding. initially, a retrieval- augmented llm, serving as the ‘professor,’ is distilled into a non-retrieval augmented teacher llm, aiming to bolster its understanding capabilities. subsequently, this enhanced teacher llm is distilled into a final student model using a large dataset, further refining the process. incorporating the qr into ir systems, ma et al. (2023c) develop a ’rewrite- retrieve-read’ framework. this process begins with an llm rewriting the queries via prompting, followed by a retrieval-augmented reading stage. to integrate the rewrit- ten queries effectively into the ir system, the knowledge gleaned from the llm is distilled into a compact student rewriter. this rewriter is then fine-tuned using feedback from the llm reader through reinforcement learning. retriever and reranker. in ir systems, the retriever is designed to efficiently locate the top-k relevant texts from a large corpus. it encodes both queries and documents into vector representations and performs retrieval by computing the dot product between these vectors. the reranker further refines the order of the retrieved documents to improve the overall quality of the output. this is achieved in two primary ways, including pointwise reranker and listwise reranker . pointwise reranker takes both the query and a single candidate document as input to directly generate a relevance score. listwise reranker directly reorders a list of input documents in terms of their relevance. 21 retriever and pointwise reranker. for the retriever and pointwise reranker, a common application of kd from llms is the generation of pseudo-queries for given documents. this approach aims to expand the pairwise data, enhancing the training of dense retrievers or rerankers. for example, inpars (bonifacio et al., 2022) utilizes gpt-3 to generate multiple pseudo-queries for an unlabeled document. to ensure the relevance of these queries, the system filters them based on the highest log probabilities of generating a query conditioned on the documents. subsequently, inpars fine-tunes a reranker based on monot5 (raffel et al., 2020). another similar approach, promptagator (dai et al., 2023b), introduces a few-shot dense retrieval method that leverages a small number of demonstrations from the target domain for pseudo-query generation. diverging from the reliance on unlabeled documents, sachan et al. (2022) distill knowl- edge from gpt-4 to curate diverse synthetic data for text embedding tasks across nearly 100 languages. they fine- tune powerful decoder-only llms, such as mistral-7b (jiang et al., 2023a), on this synthetic data using standard con- trastive loss. remarkably, this method demonstrates strong performance on text embedding and multilingual retrieval benchmarks without any labeled data. beyond generating pseudo-queries, teacher llms can also be employed to gen- erate relevance scores as soft labels. these scores are used to train the retriever by minimizing the kl-divergence loss between the teacher and student distributions, as explored by sachan et al. (2023). listwise reranker. a distinct set of studies focuses on listwise reranking, where its advantage lies in compar- ing multiple documents simultaneously to determine the optimal reorder. rankgpt (sun et al., 2023a) leverages gpt-4 to generate permutations for a group of candidate passages. to distill this listwise ranking knowledge into a pointwise student reranker, various training loss functions are employed, such as listwise cross-entropy (bruch et al., 2019), ranknet (burges et al., 2005), and lambdaloss (wang et al., 2018). building upon rankgpt’s framework, rankvi- cuna (pradeep et al., 2023a) and rankzephyr (pradeep et al., 2023b) further refine this approach by directly fine- tuning a listwise reranker using teacher-generated textual permutations. this enables the student reranker to produce sequences of ranked results directly, bypassing the interme- diate step of calculating individual relevance scores. 4.4.4 recommendation recommender systems are integral to enhancing user ex- perience in various online services, providing personalized content based on user preferences and behaviors. many works have demonstrated that llms could be directly used as recommenders without fine-tuning (wang et al., 2023i; dai et al., 2023c) or generate auxiliary textual features to benefit recommender systems (xi et al., 2023; ren et al., 2023; wei et al., 2024). (wang et al., 2023j; ren et al., 2023; wei et al., 2024). however, the real-time nature of online rec- ommender systems demands rapid response times, posing a challenge with the inherent inference latency associated with llms. to address this, several studies have explored ways to distill and integrate the knowledge from llms into recommender systems, thereby leveraging their advanced capabilities while mitigating latency issues for efficient real-time recommendations (mysore et al., 2023; zhang et al., 2023b; liu et al., 2023c). mysore et al. (2023) tackle data scarcity in narrative- driven recommendation (ndr), where users provide de- tailed descriptions of their preferences. they utilize gpt-3 to create synthetic narrative queries from user-item interac- tions via few-shot prompting, then distill this data into re- trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized content, aiding the training of content-based recommenda- tion models. to bridge the gap between language models and recommender systems, some research views behavior modeling as an extension of language modeling (cui et al., 2022; liu et al., 2023k). instructrec (zhang et al., 2023b), for instance, interprets recommendation as instruction fol- lowing. they use chatgpt to distill a wealth of user- personalized instruction data reflecting diverse preferences and intentions based on real historical interactions. this data is then used to fine-tune a 3b student language model specifically for recommendation purposes. 4.4.5 text generation evaluation text generation evaluation, i.e. nlg evaluation, focuses on assessing the quality of generated content. unlike tradi- tional nlg evaluation metrics like bleu (papineni et al., 2002) or rouge (lin, 2004), which primarily rely on surface-level text comparisons, llms, trained on extensive corpora and refined through techniques like rlhf, offer a more nuanced and human-aligned assessment. this so- phistication has led to the increasing use of llms in nlg evaluation (detailed further in (li et al., 2024b)). through kd of llms, student evaluators could enhance inference efficiency and achieve more flexible and highly customized evaluation (wang et al., 2023b; kim et al., 2024; xu et al., 2023d; jiang et al., 2023c; li et al., 2024a). pandalm (wang et al., 2023b) concentrates on a pairwise evaluator designed to compare two pieces of generated content. it utilizes a teacher llm (gpt-3.5) to judge which response is better for a given instruction and input, provid- ing reasons for its decision. addressing the need for cus- tomized and flexible criteria to meet realistic user demands, prometheus (kim et al., 2024) distills gpt-4 to construct a training dataset that includes reference answers and a vari- ety of customized scoring rubrics. this dataset is then used to tune llama for evaluating model-generated responses. instructscore (xu et al., 2023d) takes a more fine-grained ap- proach by using gpt-4 to create detailed analysis data. this data is employed to tune llama, enabling it to perform error analysis on generated texts compared to reference texts. the system further refines its evaluation capabilities through self-training with real model-generated response- reference pairs. for reference-free evaluation across diverse domains, tigerscore (jiang et al., 2023c) samples data from a variety of text generation datasets, such as summariza- tion, translation, and data-to-text. it distills error analysis knowledge from gpt-4 and uses this to fine-tune llama for more nuanced evaluation. lastly, to adapt evaluation to real-world scenarios beyond conventional nlp tasks, auto-j (li et al., 2024a) collects real-world user queries and their evaluations from a teacher llm. this massive dataset 22 of real-world scenarios is then used to distill evaluation knowledge into llama through fine-tuning, enhancing its practical applicability. 4.4.6 code llms, trained on extensive corpora containing code, are highlighted for their proficiency in code-related tasks. their capabilities extend beyond direct code generation to include the provision of external knowledge and data, which is crucial in distilling their expertise into smaller, more effi- cient models. several works have successfully distilled code knowledge from llms into those compact and specialized code models (chaudhary, 2023; rozi `ere et al., 2023; gu- nasekar et al., 2023; wei et al., 2023; chen et al., 2023a; liu et al., 2023d; yu et al., 2024; jain et al., 2023; su and mcmillan, 2023; guo et al., 2023d). a primary focus in these student code models is on code generation, a task of both common utility and practical significance. for instance, code alpaca (chaudhary, 2023) fine-tunes llama using self-instruct with chatgpt-distilled instructions specifically for code generation tasks. similarly, code llama-instruct (rozi `ere et al., 2023) is fine-tuned via self-instruct, prompting llama-2 (touvron et al., 2023) with coding problems, and further refined with unit tests. phi- 1 (gunasekar et al., 2023) aims to enhance the quality of dis- tilled code data by extracting “textbook quality” data from a teacher llm, incorporating python textbook and exercise data. magicoder (wei et al., 2023) addresses potential biases in teacher llms by referencing a wealth of open-source code, yielding more diverse and grounded data for code generation. to consider the capability of the student model and leverage the feedback of the teacher, persd (chen et al., 2023a) introduces a personalized distillation method where the teacher llm refines the student’s generated code based on the execution feedback of the executor. however, these models primarily target the code gener- ation task, lacking generalizability across a broader range of code-related tasks. to address this issue, mftcoder (liu et al., 2023d) utilizes self-instruct to distill diverse code data from teacher llms for various tasks, such as code comple- tion and text-to-code generation, training a student model via multi-task learning. wavecoder (yu et al., 2024), in contrast, creates a comprehensive instruction tuning dataset covering four universal code-related tasks distilled from gpt-3.5-turbo. wavecoder first selects a diverse coreset of raw data using the kcentergreedy (sener and savarese, 2018) clustering method, then employs the teacher llm for generating task definitions and outputs. the teacher model also plays a role in evaluating and filtering this data. notably, wavecoder demonstrates superior generalization across different code-related tasks compared to other open- source models. 4.5 multi-modality multimodal large language models (mllms) surpass tra- ditional language-only llms by understanding and pro- cessing information across multiple modalities, more closely mirroring human perception and enabling a broader range of real-world applications. there is a growing trend towards developing mllms that follow multimodal instructions,facilitating tasks with enhanced levels of interactivity. to ad- dress the scarcity of multimodal instruction-following data and to harness the commonsense and world knowledge embedded in teacher llms, numerous studies have focused on multimodal knowledge distillation from llms (liu et al., 2023e; zhao et al., 2023b; wang et al., 2023e; chen et al., 2023c; park et al., 2023; pi et al., 2023; zhao et al., 2023c; liu et al., 2023f; wu et al., 2023b; luo et al., 2023d; jiang et al., 2023d; li et al., 2023c; xu et al., 2023e). vision-language. in the vision-language domain, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, including captions and bounding boxes, and distills gpt-4 for generating new data in the context of seed examples. this approach creates a llava-instruct-150k dataset, which serves as the foundation for further developments like llava-1.5 (liu et al., 2023l) and gpt4roi (zhang et al., 2023e), enhancing the instruction- following capabilities of mllms. to expand the dataset’s scale, svit (zhao et al., 2023b) introduces a 4.2 million image dataset, distilled from gpt-4 by leveraging manual image annotations. it employs a novel data recipe to select an informative, diverse, and balanced subset of training data. lvis-instruct4v (wang et al., 2023e) leverages gpt- 4v (openai, 2023), a powerful large multimodal model, as a teacher to distill a more accurate and context-aware instruction-following dataset, focusing on fine-grained understanding. further advancements include integrating specific region referencing in image-based instruction following. for instance, shikra (chen et al., 2023c) uses gpt-4 to distill referential question-answer pairs from the flickr30k (plummer et al., 2015) dataset, enhancing the understanding of referential regions within images. lskd (park et al., 2023) introduces localized references to specific image regions, prompting the teacher llm to generate commonsense inferences about these areas. to enhance the visual instruction tuning pipeline with text-rich images, llavar (zhang et al., 2023d) employs the text-only gpt-4 as a teacher, using recognized texts and image captions to generate 16k conversation pairs for text-rich images. the resultant student mllm demonstrates enhanced interaction skills in content that combines both text and imagery. multiple modalities. to extend knowledge distillation of llms to encompass more modalities, such as audio and video, several innovative approaches have been in- troduced. these methods typically involve transforming these modalities into a textual format comprehensible to teacher llms, followed by the distillation of the teacher. macaw-llm (lyu et al., 2023) leverages gpt-4 to generate instruction-response pairs corresponding to the content of images or videos. mimic-it (li et al., 2023f) aims to broaden the scope to language, image, and video understanding, creating a substantial dataset with 2.8 million multimodal instruction-response pairs distilled from chatgpt. chat- bridge (zhao et al., 2023d), on the other hand, represents a novel approach in multimodal language modeling. it translates various non-textual modalities into text, combin- ing fine-grained and global descriptions. this information 23 verticalization distillationlaw lawyerllama (huang et al., 2023b), lawgpt (cui et al., 2023b), fuzi (wu et al., 2023d) medical and healthcarehuatuogpt (zhang et al., 2023c), huatuogpt-ii (chen et al., 2023d), doctorglm (xiong et al., 2023), alpacare (zhang et al., 2023f), huatuo (wang et al., 2023a), chatdoctor (li et al., 2023i), medalpaca (han et al., 2023), pmc-llama (wu et al., 2023e), disc-medllm (bao et al., 2023a) finance xuanyuan (zhang and yang, 2023) sciencedarwin (xie et al., 2023a), sciglm (zhang et al., 2024), wizardmath (luo et al., 2023b), mammoth (yue et al., 2023a), tora (gou et al., 2024), astrollama-chat (perkowski et al., 2024), g-llava (gao et al., 2023c), gimlet (zhao et al., 2023f), llm-prop (rubungo et al., 2023), instructmol (cao et al., 2023a), prot2text (abdine et al., 2023), biomedgpt (luo et al., 2023e), xtrimopglm (chen et al., 2024e), k2 (deng et al., 2023), oceangpt (bi et al., 2023), marinegpt (zheng et al., 2023b), geogalactica (lin et al., 2024), miscellaneous educhat (dan et al., 2023), owl (guo et al., 2023b) fig. 7: taxonomy of verticalization distillation. is then used to distill responses from chatgpt or gpt-4 through an in-context learning process, effectively bridging the gap between different modalities. others. beyond distilling instruction-following data, sev- eral methods have emerged that concentrate on harnessing different aspects of knowledge from llms. for instance, emma (yang et al., 2023c) trains an mllm to act as an embodied reflex agent within a visual environment. it achieves this by distilling gpt-4’s skills in a parallel textual world, generating actions and providing reflective feedback. silkie (li et al., 2023h) takes a unique approach by distilling preferences from gpt-4v , focusing on criteria like helpfulness and visual faithfulness. ha et al. (2023) represent another innovative direction, where it generates, labels, and distills diverse robot-centric exploration experiences by llms into a multi-task visuo-linguo-motor policy. 5 d omain -specified vertical distillation',\n",
       " '5. domain-specified vertical distillation': 'this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc. it delves into cus- tomizing distilled llms for these fields, showing its signifi- cant role in enhancing domain-specific ai applications. the taxonomy of these works is shown in figure 7. 5.1 law law holds a crucial position in molding societies, over- seeing human interactions, and ensuring justice prevails. informed decision-making, legal interpretation, and the pro- vision of legal advice by professionals hinge on precise and current information. legal intelligent applications in different scenarios usually require combinations of multiple fundamental capabilities of legal text retrieval, understand- ing, reasoning and generating (zhang et al., 2023g; sun, 2023; lai et al., 2023). addressing the intricacies of legal ter- minology, subtle interpretations, and the constant evolution of legislation presents distinctive challenges that demand customized resolutions. to handle the above challenges, several studies have investigated the customization of llms for intelligent legal services (cui et al., 2023b; yue et al., 2023b; huang et al., 2023b; wu et al., 2023d). this involves a continued pre-training process on extensive legal corpora, followed by fine-tuning with self-constructed instructions or augmented data using advanced llms.huang et al. (2023b) have unveiled a chinese legal large model named lawyerllama. the model undergoes an initial pre-training phase on an extensive legal corpus, systematically assimilating knowledge of the chinese legal system. subsequently, fine-tuning occurs through the analy- sis of objective questions from the chinese national judicial examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge to specific scenarios. cui et al. (2023b) present lawgpt, built upon the foundation of openllama. the model is trained using a construction process that incorporates real- world legal text, legal regulations, judicial interpretations, and actual legal consultation data. additionally, the authors utilize the chatgpt api for assisted construction, enabling the generation of supplementary data derived from the existing dataset. wu et al. (2023d) have developed a large- scale chinese legal model (named fuzi) with chatglm as its foundation. this model undergoes training on an extensive chinese legal corpus, which incorporates unsu- pervised judicial language data, including diverse judgment documents and legal regulations. additionally, it undergoes supervised judicial fine-tuning with data encompassing le- gal qa and case retrieval. fuzi’s training also involves both general instruction fine-tuning datasets, such as alpaca, and domain-specific instruction fine-tuning datasets from lawyerllama (huang et al., 2023b) and lawgpt (cui et al., 2023b). 5.2 medical and healthcare the integration of llms carries substantial promise in fun- damentally reshaping the landscape of medical data anal- ysis, comprehension, and smart medical services (singhal et al., 2023; yang et al., 2024b). significant research endeav- ors have been dedicated to adapting general-purpose llms to the medical domain, given the ever-expanding wealth of information encompassing electronic health records, med- ical literature, and clinical data. especially in healthcare, llms are revolutionizing patient care, research, and admin- istrative efficiency. they enhance diagnostic accuracy by an- alyzing patient data and medical literature, offering person- alized recommendations, and identifying potential drug in- teractions. llms also streamline administrative tasks by au- tomating patient documentation and processing insurance claims, reducing the burden on healthcare providers and 24 improving patient experiences. furthermore, they facilitate medical research by synthesizing vast amounts of data to uncover new insights into diseases and treatments (will be discussed later). this integration of llms into healthcare is paving the way for more informed clinical decision-making, improved patient outcomes, and more efficient healthcare systems. these adaptations extend across a spectrum, ranging from refining the precision of medical diagnoses (wang et al., 2023k) and providing personalized treatment rec- ommendations (zhu et al., 2023) to automating routine administrative processes within healthcare settings. while existing studies predominantly concentrate on training using dedicated medical dialogue datasets com- prising medical textbooks (wu et al., 2023e), biomedical papers (luo et al., 2023e) medical knowledge-graphs (bao et al., 2023b), or authentic doctor-patient interactions (bao et al., 2023b), an expanding body of research is delving into the augmentation of medical instruction-following data with advanced llms to enhance the alignment of the intricacies within practical user instructions. zhang et al. (2023c) introduce huatuogpt specifically tailored for med- ical consultations. the model leverages both distilled data from chatgpt and real-world data from doctors during the supervised fine-tuning stage. in a parallel effort, xiong et al. (2023) construct a dataset of medical dialogues in chinese, employing chatgpt’s assistance. their method- ology encompassed various techniques to train doctor- glm, an easily deployable llm designed for tasks such as diagnoses, drug recommendations, and other medical advice. zhang et al. (2023f) fine-tune llama-series models using 52k diverse, machine-generated, medical instruction- following data named medinstruct-52k. this effort resulted in the development of alpacare, a model demonstrating robust medical proficiency and generalizability across both general and medical-specific domain free-form instruction evaluations. in a different vein, wang et al. (2023a) propose huatuo, a llama-based model that undergoes supervised fine-tuning with generated qa instances. this refinement process enhances the model’s possession of more reliable medical knowledge. li et al. (2023i) introduce chatdoctor, which was first trained as a generic conversation model based on llama. it utilized 52k instruction-following data from stanford university’s alpaca project (taori et al., 2023). subsequently, the conversation model underwent fine-tuning on a dataset of 100k patient-physician conver- sations collected from an online medical consultation web- site. this two-step training process underscores the model’s adaptability to diverse conversational contexts, particularly those specific to patient-physician interactions. built upon existing datasets, medalpaca (han et al., 2023) proposes to reconstruct the data with gpt-3.5-turbo, which is then used to fine-tune llms for effective medical applications. furthermore, pmc-llama (wu et al., 2023f) proposes a training framework (i.e., continual pre-training and domain-specific multi-task supervised fine-tuning) to adapt a general llm to the medicine domain, where gpt- 4 is leveraged to write synonymous sentences for data augmentation in the sft. to adapt llms to real-world medical consultation, disc-medllm (bao et al., 2023a) leverages gpt-3.5 to 1) construct 50k qa pairs in a few-shot manner and 2) re-generate the 420k dialogues based on real cases, which are then used to train llms in a supervised fine-tuning manner. more recently, huatuogpt- ii (chen et al., 2023d) proposes a one-stage training with instruction-formatting unification of domain data collection for medical adaption upon llms, where gpt-4 is used to formulate medical questions to fine-tuning instructions. these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- ploration of various methodologies, these approaches pro- vide valuable insights into the challenges and potential breakthroughs at the intersection of cutting-edge language models and medical applications. 5.3 finance the application of llms to the finance domain (xue et al., 2023) significantly transforms how financial data is ana- lyzed, decisions are made, and customer interactions are managed. in finance, llms offer unprecedented capabil- ities in understanding complex financial documents, pre- dicting market trends, and automating risk assessment, thus enabling more informed and faster decision-making processes. by processing and analyzing vast amounts of unstructured financial data, such as news articles, reports, and real-time market feeds, llms can identify patterns and insights that were previously inaccessible, leading to more accurate forecasts and strategic financial planning. furthermore, llms enhance customer experiences through personalized financial advice, automated customer service, and sophisticated chatbots that can handle intricate queries. this level of automation and insight has the potential to increase efficiency, reduce operational costs, and improve compliance and risk management practices in financial insti- tutions, making llms a transformative force in the finance sector. knowledge distillation from a proprietary llm is still under-explored, and most existing works focus on adapting llms to finance applications by continual pre-training on finance-specific corpora (wu et al., 2023g; lu et al., 2023) or fine-tuning in a supervised manner on multi-task finance- specific instructions (yang et al., 2023e; xie et al., 2023b; wang et al., 2023l). specifically, xuanyuan (zhang and yang, 2023) lever- ages self-instruct over seed data and self-qa over struc- tured/unstructured data to generate instruction data in the finance domain, which is used to train a finance llm. 5.4 science the integration of llms into the science domain (taylor et al., 2022; yin et al., 2023b) represents a paradigm shift in research, knowledge discovery, and the dissemination of scientific information. in science, llms are leveraged to digest and synthesize vast amounts of literature, aiding in the identification of new research opportunities and the ac- celeration of scientific breakthroughs. they facilitate the un- derstanding of complex scientific concepts by summarizing research papers, generating hypotheses, and even drafting research proposals and manuscripts, thus significantly re- ducing the time researchers spend on literature review and 25 enabling them to focus more on experimental work. llms also democratize access to scientific knowledge by pro- viding layperson summaries of complex research findings, making science more accessible to non-experts and fostering a broader public understanding of scientific advancements. by enhancing the efficiency of research workflows and fostering interdisciplinary collaborations, llms are poised to accelerate the pace of scientific discovery and innovation across various fields. to distill knowledge from an llm, darwin series (xie et al., 2023a) utilizes a semi self- instruct for instruction generation for science papers, which is then used to fine-tune an llm. sciglm (zhang et al., 2024) proposes to train a scientific llm, which prompts a teacher llm to generate detailed answers for unlabelled scientific questions, as well as a self-reflective critic-and- revise to improve data quality. besides the above knowledge distillation methods to adapt llms to science, we will also delve into how the distillation happens in sub-domains, e.g., mathematics, as- tronautics, chemistry, etc. mathematics. the application of llms within the sub- domain of mathematics heralds a transformative era in mathematical research, education, and problem-solving (azerbayev et al., 2023; yu et al., 2023b). llms in mathemat- ics facilitate the exploration and understanding of complex mathematical theories and problems by providing intuitive explanations, proofs, and solutions that can bridge the gap between advanced mathematical concepts and learn- ers at various levels. these models have shown potential in conjecturing new mathematical theorems and patterns, thus opening new avenues for research and discovery that might not have been readily accessible to humans alone. in education, they serve as personalized tutors, offering students step-by-step guidance through mathematical prob- lems and adapting explanations to the learner’s level of un- derstanding. this democratizes access to high-quality math- ematical education and fosters a deeper appreciation and understanding of mathematics among a broader audience. by enhancing collaborative efforts through the generation of new ideas and the simplification of complex concepts, llms are poised to significantly advance the field of math- ematics, making it more accessible, efficient, and innova- tive. wizardmath (luo et al., 2023b) enhances the mathe- matical reasoning capabilities of llama-2 by applying the novel reinforcement learning from evol-instruct feedback (rleif) method, significantly outperforming other open- source llms on the gsm8k and math benchmarks, as well as surpassing several closed-source llms including chatgpt-3.5 and minerva. mammoth (yue et al., 2023a) is a series of open-source llms specifically developed for gen- eral math problem-solving, achieving superior performance on nine mathematical reasoning datasets. utilizing a novel instruction tuning dataset called mathinstruct, which com- bines chain-of-thought and program-of-thought rationales, mammoth models demonstrate substantial improvements over existing models. tora (gou et al., 2024), a series of tool-integrated reasoning agents, significantly advances mathematical problem-solving by combining natural lan- guage reasoning with the use of external computational tools. it markedly outperforms existing open-source modelson 10 mathematical reasoning datasets, showcasing notable improvements over both rationale-based and program- based approaches, and introduces innovative training tech- niques such as output space shaping to enhance model rea- soning capabilities. g-llava (gao et al., 2023c) introduces a significant advancement in geometric problem-solving for llms by leveraging a multimodal approach that combines text and image data. this model, utilizing the geo170k dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. astronautics. the application of llms in astronau- tics (nguyen et al., 2023) propels the field forward. astrollama-chat (perkowski et al., 2024) is an ad- vancement of the astrollama model, leveraging a 7b- parameter llama-2 model and targeted continual pre- training on a curated astronomy corpus to enhance per- formance in astronomy-focused question-answering. this model demonstrates significant improvements in special- ized topic comprehension and introduces a chat-enabled version for the astronomy community, highlighting the effectiveness of domain-specific knowledge distillation in achieving superior performance on specialized topics. chemistry and materials science. the integration of llms into chemistry and materials science has revolutionized the way researchers approach the discovery and develop- ment of new compounds and materials. by analyzing vast datasets and scientific literature, llms can predict the prop- erties and behaviors of substances, significantly accelerating the innovation cycle. gimlet (zhao et al., 2023f), graph instruction based molecule zero-shot learning, is a novel approach to molecule property prediction that integrates graph and text data within a single language model framework, aiming to improve instruction-based zero-shot learning for molec- ular tasks. by leveraging a transformer mechanism with generalized position embedding and decoupled attention, gimlet significantly outperforms traditional molecule-text baselines in zero-shot learning scenarios, demonstrating the model’s effectiveness in generalizing from instructions to a broad range of molecule-related tasks without prior explicit task-specific training. llm-prop (rubungo et al., 2023), leveraging the t5 model, showcases how llms can outperform sota graph neural networks in predicting the physical and electronic properties of crystalline solids from text descriptions. this approach underscores the potential of text-based methods in materials science, offering significant improvements in prediction accuracy while also contribut- ing a benchmark dataset, textedge, to foster further re- search in this emerging field. instructmol (cao et al., 2023a) integrates multi-modal data, aligning molecular structures with natural language instructions for drug discovery tasks. through a novel two-stage instruction-tuning approach, it significantly enhances performance in molecule-related tasks, establishing a reliable molecular assistant that outper- forms existing llms and reduces the performance gap with specialized models. this demonstrates the value of multi- modal integration in developing versatile tools for complex domains like drug discovery. 26 biology. in the field of biology, particularly in the study of pro- teins, dna, and rna, llms are revolutionizing our under- standing of the fundamental molecules of life. by analyzing vast datasets of biological sequences and structures, llms can predict the three-dimensional shapes of proteins, poten- tial functions, and interactions at a scale and speed beyond traditional computational methods. this capability is critical for unraveling the complexities of biological systems, ad- vancing drug discovery by identifying targets and designing molecules with high precision, and understanding genetic diseases through the interpretation of genomic variations. prot2text (abdine et al., 2023) introduces a novel multi- modal framework for generating protein function descrip- tions in free text by combining gnns and llms. this approach, which integrates structural and sequential protein information, highlights the transformative impact of knowl- edge distillation through the fusion of gnns and llms for accurate protein function prediction, potentially revolu- tionizing research in bioinformatics and biological sciences. biomedgpt (luo et al., 2023e) introduces a multimodal generative pre-trained transformer specifically designed for the biomedicine domain, emphasizing the significance of aligning molecular, protein, and natural language modal- ities to enhance biomedical question-answering, molecule, and protein qa tasks. this framework showcases the critical role of knowledge distillation in bridging the gap between complex biological data and human language, thereby fa- cilitating groundbreaking advancements in drug discovery and therapeutic target identification. xtrimopglm (chen et al., 2024e), a unified 100b-scale pre-trained transformer model, addresses both protein understanding and genera- tion tasks by integrating autoencoding and autoregressive pre-training objectives. its significant advancements over existing models in 18 protein understanding benchmarks and its capability in de novo protein sequence generation highlight the model’s importance in advancing the field of protein science through knowledge distillation. geography, geology, and environmental science. the inte- gration of llms into geography, geology, and environmen- tal science is revolutionizing these fields by enhancing data analysis, predictive modeling, and interdisciplinary research (roberts et al., 2023; lin et al., 2023b; wang et al., 2023m). k2 (deng et al., 2023), the first-ever llm specialized in the geoscience domain, demonstrates the significant impact of knowledge distillation in vertical domain specialization. by adapting the general-domain llama-7b model with a 5.5b token geoscience corpus and introducing the geosig- nal instruction tuning dataset, k2 showcases enhanced performance in geoscience knowledge understanding and utilization. the model’s development highlights a novel approach to efficiently gather domain-specific data and align model responses to specialized user queries, underpin- ning the importance of domain-specified vertical distillation in advancing research and applications within geoscience. oceangpt (bi et al., 2023), introduced as the first llm for ocean science tasks, underscores the vital role of knowl- edge distillation in the vertical domain of oceanography. it leverages doinstruct, a novel framework for generating domain-specific instruction data through multi-agent col-laboration, and establishes oceanbench, a benchmark for evaluating llms in the ocean domain. the model’s comprehensive experiments demonstrate its superior capa- bility in understanding and generating knowledge for ocean science, showcasing the significant potential of targeted knowledge distillation in enhancing domain-specific model performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language model tailored for marine science. utilizing the marine-5m dataset, which includes over 5 million marine image-text pairs, marinegpt excels in providing detailed, accurate, and domain-specific responses. this advancement underscores the model’s ability to significantly enhance marine knowl- edge comprehension and application, emphasizing the crit- ical role of domain-specific distillation in bridging the gap between general-purpose models and specialized domain requirements. geogalactica (lin et al., 2024) represents a pioneering step in specializing llms for geoscience, lever- aging a 30 billion parameter model pre-trained on a vast geoscience corpus. this model, notable for being the largest of its kind within the geoscience domain, showcases the significant potential of knowledge distillation in fostering scientific discoveries by bridging artificial intelligence with geoscience research and applications. 5.5 miscellaneous the expansion of llms into various verticals beyond the ones previously discussed showcases their versatility and transformative potential across numerous industries and societal sectors. llms are being tailored to meet the spe- cific needs and challenges of different domains, from legal and governmental to entertainment and beyond, providing sophisticated natural language understanding, generation, and decision-making capabilities. education. educhat (dan et al., 2023) is a large-scale lan- guage model-based chatbot system designed for the educa- tion domain. it aims to revolutionize intelligent education by providing personalized, fair, and compassionate support to teachers, students, and parents. knowledge distillation is emphasized through the pre-training on an educational corpus and fine-tuning on custom instructions to activate education-specific functions like open question answering, essay assessment, and emotional support. educhat demon- strates the importance of domain-specified knowledge dis- tillation in enhancing the performance of llms within specific verticals, offering a significant contribution to in- telligent education technology. it operations. owl (guo et al., 2023b) is a specialized llm tailored for it operations, focusing on enhancing efficiency and analysis within this domain. the model leverages a unique owl-instruct dataset covering a broad range of it-related information, employing a mixture-of- adapter strategy for efficient domain-specific tuning. this approach significantly improves it operation tasks’ perfor- mance, highlighting the critical role of knowledge distilla- tion in adapting general llms to specialized fields such as it operations, thereby pushing forward the frontier in specialized ai applications within this sector. 27 6 o penproblems',\n",
       " '6. open problems': 'further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions. in the field of instruction tuning, one of the most commonly used methods for distillation, zhou et al. (2023a) propose that only 1000 human-curated high-quality data is enough for the alignment of llms, hypothesizing that llms have learned the required knowl- edge from pretraining and only a small amount of data is required for the alignment. its finding further raises a new question, how to automatically select the data for better distillation? chen et al. (2023e) directly apply chatgpt to rate each data sample together with explanations, and then the data is selected based on the rating. cao et al. (2023b) split the existing instruction-tuning datasets and trains a linear function to select the most effective data based on their statistical properties. li et al. (2023j) propose a data selection pipeline similar to self-distillation, in which the llm firstly learns from a small subset of the data to get the basic ability, and then further uses this learned model to rate for the original dataset. du et al. (2023b) propose to consider three aspects including quality, coverage, and necessity for the filtering process. li et al. (2023k) select instruction data by evaluating their one-shot improvement on a hold-out set. li et al. (2024f) recently propose superfiltering, which is able to utilize small language models like gpt2 to filter out the high-quality subset from a given high-quality dataset. despite the emergence of these works working on data fil- tering, how to efficiently select the optimal distillation data for llms, and how much data is required for distillation are still unsolved. reduce the distillation cost (lightweight methods) de- spite the remarkable abilities of the latest llms, their sig- nificant resource requirements underscore the urgent need to find efficient solutions to overcome these challenges. common ways to further reduce the distillation cost include model compression and efficient fine-tuning. in the realm of model compression, quantization (frantar et al., 2023; dettmers et al., 2022; kim et al., 2023c; tao et al., 2022b; yao et al., 2022; xiao et al., 2023), parameter pruning (ma et al., 2023d; zhang et al., 2023h; frantar and alistarh, 2023), and low-rank approximation (xu et al., 2023g; li et al., 2023l) are commonly utilized. in the realm of efficient fine-tuning, parameter efficient fine-tuning (hu et al., 2023b; liu et al., 2022c; wang et al., 2022b; hu et al., 2021; li and liang, 2021; liu et al., 2022d), and memory efficient fine-tuning (dettmers et al., 2023; kim et al., 2023d; malladi et al., 2024) are utilized. a detailed survey on efficient large language models can be found here in wan et al. (2024b). the problem that remains is how can we further compress the model and build effective distillation algorithms. multi-teacher distillation most of the existing distilled models are distilled from a single teacher model, how- ever, it is widely accepted that models trained with dif- ferent sources of data have various capabilities. thus a question arises: is it possible to distill knowledge from different teacher models into one student model? babyl- lama (timiryasov and tastet, 2023) proposes to distill the knowledge from both the gpt2 and llama into the small-size student models. ensemble-instruct (lee et al., 2023b) tries to generate both instructions and responses ensembled from several different llms with rougel as the indicator. fusellm (wan et al., 2024a) externalizes the collective knowledge and unique strengths by leveraging the genera- tive distributions of different llms aiming to train a student model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated in table 3, the majority of teacher llms are closed-source due to their advanced capabilities. consequently, current methodologies primarily focus on using the generations from these models as hard labels, training student models through simple supervised fine-tuning. however, beyond the straightforward imitation of output behaviors via hard labels, there is a growing interest in harnessing richer knowledge from teacher llms, including feedback and feature knowledge, as well as exploring diverse combina- tions of knowledge elicitation methods. as highlighted in thefeedback section, teachers can provide various types of feedback based on the student’s outputs (lee et al., 2023a; jiang et al., 2023b; chen et al., 2023a). similarly, the feature section discusses how knowledge based on features, such as logits serving as soft labels, can offer deeper, intrinsic insights into the teacher model (gu et al., 2024; agarwal et al., 2024). these explorations have demonstrated promis- ing outcomes, suggesting that access to a broader spectrum of knowledge can significantly enhance student model per- formance beyond what is achievable through simple sft distillation alone. this highlights the critical need for further research into varied knowledge extraction methods from teacher llms to augment the effectiveness of kd processes. overcoming catastrophic forgetting during distillation previous research has delved into the fine-tuning of llms to acquire the ability to follow instructions or transfer knowledge for forthcoming tasks, skills, or domains, lever- aging advancements in llm technology. nevertheless, in- vestigations have revealed that the continual fine-tuning of llms on particular datasets (skills, domains) can lead to a phenomenon known as catastrophic forgetting, wherein previously acquired knowledge and problem-solving abil- ities for earlier tasks are compromised (chen et al., 2023f; kotha et al., 2023; koloski et al., 2023; wu et al., 2024; luo et al., 2023f). earlier studies in machine learning and deep learning have investigated various techniques to help mitigate forgetting during the fine-tuning or continue learn- ing process, such as rehearsal, which entails periodically revisiting and training on past data (kirkpatrick et al., 2017; rostami et al., 2019; rolnick et al., 2019), as well as reg- ularization methods like elastic weight consolidation (lee et al., 2017), or dynamic architecture methods (mallya et al., 2018; wang et al., 2022c; hu et al., 2023c; chen et al., 2023f). to address the challenges of catastrophic forgetting and to enhance the diversity of generated instructions in knowl- edge distillation for llms, jiang et al. (2023b) randomly sample an instruction from the easy instructions and also prompt the generator to generate a new instruction that belongs to the same domain as the sampled one. in a similar vein, li et al. (2023m) study the problem of instruction- 28 tuning in multi-modal llms knowledge distillation and introduce a competitive distillation framework. the model tries to produce new instructions that differ in content but are similar in difficulty to the original pictures in the multi- modal augmentation phase, so as to alleviate catastrophic forgetting of the model and enhance the diversity of the instruction tuning pool. chen et al. (2023f) propose the lifelong-moe (mixture-of experts) architecture based on general language models, which dynamically adds model capacity via adding experts with regularized pretraining. additionally, the model also introduces implicit regulariza- tion via distillation of the knowledge from old experts and gatings to effectively preserve old knowledge. zeng et al. (2023b) propose a new generative-based rehearsal method as dirichlet continual learning (dcl). this method com- bines task distribution modeling and knowledge distillation to mitigate catastrophic forgetting without requiring access to the old data. to evaluate the effectiveness of instruction tuning in the context of continuous learning tasks, zhang et al. (2023i) introduce a more challenging yet practical problem called continual instruction tuning (cit) and also establish a benchmark suite consisting of learning and eval- uation protocols. although current research has explored some simple methods to alleviate knowledge forgetting dur- ing model fine-tuning or knowledge distillation processes, effectively avoiding catastrophic forgetting across domains and skills remains a challenging issue. how to retain the original model’s capabilities effectively during knowledge distillation or transfer processes is still a challenging prob- lem. trustworthy knowledge distillation trustworthiness in llms is paramount, encompassing attributes such as truth- fulness, safety, fairness, robustness, privacy, and adherence to machine ethics (sun et al., 2024a). the rapid advancement of llms brings to the forefront concerns regarding their trustworthiness, stemming from their complex outputs, the biases present in vast training datasets, and the potential inclusion of private information. current efforts in kd of llms primarily focus on distilling various skills from llms, with relatively little attention paid to trustworthiness aspects. existing studies tend to concentrate on a subset of trustworthiness aspects, such as helpfulness, honesty, and harmlessness (bai et al., 2022a; yang et al., 2024a; cui et al., 2023a). consequently, in the distillation process, student models may inherit issues related to trustworthiness from their teacher llms. as assessed in sun et al. (2024a), smaller open-source llms generally fall short of their proprietary counterparts in trustworthiness metrics. therefore, consid- ering trustworthiness alongside the distillation of capabil- ities into student models is crucial. it is imperative that future research on kd not only enhances the capabilities of student models but also ensures that broader aspects of trustworthiness are meticulously addressed. weak-to-strong distillation. the concept of “weak-to- strong generalization” in llms (burns et al., 2023) empha- sizes the potential to leverage weak supervision to elicit the advanced capabilities of more powerful models. this approach challenges the traditional distillation paradigm by suggesting that even with limited or imperfect supervision, it is possible to enhance the performance of llms sig-nificantly. this necessitates exploring innovative strategies that enable weaker models to guide the learning process of stronger ones effectively, highlighting the importance of developing methods that can bridge the gap between these models. such research could unlock new avenues for improving llms’ efficiency and effectiveness, making the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision can unlock the full capabilities of much stronger models. through experiments with pre-trained language models in the gpt-4 family across nlp , chess, and reward modeling tasks, it finds that finetuning strong models on weak labels leads to better performance than their weak supervisors, demonstrating weak-to-strong generalization. then, li et al. (2024g) introduce superfiltering, a method that employs smaller, weaker models like gpt-2 to select high-quality data for fine-tuning larger, more capable models such as llama2. this approach is rooted in discovering a strong consistency in evaluating instruction tuning data difficulty across models of varying sizes. more recently, ji et al. (2024) introduce aligner, a novel approach for aligning llms with human values and intentions by utilizing weak supervisory signals from smaller models to improve the performance of larger models. however, burns et al. (2023) find that achieving the full capabilities of strong models requires more than naive finetuning, suggesting the need for further research in this area. therefore, open questions still remain about 1) what are the theoretical and practical limits of weak-to-strong distillation? can weak supervision reliably extract and enhance the full spectrum of capabilities in stronger models across all domains, or are there inherent limitations based on model architecture or task specificity? 2) how do we identify or design the optimal weak su- pervisors for distilling knowledge into stronger models? is there a framework or criteria to predict which weak models would be most effective in guiding the learning process of more complex models for specific tasks? 3) to what extent are weak-to-strong distillation techniques transferable and scalable across different sizes and types of models? how can these methods be adapted to ensure efficacy and ef- ficiency in distilling knowledge from very large models to significantly smaller ones, especially in resource-constrained environments? self-alignment. aligning llms traditionally relies heavily on human or teacher llms to supply extensive preference data. consequently, the alignment of the student model is limited by the quantity of distilled preference data and the teacher’s capabilities. self-alignment offers a promising alternative, aiming to enhance alignment beyond the con- straints of teacher-provided preferences. in self-alignment, the student model endeavors to autonomously improve and align its responses with desired behaviors, including generating model-written feedback, critiques, and explana- tions. several studies have explored utilizing the student model’s inherent capabilities to generate knowledge for alignment (bai et al., 2022a; sun et al., 2024b; li et al., 2024c; yuan et al., 2024a). beyond merely producing improved responses (bai et al., 2022a; sun et al., 2024b), implemen- tations of self-alignment include employing the student as 29 its reward model to offer feedback (yuan et al., 2024a), a strategy that merges self-knowledge with feedback methods of eliciting knowledge. we advocate for increasingly lever- aging the student model itself to provide feedback, thereby enhancing self-alignment capabilities. this approach not only facilitates moving beyond traditional human/teacher preference-based rewards but also opens avenues for con- tinual self-improvement and alignment. 7 c onclusion and discussion',\n",
       " '7. conclusion and discussion': 'this survey has traversed the expansive domain of knowl- edge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field. we have underscored the pivotal role of kd in democratizing access to the advanced capabilities of proprietary llms, thereby fostering a more equitable ai landscape. through meticulous examination, we have highlighted how kd serves as a bridge, enabling resource- constrained entities to benefit from the profound advance- ments in llms without the prohibitive costs associated with training and deploying state-of-the-art models. our exploration delineates the multifaceted approaches to kd, ranging from algorithmic innovations and skill en- hancement to domain-specific distillations. each segment reveals the nuanced complexities and potentialities inherent in tailoring distilled models to emulate the sophisticated un- derstandings and functionalities of their more cumbersome counterparts. notably, the integration of data augmentation strategies within kd processes emerges as a critical lever for enhancing distillation in this llm era, underscoring the synergistic potential between generating context-rich training data and the distillation endeavor. as we project into the future, several avenues for re- search beckon. the evolving landscape of ai, marked by rapid advancements in model architectures and training methodologies, presents both challenges and opportunities for kd. the quest for more efficient, transparent, and ethical ai models necessitates continued innovation in kd tech- niques, especially those that can navigate the delicate bal- ance between model fidelity, computational efficiency, and ethical considerations. furthermore, the exploration of kd in nascent areas such as weak-to-strong generalization, self- alignment, multi-modal llms, real-time adaptation, and personalized ai services promises to expand the horizons of what distilled models can achieve. therefore, knowledge distillation of llms stands at a critical juncture, embodying the potential to significantly influence the trajectory of ai development and application. as this survey elucidates, the concerted efforts of the re- search community in pushing the boundaries of kd will be instrumental in realizing the vision of accessible, efficient, and responsible ai for all. legal considerations for using llm outputs: impor- tantly, it’s crucial to note the legal implications of utilizing llm outputs, such as those from chatgpt4, llama5, etc. we strongly advocate compliance with the terms of use specified by the model providers, such as the restrictions on developing competitive products, and so on. 4. https://openai.com/policies/business-terms 5. https://llama.meta.com/llama-downloads/references l. ouyang, j. wu, x. jiang, d. almeida, c. wainwright, p . mishkin, c. zhang, s. agarwal, k. slama, a. ray et al. , “training language models to follow instructions with human feedback,” advances in neural information processing systems , vol. 35, pp. 27 730–27 744, 2022. openai, :, j. achiam, s. adler, s. agarwal, l. ahmad, i. akkaya, f. l. aleman, d. almeida, j. altenschmidt, s. altman, s. anadkat, r. avila, i. babuschkin, s. balaji, v . balcom, p . baltescu, h. bao, m. bavarian, j. belgum, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, a. cann, b. carey, c. carlson, r. carmichael, b. chan, c. chang, f. chantzis, d. chen, s. chen, r. chen, j. chen, m. chen, b. chess, c. cho, c. chu, h. w. chung, d. cummings, j. currier, y. dai, c. decareaux, t. degry, n. deutsch, d. deville, a. dhar, d. dohan, s. dowling, s. dunning, a. ecoffet, a. eleti, t. eloundou, d. farhi, l. fedus, n. felix, s. p . fishman, j. forte, i. fulford, l. gao, e. georges, c. gibson, v . goel, t. gogineni, g. goh, r. gontijo-lopes, j. gordon, m. grafstein, s. gray, r. greene, j. gross, s. s. gu, y. guo, c. hallacy, j. han, j. harris, y. he, m. heaton, j. heidecke, c. hesse, a. hickey, w. hickey, p . hoeschele, b. houghton, k. hsu, s. hu, x. hu, j. huizinga, s. jain, s. jain, j. jang, a. jiang, r. jiang, h. jin, d. jin, s. jomoto, b. jonn, h. jun, t. kaf- tan, łukasz kaiser, a. kamali, i. kanitscheider, n. s. keskar, t. khan, l. kilpatrick, j. w. kim, c. kim, y. kim, h. kirchner, j. kiros, m. knight, d. kokotajlo, łukasz kondraciuk, a. kondrich, a. konstantinidis, k. kosic, g. krueger, v . kuo, m. lampe, i. lan, t. lee, j. leike, j. leung, d. levy, c. m. li, r. lim, m. lin, s. lin, m. litwin, t. lopez, r. lowe, p . lue, a. makanju, k. mal- facini, s. manning, t. markov, y. markovski, b. mar- tin, k. mayer, a. mayne, b. mcgrew, s. m. mckin- ney, c. mcleavey, p . mcmillan, j. mcneil, d. medina, a. mehta, j. menick, l. metz, a. mishchenko, p . mishkin, v . monaco, e. morikawa, d. mossing, t. mu, m. murati, o. murk, d. m ´ely, a. nair, r. nakano, r. nayak, a. nee- lakantan, r. ngo, h. noh, l. ouyang, c. o’keefe, j. pa- chocki, a. paino, j. palermo, a. pantuliano, g. parascan- dolo, j. parish, e. parparita, a. passos, m. pavlov, a. peng, a. perelman, f. de avila belbute peres, m. petrov, h. p . de oliveira pinto, michael, pokorny, m. pokrass, v . pong, t. powell, a. power, b. power, e. proehl, r. puri, a. rad- ford, j. rae, a. ramesh, c. raymond, f. real, k. rimbach, c. ross, b. rotsted, h. roussez, n. ryder, m. saltarelli, t. sanders, s. santurkar, g. sastry, h. schmidt, d. schnurr, j. schulman, d. selsam, k. sheppard, t. sherbakov, j. shieh, s. shoker, p . shyam, s. sidor, e. sigler, m. simens, j. sitkin, k. slama, i. sohl, b. sokolowsky, y. song, n. staudacher, f. p . such, n. summers, i. sutskever, j. tang, n. tezak, m. thompson, p . tillet, a. tootoonchian, e. tseng, p . tuggle, n. turley, j. tworek, j. f. c. uribe, a. vallone, a. vijayvergiya, c. voss, c. wainwright, j. j. wang, a. wang, b. wang, j. ward, j. wei, c. weinmann, a. welihinda, p . welinder, j. weng, l. weng, m. wiethoff, d. willner, c. winter, s. wolrich, h. wong, l. workman, s. wu, j. wu, m. wu, k. xiao, t. xu, s. yoo, k. yu, 30 q. yuan, w. zaremba, r. zellers, c. zhang, m. zhang, s. zhao, t. zheng, j. zhuang, w. zhuk, and b. zoph, “gpt- 4 technical report,” 2023. g. team, r. anil, s. borgeaud, y. wu, j.-b. alayrac, j. yu, r. soricut, j. schalkwyk, a. m. dai, a. hauth et al. , “gemini: a family of highly capable multimodal models,” arxiv preprint arxiv:2312.11805 , 2023. j. wei, y. tay, r. bommasani, c. raffel, b. zoph, s. borgeaud, d. yogatama, m. bosma, d. zhou, d. metzler, e. h. chi, t. hashimoto, o. vinyals, p . liang, j. dean, and w. fedus, “emergent abilities of large language models,” trans. mach. learn. res. , vol. 2022, 2022. [online]. available: https://openreview.net/forum?id=yzksu5zdwd j. wei, x. wang, d. schuurmans, m. bosma, f. xia, e. chi, q. v . le, d. zhou et al. , “chain-of-thought prompting elicits reasoning in large language models,” advances in neural information processing systems , vol. 35, pp. 24 824– 24 837, 2022. x. xu, c. tao, t. shen, c. xu, h. xu, g. long, and j. guang lou, “re-reading improves reasoning in large language models,” 2024. p . liang, r. bommasani, t. lee, d. tsipras, d. soylu, m. yasunaga, y. zhang, d. narayanan, y. wu, a. kumar, b. newman, b. yuan, b. yan, c. zhang, c. cosgrove, c. d. manning, c. r ´e, d. acosta-navas, d. a. hudson, e. zelikman, e. durmus, f. ladhak, f. rong, h. ren, h. yao, j. wang, k. santhanam, l. j. orr, l. zheng, m. y ¨uksekg ¨on¨ul, m. suzgun, n. kim, n. guha, n. s. chatterji, o. khattab, p . henderson, q. huang, r. chi, s. m. xie, s. santurkar, s. ganguli, t. hashimoto, t. icard, t. zhang, v . chaudhary, w. wang, x. li, y. mai, y. zhang, and y. koreeda, “holistic evaluation of language models,” corr , vol. abs/2211.09110, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09110 x. wu, r. duan, and j. ni, “unveiling security, privacy, and ethical concerns of chatgpt,” journal of information and intelligence , 2023. h. touvron, l. martin, k. stone, p . albert, a. almahairi, y. babaei, n. bashlykov, s. batra, p . bhargava, s. bhosale, d. bikel, l. blecher, c. c. ferrer, m. chen, g. cucurull, d. esiobu, j. fernandes, j. fu, w. fu, b. fuller, c. gao, v . goswami, n. goyal, a. hartshorn, s. hosseini, r. hou, h. inan, m. kardas, v . kerkez, m. khabsa, i. kloumann, a. korenev, p . s. koura, m.-a. lachaux, t. lavril, j. lee, d. liskovich, y. lu, y. mao, x. martinet, t. mihaylov, p . mishra, i. molybog, y. nie, a. poulton, j. reizen- stein, r. rungta, k. saladi, a. schelten, r. silva, e. m. smith, r. subramanian, x. e. tan, b. tang, r. taylor, a. williams, j. x. kuan, p . xu, z. yan, i. zarov, y. zhang, a. fan, m. kambadur, s. narang, a. rodriguez, r. stojnic, s. edunov, and t. scialom, “llama 2: open foundation and fine-tuned chat models,” 2023. a. q. jiang, a. sablayrolles, a. mensch, c. bamford, d. s. chaplot, d. de las casas, f. bressand, g. lengyel, g. lam- ple, l. saulnier, l. r. lavaud, m.-a. lachaux, p . stock, t. l. scao, t. lavril, t. wang, t. lacroix, and w. e. sayed, “mistral 7b,” 2023. l. zheng, w. chiang, y. sheng, s. zhuang, z. wu, y. zhuang, z. lin, z. li, d. li, e. p . xing, h. zhang, j. e. gonzalez, and i. stoica, “judging llm-as-a-judge with mt-bench and chatbot arena,” corr , vol. abs/2306.05685, 2023. [online].available: https://doi.org/10.48550/arxiv.2306.05685 l. sun, y. huang, h. wang, s. wu, q. zhang, c. gao, y. huang, w. lyu, y. zhang, x. li, z. liu, y. liu, y. wang, z. zhang, b. kailkhura, c. xiong, c. xiao, c. li, e. xing, f. huang, h. liu, h. ji, h. wang, h. zhang, h. yao, m. kellis, m. zitnik, m. jiang, m. bansal, j. zou, j. pei, j. liu, j. gao, j. han, j. zhao, j. tang, j. wang, j. mitchell, k. shu, k. xu, k.-w. chang, l. he, l. huang, m. backes, n. z. gong, p . s. yu, p .-y. chen, q. gu, r. xu, r. ying, s. ji, s. jana, t. chen, t. liu, t. zhou, w. wang, x. li, x. zhang, x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. j. gou, b. yu, s. j. maybank, and d. tao, “knowledge distillation: a survey,” international journal of computer vision , vol. 129, pp. 1789–1819, 2021. m. gupta and p . agrawal, “compression of deep learning models for text: a survey,” acm transactions on knowledge discovery from data (tkdd) , vol. 16, no. 4, pp. 1–55, 2022. s. y. feng, v . gangal, j. wei, s. chandar, s. vosoughi, t. mi- tamura, and e. hovy, “a survey of data augmentation approaches for nlp,” arxiv preprint arxiv:2105.03075 , 2021. r. taori, i. gulrajani, t. zhang, y. dubois, x. li, c. guestrin, p . liang, and t. b. hashimoto, “stanford alpaca: an instruction-following llama model,” https://github.com/ tatsu-lab/stanford alpaca, 2023. y. gu, l. dong, f. wei, and m. huang, “minillm: knowledge distillation of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=5h0qf7ibzz r. agarwal, n. vieillard, y. zhou, p . stanczyk, s. r. garea, m. geist, and o. bachem, “on-policy distillation of language models: learning from self-generated mistakes,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=3zktaqxlhw w. yuan, r. y. pang, k. cho, s. sukhbaatar, j. xu, and j. weston, “self-rewarding language models,” 2024. z. chen, y. deng, h. yuan, k. ji, and q. gu, “self-play fine-tuning converts weak language models to strong language models,” 2024. y. huang, y. chen, z. yu, and k. mckeown, “in-context learning distillation: transferring few-shot learning abil- ity of pre-trained language models,” 2022. g. cui, l. yuan, n. ding, g. yao, w. zhu, y. ni, g. xie, z. liu, and m. sun, “ultrafeedback: boosting lan- guage models with high-quality feedback,” arxiv preprint arxiv:2310.01377 , 2023. s. mukherjee, a. mitra, g. jawahar, s. agarwal, h. palangi, and a. awadallah, “orca: progressive learning from complex explanation traces of gpt-4,” arxiv preprint arxiv:2306.02707 , 2023. b. ding, c. qin, l. liu, y. k. chia, b. li, s. joty, and l. bing, “is gpt-3 a good data annotator?” in acl (1) . asso- ciation for computational linguistics, 2023, pp. 11 173– 11 195. s. chaudhary, “code alpaca: an instruction-following llama model for code generation,” https://github.com/ sahil280114/codealpaca, 2023. h. wang, c. liu, n. xi, z. qiang, s. zhao, b. qin, and 31 t. liu, “huatuo: tuning llama model with chinese medi- cal knowledge,” arxiv preprint arxiv:2304.06975 , 2023. lawgpt . github, 2023. d. zhang, z. hu, s. zhoubian, z. du, k. yang, z. wang, y. yue, y. dong, and j. tang, “sciglm: training scientific language models with self-reflective instruction annotation and tuning,” corr , vol. abs/2401.07950, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401. 07950 w.-l. chiang, z. li, z. lin, y. sheng, z. wu, h. zhang, l. zheng, s. zhuang, y. zhuang, j. e. gonzalez, i. stoica, and e. p . xing, “vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” march 2023. [online]. available: https://lmsys.org/blog/2023-03-30- vicuna/ c. xu, q. sun, k. zheng, x. geng, p . zhao, j. feng, c. tao, and d. jiang, “wizardlm: empowering large language models to follow complex instructions,” arxiv preprint arxiv:2304.12244 , 2023. w. x. zhao, k. zhou, j. li, t. tang, x. wang, y. hou, y. min, b. zhang, j. zhang, z. dong, y. du, c. yang, y. chen, z. chen, j. jiang, r. ren, y. li, x. tang, z. liu, p . liu, j.-y. nie, and j.-r. wen, “a survey of large language models,” 2023. x. he, z. lin, y. gong, a. jin, h. zhang, c. lin, j. jiao, s. m. yiu, n. duan, w. chen et al. , “annollm: making large language models to be better crowdsourced annotators,” arxiv preprint arxiv:2303.16854 , 2023. y. wang, z. yu, z. zeng, l. yang, c. wang, h. chen, c. jiang, r. xie, j. wang, x. xie, w. ye, s. zhang, and y. zhang, “pandalm: an automatic evaluation benchmark for llm instruction tuning optimization,” 2023. c. hsieh, c. li, c. yeh, h. nakhost, y. fujii, a. ratner, r. krishna, c. lee, and t. pfister, “distilling step-by-step! outperforming larger language models with less training data and smaller model sizes,” in acl (findings) . associ- ation for computational linguistics, 2023, pp. 8003–8017. a. mitra, l. d. corro, s. mahajan, a. codas, c. simoes, s. agarwal, x. chen, a. razdaibiedina, e. jones, k. aggar- wal, h. palangi, g. zheng, c. rosset, h. khanpour, and a. awadallah, “orca 2: teaching small language models how to reason,” 2023. c. xu, d. guo, n. duan, and j. j. mcauley, “baize: an open- source chat model with parameter-efficient tuning on self- chat data,” in emnlp . association for computational linguistics, 2023, pp. 6268–6278. x. yue, x. qu, g. zhang, y. fu, w. huang, h. sun, y. su, and w. chen, “mammoth: building math generalist mod- els through hybrid instruction tuning,” arxiv preprint arxiv:2309.05653 , 2023. l. chenglin, c. qianglong, w. caiyu, and z. yin, “mixed distillation helps smaller language model better reason- ing,” 2023. y. wang, y. kordi, s. mishra, a. liu, n. a. smith, d. khashabi, and h. hajishirzi, “self-instruct: aligning language model with self generated instructions,” arxiv preprint arxiv:2212.10560 , 2022. z. sun, y. shen, q. zhou, h. zhang, z. chen, d. cox, y. yang, and c. gan, “principle-driven self-alignment of language models from scratch with minimal human supervision,” advances in neural information processingsystems , vol. 36, 2024. z. luo, c. xu, p . zhao, q. sun, x. geng, w. hu, c. tao, j. ma, q. lin, and d. jiang, “wizardcoder: empowering code large language models with evol-instruct,” arxiv preprint arxiv:2306.08568 , 2023. h. luo, q. sun, c. xu, p . zhao, j. lou, c. tao, x. geng, q. lin, s. chen, and d. zhang, “wizardmath: empower- ing mathematical reasoning for large language models via reinforced evol-instruct,” arxiv preprint arxiv:2308.09583 , 2023. h. dai, z. liu, w. liao, x. huang, y. cao, z. wu, l. zhao, s. xu, w. liu, n. liu, s. li, d. zhu, h. cai, l. sun, q. li, d. shen, t. liu, and x. li, “auggpt: leveraging chatgpt for text data augmentation,” 2023. z. he, m. t. ribeiro, and f. khani, “targeted data generation: finding and fixing model weaknesses,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8506–8520. [online]. available: https://aclanthology.org/2023.acl-long.474 n. ding, y. chen, b. xu, y. qin, s. hu, z. liu, m. sun, and b. zhou, “enhancing chat language models by scaling high-quality instructional conversations,” in emnlp . as- sociation for computational linguistics, 2023, pp. 3029– 3051. s. gunasekar, y. zhang, j. aneja, c. c. t. mendes, a. d. giorno, s. gopi, m. javaheripi, p . kauffmann, g. de rosa, o. saarikivi, a. salim, s. shah, h. s. behl, x. wang, s. bubeck, r. eldan, a. t. kalai, y. t. lee, and y. li, “textbooks are all you need,” 2023. y. li, s. bubeck, r. eldan, a. del giorno, s. gunasekar, and y. t. lee, “textbooks are all you need ii: phi-1.5 technical report,” arxiv preprint arxiv:2309.05463 , 2023. phi-2: the surprising power of small lan- guage models , december 2023. [online]. avail- able: https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/ y. wei, z. wang, j. liu, y. ding, and l. zhang, “magicoder: source code is all you need,” 2023. z. yu, x. zhang, n. shang, y. huang, c. xu, y. zhao, w. hu, and q. yin, “wavecoder: widespread and versatile en- hanced instruction tuning with refined data generation,” 2024. j. ye, j. gao, q. li, h. xu, j. feng, z. wu, t. yu, and l. kong, “zerogen: efficient zero-shot learning via dataset generation,” in emnlp . association for computational linguistics, 2022, pp. 11 653–11 669. j. gao, r. pi, y. lin, h. xu, j. ye, z. wu, w. zhang, x. liang, z. li, and l. kong, “self-guided noise-free data generation for efficient zero-shot learning,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=h5opjgd lo6 l. h. bonifacio, h. q. abonizio, m. fadaee, and r. f. nogueira, “inpars: data augmentation for information retrieval using large language models,” corr , vol. abs/2202.05144, 2022. [online]. available: https://arxiv. org/abs/2202.05144 i. timiryasov and j.-l. tastet, “baby llama: knowledge 32 distillation from an ensemble of teachers trained on a small dataset with no performance penalty,” in proceedings of the babylm challenge at the 27th conference on computational natural language learning , a. warstadt, a. mueller, l. choshen, e. wilcox, c. zhuang, j. ciro, r. mosquera, b. paranjabe, a. williams, t. linzen, and r. cotterell, eds. singapore: association for computational linguistics, dec. 2023, pp. 279–289. [online]. available: https://aclanthology.org/2023.conll- babylm.24 c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre- trained language models via quantization,” arxiv preprint arxiv:2203.10705 , 2022. z. liu, b. oguz, c. zhao, e. chang, p . stock, y. mehdad, y. shi, r. krishnamoorthi, and v . chandra, “llm-qat: data-free quantization aware training for large language models,” arxiv preprint arxiv:2305.17888 , 2023. y. bai, s. kadavath, s. kundu, a. askell, j. kernion, a. jones, a. chen, a. goldie, a. mirhoseini, c. mckinnon, c. chen, c. olsson, c. olah, d. hernandez, d. drain, d. gan- guli, d. li, e. tran-johnson, e. perez, j. kerr, j. mueller, j. ladish, j. landau, k. ndousse, k. lukosuite, l. lovitt, m. sellitto, n. elhage, n. schiefer, n. mercado, n. das- sarma, r. lasenby, r. larson, s. ringer, s. johnston, s. kravec, s. e. showk, s. fort, t. lanham, t. telleen- lawton, t. conerly, t. henighan, t. hume, s. r. bow- man, z. hatfield-dodds, b. mann, d. amodei, n. joseph, s. mccandlish, t. brown, and j. kaplan, “constitutional ai: harmlessness from ai feedback,” 2022. l. tunstall, e. beeching, n. lambert, n. rajani, k. rasul, y. belkada, s. huang, l. von werra, c. fourrier, n. habib et al. , “zephyr: direct distillation of lm alignment,” arxiv preprint arxiv:2310.16944 , 2023. j. hong, q. tu, c. chen, x. gao, j. zhang, and r. yan, “cyclealign: iterative distillation from black-box llm to white-box models for better human alignment,” arxiv preprint arxiv:2310.16271 , 2023. h. lee, s. phatale, h. mansoor, k. lu, t. mesnard, c. bishop, v . carbune, and a. rastogi, “rlaif: scaling reinforcement learning from human feedback with ai feedback,” arxiv preprint arxiv:2309.00267 , 2023. y. jiang, c. chan, m. chen, and w. wang, “lion: adversarial distillation of closed-source large language model,” arxiv preprint arxiv:2305.12870 , 2023. h. chen, a. saha, s. hoi, and s. joty, “personalized distillation: empowering open-sourced llms with adaptive learning for code generation,” in the 2023 conference on empirical methods in natural language processing , 2023. [online]. available: https://openreview. net/forum?id=alxwmbcnvn k. yang, d. klein, a. celikyilmaz, n. peng, and y. tian, “rlcd: reinforcement learning from contrastive distilla- tion for lm alignment,” in the twelfth international confer- ence on learning representations , 2024. [online]. available: https://openreview.net/forum?id=v3xxtxwki6 j. jung, p . west, l. jiang, f. brahman, x. lu, j. fisher, t. sorensen, and y. choi, “impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing,” 2023. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, andj. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 c. gulcehre, t. l. paine, s. srinivasan, k. konyushkova, l. weerts, a. sharma, a. siddhant, a. ahern, m. wang, c. gu, w. macherey, a. doucet, o. firat, and n. de freitas, “reinforced self-training (rest) for language modeling,” 2023. e. zelikman, y. wu, j. mu, and n. d. goodman, “star: boot- strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,” arxiv preprint arxiv:1910.01108 , 2019. y. wen, z. li, w. du, and l. mou, “f-divergence minimization for sequence-level knowledge distillation,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 10 817–10 834. [online]. available: https: //aclanthology.org/2023.acl-long.605 c. liang, s. zuo, q. zhang, p . he, w. chen, and t. zhao, “less is more: task-aware layer-wise distillation for lan- guage model compression,” in international conference on machine learning . pmlr, 2023, pp. 20 852–20 867. m. kwon, s. m. xie, k. bullard, and d. sadigh, “reward de- sign with language models,” in iclr . openreview.net, 2023. b. peng, c. li, p . he, m. galley, and j. gao, “instruction tuning with gpt-4,” 2023. g. li, h. a. a. k. hammoud, h. itani, d. khizbullin, and b. ghanem, “camel: communicative agents for” mind” exploration of large scale language model society,” arxiv preprint arxiv:2303.17760 , 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, “openchat: advancing open-source language models with mixed-quality data,” sep. 2023, arxiv:2309.11235 [cs]. [online]. available: http://arxiv.org/abs/2309.11235 m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” arxiv preprint arxiv:2305.18395 , 2023. h. luo, y.-s. chuang, y. gong, t. zhang, y. kim, x. wu, d. fox, h. meng, and j. glass, “sail: search-augmented in- struction learning,” arxiv preprint arxiv:2305.15225 , 2023. a. asai, z. wu, y. wang, a. sil, and h. hajishirzi, “self- rag: learning to retrieve, generate, and critique through self-reflection,” arxiv preprint arxiv:2310.11511 , 2023. s. ye, y. jo, d. kim, s. kim, h. hwang, and m. seo, “selfee: iterative self-revising llm empowered by self-feedback generation,” blog post, may 2023. [online]. available: https://kaistai.github.io/selfee/ p . wang, l. li, l. chen, f. song, b. lin, y. cao, t. liu, and z. sui, “making large language models better reasoners with alignment,” 2023. d. cheng, s. huang, and f. wei, “adapting large language models via reading comprehension,” 2023. y. zhang, z. chen, y. fang, l. cheng, y. lu, f. li, w. zhang, 33 and h. chen, “knowledgeable preference alignment for llms in domain-specific question answering,” 2023. j. scheurer, j. a. campos, t. korbak, j. s. chan, a. chen, k. cho, and e. perez, “training language models with language feedback at scale,” 2023. s. kim, s. bae, j. shin, s. kang, d. kwak, k. yoo, and m. seo, “aligning large language models through synthetic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 677–13 700. [online]. available: https://aclanthology. org/2023.emnlp-main.844 p . roit, j. ferret, l. shani, r. aharoni, g. cideron, r. dadashi, m. geist, s. girgin, l. hussenot, o. keller, n. momchev, s. ramos garea, p . stanczyk, n. vieillard, o. bachem, g. elidan, a. hassidim, o. pietquin, and i. szpektor, “factually consistent summarization via reinforcement learning with textual entailment feedback,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 6252–6272. [online]. available: https://aclanthology.org/2023.acl-long.344 y. yang, e. chern, x. qiu, g. neubig, and p . liu, “alignment for honesty,” arxiv preprint arxiv:2312.07000 , 2023. r. liu, r. yang, c. jia, g. zhang, d. zhou, a. m. dai, d. yang, and s. vosoughi, “training socially aligned lan- guage models on simulated social interactions,” 2023. t. schick, j. dwivedi-yu, r. dess `ı, r. raileanu, m. lomeli, l. zettlemoyer, n. cancedda, and t. scialom, “tool- former: language models can teach themselves to use tools,” 2023. j. zhang, “graph-toolformer: to empower llms with graph reasoning ability via prompt augmented by chatgpt,” arxiv preprint arxiv:2304.11116 , 2023. s. g. patil, t. zhang, x. wang, and j. e. gonzalez, “gorilla: large language model connected with massive apis,” 2023. q. tang, z. deng, h. lin, x. han, q. liang, b. cao, and l. sun, “toolalpaca: generalized tool learning for lan- guage models with 3000 simulated cases,” 2023. y. qin, s. liang, y. ye, k. zhu, l. yan, y. lu, y. lin, x. cong, x. tang, b. qian, s. zhao, l. hong, r. tian, r. xie, j. zhou, m. gerstein, d. li, z. liu, and m. sun, “toolllm: facilitating large language models to master 16000+ real- world apis,” 2023. l. yuan, y. chen, x. wang, y. r. fung, h. peng, and h. ji, “craft: customizing llms by creating and retrieving from specialized toolsets,” 2023. s. gao, z. shi, m. zhu, b. fang, x. xin, p . ren, z. chen, j. ma, and z. ren, “confucius: iterative tool learning from introspection feedback by easy-to-difficult curriculum,” 2023. c. wang, w. luo, q. chen, h. mai, j. guo, s. dong, xiaohua, xuan, z. li, l. ma, and s. gao, “mllm-tool: a multimodal large language model for tool agent learning,” 2024. w. shen, c. li, h. chen, m. yan, x. quan, h. chen, j. zhang, and f. huang, “small llms are weak tool learners: a multi- llm agent,” 2024.b. chen, c. shu, e. shareghi, n. collier, k. narasimhan, and s. yao, “fireact: toward language agent fine-tuning,” 2023. a. zeng, m. liu, r. lu, b. wang, x. liu, y. dong, and j. tang, “agenttuning: enabling generalized agent abilities for llms,” 2023. d. yin, f. brahman, a. ravichander, k. chandu, k.-w. chang, y. choi, and b. y. lin, “lumos: learning agents with unified data, modular design, and open-source llms,” 2023. s. qiao, n. zhang, r. fang, y. luo, w. zhou, y. e. jiang, c. lv, and h. chen, “autoact: automatic agent learning from scratch via self-planning,” 2024. y. kong, j. ruan, y. chen, b. zhang, t. bao, s. shi, g. du, x. hu, h. mao, z. li, x. zeng, and r. zhao, “tptu-v2: boosting task planning and tool usage of large language model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” proceedings of the national academy of sciences , vol. 120, no. 30, jul. 2023. [online]. available: http: //dx.doi.org/10.1073/pnas.2305016120 z. wang, a. w. yu, o. firat, and y. cao, “towards zero-label language learning,” 2021. y. xu, r. xu, d. iter, y. liu, s. wang, c. zhu, and m. zeng, “inheritsumm: a general, versatile and compact summarizer by distilling from gpt,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 879–13 892. [online]. available: https: //aclanthology.org/2023.findings-emnlp.927 f. xu, w. shi, and e. choi, “recomp: improving retrieval- augmented lms with context compression and selective augmentation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=mljlvignhp s. ramnath, b. joshi, s. hallinan, x. lu, l. h. li, a. chan, j. hessel, y. choi, and x. ren, “tailoring self-rationalizers with multi-reward distillation,” 2023. s. wang, y. liu, y. xu, c. zhu, and m. zeng, “want to reduce labeling cost? gpt-3 can help,” in findings of the association for computational linguistics: emnlp 2021 , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 4195–4205. [online]. available: https: //aclanthology.org/2021.findings-emnlp.354 z. guo, p . wang, y. wang, and s. yu, “improving small language models on pubmedqa via generative data aug- mentation,” 2023. w. yang and g. nicolai, “neural machine translation data generation and augmentation using chatgpt,” 2023. k. srinivasan, k. raman, a. samanta, l. liao, l. bertelli, and m. bendersky, “quill: query intent with large language models using retrieval augmentation and multi-stage distillation,” in proceedings of the 2022 conference on empirical methods in natural language processing: industry track , y. li and a. lazaridou, eds. abu dhabi, uae: association for computational linguistics, dec. 2022, pp. 492–501. [online]. available: 34 https://aclanthology.org/2022.emnlp-industry.50 z. dai, v . y. zhao, j. ma, y. luan, j. ni, j. lu, a. bakalov, k. guu, k. b. hall, and m. chang, “promptagator: few-shot dense retrieval from 8 examples,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=gml46ympu2j r. meng, y. liu, s. yavuz, d. agarwal, l. tu, n. yu, j. zhang, m. bhat, and y. zhou, “augtriever: unsupervised dense retrieval by scalable data augamentation,” 2023. w. sun, l. yan, x. ma, s. wang, p . ren, z. chen, d. yin, and z. ren, “is chatgpt good at search? investigating large language models as re-ranking agents,” 2023. r. pradeep, s. sharifymoghaddam, and j. lin, “rankvicuna: zero-shot listwise document reranking with open-source large language models,” 2023. ——, “rankzephyr: effective and robust zero-shot listwise reranking is a breeze!” 2023. f. ferraretto, t. laitz, r. lotufo, and r. nogueira, “exaranker: synthetic explanations improve neural rankers,” in proceedings of the 46th international acm sigir conference on research and development in information retrieval , ser. sigir ’23. new york, ny, usa: association for computing machinery, 2023, p. 2409–2414. [online]. available: https://doi.org/10.1145/3539618.3592067 s. mysore, a. mccallum, and h. zamani, “large language model augmented narrative driven recommendations,” inproceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 777–783. [online]. available: https://doi.org/10.1145/3604915.3608829 j. zhang, r. xie, y. hou, w. x. zhao, l. lin, and j.-r. wen, “recommendation as instruction following: a large language model empowered recommendation approach,” 2023. q. liu, n. chen, t. sakai, and x.-m. wu, “once: boost- ing content-based recommendation with both open- and closed-source large language models,” 2023. s. kim, j. shin, y. cho, j. jang, s. longpre, h. lee, s. yun, s. shin, s. kim, j. thorne, and m. seo, “prometheus: inducing evaluation capability in language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=8eujatvekw w. xu, d. wang, l. pan, z. song, m. freitag, w. wang, and l. li, “instructscore: towards explainable text generation evaluation with automatic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5967–5994. [online]. available: https://aclanthology.org/2023.emnlp-main.365 d. jiang, y. li, g. zhang, w. huang, b. y. lin, and w. chen, “tigerscore: towards building explainable metric for all text generation tasks,” 2023. j. li, s. sun, w. yuan, r.-z. fan, hai zhao, and p . liu, “generative judge for evaluating alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=gtkfw6szgsb. rozi `ere, j. gehring, f. gloeckle, s. sootla, i. gat, x. e. tan, y. adi, j. liu, t. remez, j. rapin, a. kozhevnikov, i. evtimov, j. bitton, m. bhatt, c. c. ferrer, a. grattafiori, w. xiong, a. d ´efossez, j. copet, f. azhar, h. touvron, l. martin, n. usunier, t. scialom, and g. synnaeve, “code llama: open foundation models for code,” 2023. b. liu, c. chen, c. liao, z. gong, h. wang, z. lei, m. liang, d. chen, m. shen, h. zhou, h. yu, and j. li, “mftcoder: boosting code llms with multitask fine-tuning,” 2023. n. jain, t. zhang, w. chiang, j. e. gonzalez, k. sen, and i. stoica, “llm-assisted code cleaning for training accurate code generators,” corr , vol. abs/2311.14904, 2023. [online]. available: https://doi.org/10.48550/ arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up visual instruction tuning,” 2023. j. wang, l. meng, z. weng, b. he, z. wu, and y.-g. jiang, “to see is to believe: prompting gpt-4v for better visual instruction tuning,” 2023. k. chen, z. zhang, w. zeng, r. zhang, f. zhu, and r. zhao, “shikra: unleashing multimodal llm’s referential dialogue magic,” 2023. j. s. park, j. hessel, k. r. chandu, p . p . liang, x. lu, p . west, y. yu, q. huang, j. gao, a. farhadi, and y. choi, “localized symbolic knowledge distillation for visual commonsense models,” 2023. r. pi, j. gao, s. diao, r. pan, h. dong, j. zhang, l. yao, j. han, h. xu, l. kong, and t. zhang, “detgpt: detect what you need via reasoning,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 14 172–14 189. [online]. available: https: //aclanthology.org/2023.emnlp-main.876 l. zhao, e. yu, z. ge, j. yang, h. wei, h. zhou, j. sun, y. peng, r. dong, c. han, and x. zhang, “chatspot: bootstrapping multimodal llms via precise referring in- struction tuning,” 2023. f. liu, k. lin, l. li, j. wang, y. yacoob, and l. wang, “mitigating hallucination in large multi-modal models via robust instruction tuning,” 2023. s. wu, h. fei, l. qu, w. ji, and t.-s. chua, “next-gpt: any- to-any multimodal llm,” 2023. r. luo, z. zhao, m. yang, j. dong, d. li, p . lu, t. wang, l. hu, m. qiu, and z. wei, “valley: video assistant with large language model enhanced ability,” 2023. y. jiang, e. schoop, a. swearngin, and j. nichols, “iluvui: instruction-tuned language-vision modeling of uis from machine conversations,” 2023. y. li, c. zhang, g. yu, z. wang, b. fu, g. lin, c. shen, l. chen, and y. wei, “stablellava: enhanced visual in- struction tuning with synthesized image-dialogue data,” 2023. r. xu, x. wang, t. wang, y. chen, j. pang, and d. lin, “pointllm: empowering large language models to under- stand point clouds,” 2023. q. huang, m. tao, z. an, c. zhang, c. jiang, z. chen, z. wu, and y. feng, “lawyer llama technical report,” arxiv preprint arxiv:2305.15062 , 2023. 35 j. cui, z. li, y. yan, b. chen, and l. yuan, “chatlaw: open- source legal large language model with integrated ex- ternal knowledge bases,” arxiv preprint arxiv:2306.16092 , 2023. h. zhang, j. chen, f. jiang, f. yu, z. chen, g. chen, j. li, x. wu, z. zhiyi, q. xiao, x. wan, b. wang, and h. li, “huatuogpt, towards taming language model to be a doctor,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 10 859– 10 885. [online]. available: https://aclanthology.org/ 2023.findings-emnlp.725 j. chen, x. wang, a. gao, f. jiang, s. chen, h. zhang, d. song, w. xie, c. kong, j. li, x. wan, h. li, and b. wang, “huatuogpt-ii, one-stage training for medical adaption of llms,” corr , vol. abs/2311.09774, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09774 x. zhang and q. yang, “xuanyuan 2.0: a large chinese financial chat model with hundreds of billions parameters,” in proceedings of the 32nd acm international conference on information and knowledge management, cikm 2023, birmingham, united kingdom, october 21- 25, 2023 , i. frommholz, f. hopfgartner, m. lee, m. oakes, m. lalmas, m. zhang, and r. l. t. santos, eds. acm, 2023, pp. 4435–4439. [online]. available: https://doi.org/10.1145/3583780.3615285 t. xie, y. wan, w. huang, z. yin, y. liu, s. wang, q. linghu, c. kit, c. grazian, w. zhang, i. razzak, and b. hoex, “darwin series: domain specific large language models for natural science,” corr , vol. abs/2308.13565, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2308.13565 y. dan, z. lei, y. gu, y. li, j. yin, j. lin, l. ye, z. tie, y. zhou, y. wang, a. zhou, z. zhou, q. chen, j. zhou, l. he, and x. qiu, “educhat: a large-scale language model-based chatbot system for intelligent education,” corr , vol. abs/2308.02773, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.02773 h. guo, j. yang, j. liu, l. yang, l. chai, j. bai, j. peng, x. hu, c. chen, d. zhang, x. shi, t. zheng, l. zheng, b. zhang, k. xu, and z. li, “owl: a large language model for it operations,” corr , vol. abs/2309.09298, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.09298 y. kim and a. m. rush, “sequence-level knowledge distil- lation,” arxiv preprint arxiv:1606.07947 , 2016. s. han, h. mao, and w. j. dally, “deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding,” international confer- ence on learning representations (iclr) , 2016. v . gangal, s. y. feng, m. alikhani, t. mitamura, and e. hovy, “nareor: the narrative reordering problem,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 10 645–10 653. s. longpre, y. lu, z. tu, and c. dubois, “an exploration of data augmentation and sampling techniques for domain- agnostic question answering,” in proceedings of the 2nd workshop on machine reading for question answering , a. fisch, a. talmor, r. jia, m. seo, e. choi, and d. chen, eds. hong kong, china: association for computational linguistics, nov. 2019, pp. 220–227. [online]. available:https://aclanthology.org/d19-5829 p . west, c. bhagavatula, j. hessel, j. hwang, l. jiang, r. le bras, x. lu, s. welleck, and y. choi, “symbolic knowledge distillation: from general language models to commonsense models,” in proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 4602–4625. [online]. available: https://aclanthology.org/2022.naacl-main.341 z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, j. qian, b. peng, y. mao, w. chen, and x. yan, “explana- tions from large language models make small reasoners better,” 2022. n. ho, l. schmid, and s. yun, “large language models are reasoning teachers,” in acl (1) . association for computational linguistics, 2023, pp. 14 852–14 882. l. c. magister, j. mallinson, j. adamek, e. malmi, and a. severyn, “teaching small language models to reason,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: short papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 1773–1781. [online]. available: https://aclanthology.org/2023.acl-short.151 y. fu, h. peng, l. ou, a. sabharwal, and t. khot, “specializ- ing smaller language models towards multi-step reason- ing,” 2023. l. h. li, j. hessel, y. yu, x. ren, k.-w. chang, and y. choi, “symbolic chain-of-thought distillation: small models can also “think” step-by-step,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2665–2679. [online]. available: https://aclanthology.org/2023.acl- long.150 w. liu, g. li, k. zhang, b. du, q. chen, x. hu, h. xu, j. chen, and j. wu, “mind’s mirror: distilling self- evaluation capability and comprehensive thinking from large language models,” 2023. s. longpre, l. hou, t. vu, a. webson, h. w. chung, y. tay, d. zhou, q. v . le, b. zoph, j. wei et al. , “the flan collec- tion: designing data and methods for effective instruction tuning,” arxiv preprint arxiv:2301.13688 , 2023. y. anand, z. nussbaum, b. duderstadt, b. schmidt, and a. mulyar, “gpt4all: training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo,” github , 2023. q. si, t. wang, z. lin, x. zhang, y. cao, and w. wang, “an empirical study of instruction-tuning large language models in chinese,” in emnlp (findings) . association for computational linguistics, 2023, pp. 4086–4107. y. ji, y. deng, y. gong, y. peng, q. niu, l. zhang, b. ma, and x. li, “exploring the impact of instruction data scaling on large language models: an empirical study on real-world use cases,” 2023. m. wu, a. waheed, c. zhang, m. abdul-mageed, and a. f. 36 aji, “lamini-lm: a diverse herd of distilled models from large-scale instructions,” 2023. w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” 2023. y. yu, y. zhuang, j. zhang, y. meng, a. ratner, r. krishna, j. shen, and c. zhang, “large language model as at- tributed training data generator: a tale of diversity and bias,” 2023. f. wan, x. huang, d. cai, x. quan, w. bi, and s. shi, “knowledge fusion of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=jidsk12qcz q. zhao and b. zhu, “towards the fundamental limits of knowledge transfer over finite domains,” inneurips 2023 workshop on mathematics of modern machine learning , 2023. [online]. available: https: //openreview.net/forum?id=9qxoxqxa0n c. qin, w. xia, f. jiao, and s. joty, “improving in-context learning via bidirectional alignment,” 2023. n. boizard, k. el-haddad, c. hudelot, and p . colombo, “towards cross-tokenizer distillation: the universal logit distillation loss for llms,” arxiv preprint arxiv:2402.12030 , 2024. q. zhong, l. ding, l. shen, j. liu, b. du, and d. tao, “revis- iting knowledge distillation for autoregressive language models,” 2024. m. kim, s. lee, j. lee, s. hong, d.-s. chang, w. sung, and j. choi, “token-scaled logit distillation for ternary weight generative language models,” arxiv preprint arxiv:2308.06744 , 2023. z. chen, k. zhou, w. x. zhao, j. wan, f. zhang, d. zhang, and j.-r. wen, “improving large language models via fine- grained reinforcement learning with minimum editing constraint,” 2024. g. guo, r. zhao, t. tang, x. zhao, and j.-r. wen, “beyond imitation: leveraging fine-grained quality signals for alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=lnlju5c5dk z. allen-zhu and y. li, “towards understanding ensemble, knowledge distillation and self-distillation in deep learn- ing,” arxiv preprint arxiv:2012.09816 , 2020. t. zheng, s. guo, x. qu, j. guo, w. zhang, x. du, c. lin, w. huang, w. chen, j. fu et al. , “kun: answer polish- ment for chinese self-alignment with instruction back- translation,” arxiv preprint arxiv:2401.06477 , 2024. x. li, p . yu, c. zhou, t. schick, o. levy, l. zettlemoyer, j. e. weston, and m. lewis, “self-alignment with instruction backtranslation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=1oijhjbrst b. zhao, h. hajishirzi, and q. cao, “apt: adaptive pruning and tuning pretrained language models for efficient train- ing and inference,” arxiv preprint arxiv:2401.12200 , 2024. a. singh, j. d. co-reyes, r. agarwal, a. anand, p . patil, p . j. liu, j. harrison, j. lee, k. xu, a. parisi et al. , “beyond hu- man data: scaling self-training for problem-solving with language models,” arxiv preprint arxiv:2312.06585 , 2023. w. chen, d. song, and b. li, “grath: gradual self-truthifyingfor large language models,” 2024. a. hosseini, x. yuan, n. malkin, a. courville, a. sordoni, and r. agarwal, “v-star: training verifiers for self-taught reasoners,” 2024. a. askell, y. bai, a. chen, d. drain, d. ganguli, t. henighan, a. jones, n. joseph, b. mann, n. dassarma, n. elhage, z. hatfield-dodds, d. hernandez, j. kernion, k. ndousse, c. olsson, d. amodei, t. brown, j. clark, s. mccandlish, c. olah, and j. kaplan, “a general lan- guage assistant as a laboratory for alignment,” 2021. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, and j. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 h. chen, x. quan, h. chen, m. yan, and j. zhang, “knowl- edge distillation for closed-source language models,” arxiv preprint arxiv:2401.07013 , 2024. i. sason and s. verd ´u, “f-divergence inequalities,” ieee transactions on information theory , vol. 62, no. 11, pp. 5973– 6006, 2016. s. sun, y. cheng, z. gan, and j. liu, “patient knowledge distillation for bert model compression,” 2019. z. sun, h. yu, x. song, r. liu, y. yang, and d. zhou, “mobilebert: a compact task-agnostic bert for resource-limited devices,” in proceedings of the 58th annual meeting of the association for computational linguistics , d. jurafsky, j. chai, n. schluter, and j. tetreault, eds. online: association for computational linguistics, jul. 2020, pp. 2158–2170. [online]. available: https://aclanthology.org/2020.acl-main.195 x. jiao, y. yin, l. shang, x. jiang, x. chen, l. li, f. wang, and q. liu, “tinybert: distilling bert for natural language understanding,” in findings of the association for computational linguistics: emnlp 2020 , t. cohn, y. he, and y. liu, eds. online: association for computational linguistics, nov. 2020, pp. 4163–4174. [online]. available: https://aclanthology.org/2020.findings-emnlp.372 l. hou, z. huang, l. shang, x. jiang, x. chen, and q. liu, “dynabert: dynamic bert with adaptive width and depth,” advances in neural information processing systems , vol. 33, pp. 9782–9793, 2020. s. zuo, q. zhang, c. liang, p . he, t. zhao, and w. chen, “moebert: from bert to mixture-of-experts via importance- guided adaptation,” arxiv preprint arxiv:2204.07675 , 2022. k. j. liang, w. hao, d. shen, y. zhou, w. chen, c. chen, and l. carin, “mixkd: towards efficient distillation of large- scale language models,” in 9th international conference on learning representations, iclr 2021, virtual event, austria, may 3-7, 2021 . openreview.net, 2021. [online]. available: https://openreview.net/forum?id=ufgeeljklu5 y. j. ma, w. liang, g. wang, d.-a. huang, o. bastani, d. ja- yaraman, y. zhu, l. fan, and a. anandkumar, “eureka: human-level reward design via coding large language models,” 2023. j.-c. pang, p . wang, k. li, x.-h. chen, j. xu, z. zhang, and y. yu, “language model self-improvement by reinforce- ment learning contemplation,” 2023. y. du, o. watkins, z. wang, c. colas, t. darrell, p . abbeel, 37 a. gupta, and j. andreas, “guiding pretraining in reinforcement learning with large language models,” inproceedings of the 40th international conference on machine learning , ser. proceedings of machine learning research, a. krause, e. brunskill, k. cho, b. engelhardt, s. sabato, and j. scarlett, eds., vol. 202. pmlr, 23–29 jul 2023, pp. 8657–8677. [online]. available: https://proceedings.mlr.press/v202/du23f.html j. schulman, f. wolski, p . dhariwal, a. radford, and o. klimov, “proximal policy optimization algorithms,” 2017. r. rafailov, a. sharma, e. mitchell, s. ermon, c. d. man- ning, and c. finn, “direct preference optimization: your language model is secretly a reward model,” 2023. f. song, b. yu, m. li, h. yu, f. huang, y. li, and h. wang, “preference ranking optimization for human alignment,” arxiv preprint arxiv:2306.17492 , 2023. z. yuan, h. yuan, c. tan, w. wang, s. huang, and f. huang, “rrhf: rank responses to align language mod- els with human feedback without tears,” arxiv preprint arxiv:2304.05302 , 2023. m. li, l. chen, j. chen, s. he, and t. zhou, “reflection-tuning: recycling data for better instruction- tuning,” in neurips 2023 workshop on instruction tuning and instruction following , 2023. [online]. available: https://openreview.net/forum?id=xaqozzqkpu m. li, l. chen, j. chen, s. he, j. gu, and t. zhou, “selective reflection-tuning: student-selected data recycling for llm instruction-tuning,” 2024. [online]. available: https: //api.semanticscholar.org/corpusid:267682220 x. geng, a. gudibande, h. liu, e. wallace, p . abbeel, s. levine, and d. song, “koala: a dialogue model for academic research,” blog post, april 2023. [online]. available: https://bair.berkeley.edu/blog/2023/04/03/ koala/ m. li, j. chen, l. chen, and t. zhou, “can llms speak for diverse people? tuning llms via debate to generate controllable controversial statements,” 2024. m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” 2023. r. yang, l. song, y. li, s. zhao, y. ge, x. li, and y. shan, “gpt4tools: teaching large language model to use tools via self-instruction,” 2023. a. yehudai, b. carmeli, y. mass, o. arviv, n. mills, a. toledo, e. shnarch, and l. choshen, “genie: achieving human parity in content-grounded datasets generation,” 2024. y. zhang, r. zhang, j. gu, y. zhou, n. lipka, d. yang, and t. sun, “llavar: enhanced visual instruction tuning for text-rich image understanding,” 2023. c. lyu, m. wu, l. wang, x. huang, b. liu, z. du, s. shi, and z. tu, “macaw-llm: multi-modal language modeling with image, audio, video, and text integration,” arxiv preprint arxiv:2306.09093 , 2023. b. li, y. zhang, l. chen, j. wang, f. pu, j. yang, c. li, and z. liu, “mimic-it: multi-modal in-context instruction tuning,” 2023. z. zhao, l. guo, t. yue, s. chen, s. shao, x. zhu, z. yuan, and j. liu, “chatbridge: bridging modalities with large language model as a language catalyst,” 2023.y. zhao, b. yu, b. hui, h. yu, f. huang, y. li, and n. l. zhang, “a preliminary study of the intrinsic relationship between complexity and alignment,” 2023. a. gudibande, e. wallace, c. snell, x. geng, h. liu, p . abbeel, s. levine, and d. song, “the false promise of imitating proprietary llms,” arxiv preprint arxiv:2305.15717 , 2023. c. zhou, p . liu, p . xu, s. iyer, j. sun, y. mao, x. ma, a. efrat, p . yu, l. yu, s. zhang, g. ghosh, m. lewis, l. zettlemoyer, and o. levy, “lima: less is more for alignment,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview.net/forum?id=kbmokmx2he m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to-strong data filtering for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and z. mao, “expertprompting: instructing large language models to be distinguished experts,” 2023. w. liu, w. zeng, k. he, y. jiang, and j. he, “what makes good data for alignment? a comprehensive study of auto- matic data selection in instruction tuning,” 2023. r. lou, k. zhang, j. xie, y. sun, j. ahn, h. xu, y. su, and w. yin, “muffin: curating multi-faceted instructions for improving instruction-following,” 2023. t. schick, j. dwivedi-yu, z. jiang, f. petroni, p . lewis, g. izacard, q. you, c. nalmpantis, e. grave, and s. riedel, “peer: a collaborative language model,” 2022. a. madaan, n. tandon, p . gupta, s. hallinan, l. gao, s. wiegreffe, u. alon, n. dziri, s. prabhumoye, y. yang, s. gupta, b. p . majumder, k. hermann, s. welleck, a. yaz- danbakhsh, and p . clark, “self-refine: iterative refinement with self-feedback,” 2023. w. saunders, c. yeh, j. wu, s. bills, l. ouyang, j. ward, and j. leike, “self-critiquing models for assisting human evaluators,” 2022. d. m. ziegler, n. stiennon, j. wu, t. b. brown, a. radford, d. amodei, p . christiano, and g. irving, “fine-tuning language models from human preferences,” arxiv preprint arxiv:1909.08593 , 2019. n. stiennon, l. ouyang, j. wu, d. ziegler, r. lowe, c. voss, a. radford, d. amodei, and p . f. christiano, “learning to summarize with human feedback,” advances in neu- ral information processing systems , vol. 33, pp. 3008–3021, 2020. j. wu, l. ouyang, d. m. ziegler, n. stiennon, r. lowe, j. leike, and p . christiano, “recursively summarizing books with human feedback,” 2021. y. bai, a. jones, k. ndousse, a. askell, a. chen, n. das- sarma, d. drain, s. fort, d. ganguli, t. henighan et al. , “training a helpful and harmless assistant with rein- forcement learning from human feedback,” arxiv preprint arxiv:2204.05862 , 2022. a. k ¨opf, y. kilcher, d. von r ¨utte, s. anagnostidis, z.-r. tam, k. stevens, a. barhoum, n. m. duc, o. stanley, r. nagyfi, s. es, s. suri, d. glushkov, a. dantuluri, a. maguire, c. schuhmann, h. nguyen, and a. mattick, “openassis- tant conversations – democratizing large language model alignment,” 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, 38 “openchat: advancing open-source language models with mixed-quality data,” 2023. l. weidinger, j. mellor, m. rauh, c. griffin, j. uesato, p .- s. huang, m. cheng, m. glaese, b. balle, a. kasirzadeh, z. kenton, s. brown, w. hawkins, t. stepleton, c. biles, a. birhane, j. haas, l. rimell, l. a. hendricks, w. isaac, s. legassick, g. irving, and i. gabriel, “ethical and social risks of harm from language models,” 2021. j. ji, m. liu, j. dai, x. pan, c. zhang, c. bian, c. zhang, r. sun, y. wang, and y. yang, “beavertails: towards improved safety alignment of llm via a human-preference dataset,” 2023. i. solaiman and c. dennison, “process for adapting lan- guage models to society (palms) with values-targeted datasets,” advances in neural information processing sys- tems , vol. 34, pp. 5861–5873, 2021. l. qiu, y. zhao, j. li, p . lu, b. peng, j. gao, and s.-c. zhu, “valuenet: a new dataset for human value driven dialogue system,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 11 183– 11 191. j. kiesel, m. alshomary, n. handke, x. cai, h. wachsmuth, and b. stein, “identifying the human values behind arguments,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4459–4471. [online]. available: https://aclanthology.org/2022.acl-long.306 r. liu, g. zhang, x. feng, and s. vosoughi, “aligning generative language models with human values,” in findings of the association for computational linguistics: naacl 2022 , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 241– 252. [online]. available: https://aclanthology.org/2022. findings-naacl.18 a. glaese, n. mcaleese, m. trebacz, j. aslanides, v . firoiu, t. ewalds, m. rauh, l. weidinger, m. chadwick, p . thacker et al. , “improving alignment of dialogue agents via targeted human judgements,” arxiv preprint arxiv:2209.14375 , 2022. h. sun, z. zhang, f. mi, y. wang, w. liu, j. cui, b. wang, q. liu, and m. huang, “moraldial: a framework to train and evaluate moral dialogue systems via moral discussions,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2213–2230. [online]. available: https://aclanthology.org/2023.acl-long.123 j. yao, x. yi, x. wang, j. wang, and x. xie, “from instructions to intrinsic human values – a survey of alignment goals for big models,” 2023. y. liu, y. yao, j.-f. ton, x. zhang, r. g. h. cheng, y. klochkov, m. f. taufiq, and h. li, “trustworthy llms: a survey and guideline for evaluating large language models’ alignment,” arxiv preprint arxiv:2308.05374 , 2023. j. qian, h. wang, z. li, s. li, and x. yan, “limitations of language models in arithmetic and symbolic induction,” 2022.x. she, y. liu, y. zhao, y. he, l. li, c. tantithamthavorn, z. qin, and h. wang, “pitfalls in language models for code intelligence: a taxonomy and survey,” 2023. h. manikandan, y. jiang, and j. z. kolter, “language models are weak learners,” 2023. y. liang, c. wu, t. song, w. wu, y. xia, y. liu, y. ou, s. lu, l. ji, s. mao, y. wang, l. shou, m. gong, and n. duan, “taskmatrix.ai: completing tasks by connecting foundation models with millions of apis,” 2023. g. mialon, r. dess `ı, m. lomeli, c. nalmpantis, r. pa- sunuru, r. raileanu, b. rozi `ere, t. schick, j. dwivedi- yu, a. celikyilmaz, e. grave, y. lecun, and t. scialom, “augmented language models: a survey,” 2023. a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, k. cobbe, t. eloundou, g. krueger, k. button, m. knight, b. chess, and j. schulman, “webgpt: browser-assisted question-answering with human feedback,” 2022. y. qin, z. cai, d. jin, l. yan, s. liang, k. zhu, y. lin, x. han, n. ding, h. wang, r. xie, f. qi, z. liu, m. sun, and j. zhou, “webcpm: interactive web search for chinese long-form question answering,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8968–8988. [online]. available: https://aclanthology.org/2023.acl-long.499 y. song, w. xiong, d. zhu, w. wu, h. qian, m. song, h. huang, c. li, k. wang, r. yao, y. tian, and s. li, “restgpt: connecting large language models with real- world restful apis,” 2023. t. cai, x. wang, t. ma, x. chen, and d. zhou, “large language models as tool makers,” 2023. y. shen, k. song, x. tan, d. li, w. lu, and y. zhuang, “hugginggpt: solving ai tasks with chatgpt and its friends in hugging face,” 2023. s. hao, t. liu, z. wang, and z. hu, “toolkengpt: augment- ing frozen language models with massive tools via tool embeddings,” 2024. s. yuan, k. song, j. chen, x. tan, y. shen, r. kan, d. li, and d. yang, “easytool: enhancing llm-based agents with concise tool instruction,” 2024. s. zhang, s. roller, n. goyal, m. artetxe, m. chen, s. chen, c. dewan, m. diab, x. li, x. v . lin, t. mihaylov, m. ott, s. shleifer, k. shuster, d. simig, p . s. koura, a. sridhar, t. wang, and l. zettlemoyer, “opt: open pre-trained transformer language models,” 2022. t. brown, b. mann, n. ryder, m. subbiah, j. d. ka- plan, p . dhariwal, a. neelakantan, p . shyam, g. sastry, a. askell et al. , “language models are few-shot learners,” advances in neural information processing systems , vol. 33, pp. 1877–1901, 2020. w. huang, p . abbeel, d. pathak, and i. mordatch, “lan- guage models as zero-shot planners: extracting actionable knowledge for embodied agents,” in international confer- ence on machine learning . pmlr, 2022, pp. 9118–9147. i. singh, v . blukis, a. mousavian, a. goyal, d. xu, j. trem- blay, d. fox, j. thomason, and a. garg, “progprompt: 39 generating situated robot task plans using large language models,” 2022. d. zhou, n. sch ¨arli, l. hou, j. wei, n. scales, x. wang, d. schuurmans, c. cui, o. bousquet, q. le, and e. chi, “least-to-most prompting enables complex reasoning in large language models,” 2023. c. h. song, j. wu, c. washington, b. m. sadler, w.-l. chao, and y. su, “llm-planner: few-shot grounded planning for embodied agents with large language models,” in proceed- ings of the ieee/cvf international conference on computer vision , 2023, pp. 2998–3009. z. wang, s. cai, a. liu, x. ma, and y. liang, “describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents,” arxiv preprint arxiv:2302.01560 , 2023. s. yao, d. yu, j. zhao, i. shafran, t. l. griffiths, y. cao, and k. narasimhan, “tree of thoughts: deliberate prob- lem solving with large language models,” arxiv preprint arxiv:2305.10601 , 2023. b. liu, y. jiang, x. zhang, q. liu, s. zhang, j. biswas, and p . stone, “llm+ p: empowering large language mod- els with optimal planning proficiency,” arxiv preprint arxiv:2304.11477 , 2023. s. hao, y. gu, h. ma, j. j. hong, z. wang, d. z. wang, and z. hu, “reasoning with language model is planning with world model,” arxiv preprint arxiv:2305.14992 , 2023. m. hu, y. mu, x. yu, m. ding, s. wu, w. shao, q. chen, b. wang, y. qiao, and p . luo, “tree-planner: efficient close-loop task planning with large language models,” arxiv preprint arxiv:2310.08582 , 2023. b. y. lin, c. huang, q. liu, w. gu, s. sommerer, and x. ren, “on grounded planning for embodied tasks with language models,” in proceedings of the aaai conference on artificial intelligence , vol. 37, no. 11, 2023, pp. 13 192– 13 200. k. valmeekam, m. marquez, s. sreedharan, and s. kambhampati, “on the planning abilities of large language models - a critical investigation,” in thirty-seventh conference on neural informa- tion processing systems , 2023. [online]. available: https://openreview.net/forum?id=x6deqxisew t. sumers, k. marino, a. ahuja, r. fergus, and i. dasgupta, “distilling internet-scale vision-language models into em- bodied agents,” in proceedings of the 40th international conference on machine learning , ser. icml’23. jmlr.org, 2023. y. yang, t. zhou, k. li, d. tao, l. li, l. shen, x. he, j. jiang, and y. shi, “embodied multi-modal agent trained by an llm from a parallel textworld,” 2023. a. vaswani, n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez, ł. kaiser, and i. polosukhin, “attention is all you need,” advances in neural information processing systems , vol. 30, 2017. y. liu, m. ott, n. goyal, j. du, m. joshi, d. chen, o. levy, m. lewis, l. zettlemoyer, and v . stoyanov, “roberta: a robustly optimized bert pretraining approach,” 2019. j. li, l. gui, y. zhou, d. west, c. aloisi, and y. he, “dis- tilling chatgpt for explainable automated student answer assessment,” in emnlp (findings) . association for com- putational linguistics, 2023, pp. 6007–6026. r. tang, x. han, x. jiang, and x. hu, “does syntheticdata generation of llms help clinical text mining?” arxiv preprint arxiv:2303.04360 , 2023. x. he, i. nassar, j. kiros, g. haffari, and m. norouzi, “generate, annotate, and learn: nlp with synthetic text,” trans. assoc. comput. linguistics , vol. 10, pp. 826–842, 2022. [online]. available: https://transacl.org/ojs/index. php/tacl/article/view/3811 y. meng, j. huang, y. zhang, and j. han, “generating training data with language models: towards zero-shot language understanding,” in advances in neural information processing systems 35: annual conference on neural information processing systems 2022, neurips 2022, new orleans, la, usa, november 28 - december 9, 2022 , 2022. [online]. available: http://papers.nips.cc/paper files/ paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms generate high-quality synthetic note-oriented doctor- patient conversations?” in proceedings of the 5th clinical natural language processing workshop , t. naumann, a. ben abacha, s. bethard, k. roberts, and a. rumshisky, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 460–471. [online]. available: https://aclanthology.org/2023.clinicalnlp-1.49 z. yang, s. cherian, and s. vucetic, “data augmentation for radiology report simplification,” in findings of the association for computational linguistics: eacl 2023 , a. vlachos and i. augenstein, eds. dubrovnik, croatia: association for computational linguistics, may 2023, pp. 1922–1932. [online]. available: https: //aclanthology.org/2023.findings-eacl.144 z. cai, c. tao, t. shen, c. xu, x. geng, x. a. lin, l. he, and d. jiang, “hyper: multitask hyper-prompted training en- ables large-scale retrieval generalization,” in the eleventh international conference on learning representations , 2022. c. liu, c. tao, x. geng, t. shen, d. zhao, c. xu, b. jiao, and d. jiang, “adam: dense retrieval distillation with adaptive dark examples,” arxiv preprint arxiv:2212.10192 , 2022. j. feng, c. tao, x. geng, t. shen, c. xu, g. long, d. zhao, and d. jiang, “knowledge refinement via interaction be- tween search engines and large language models,” arxiv preprint arxiv:2305.07402 , 2023. t. shen, g. long, x. geng, c. tao, t. zhou, and d. jiang, “large language models are strong zero-shot retriever,” arxiv preprint arxiv:2304.14233 , 2023. x. ma, x. zhang, r. pradeep, and j. lin, “zero-shot listwise document reranking with a large language model,” 2023. z. qin, r. jagerman, k. hui, h. zhuang, j. wu, j. shen, t. liu, j. liu, d. metzler, x. wang, and m. bendersky, “large language models are effective text rankers with pairwise ranking prompting,” 2023. x. ma, y. gong, p . he, h. zhao, and n. duan, “query rewriting in retrieval-augmented large language models,” inproceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5303–5315. [online]. available: https://aclanthology.org/2023.emnlp-main.322 40 d. sachan, m. lewis, m. joshi, a. aghajanyan, w.- t. yih, j. pineau, and l. zettlemoyer, “improving passage retrieval with zero-shot question generation,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 3781–3797. [online]. available: https://aclanthology.org/2022.emnlp-main.249 d. s. sachan, m. lewis, d. yogatama, l. zettlemoyer, j. pineau, and m. zaheer, “questions are all you need to train a dense passage retriever,” transactions of the association for computational linguistics , vol. 11, pp. 600–616, 2023. [online]. available: https://aclanthology. org/2023.tacl-1.35 t. schick and h. sch ¨utze, “generating datasets with pretrained language models,” in proceedings of the 2021 conference on empirical methods in natural language processing , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. online and punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 6943–6951. [online]. available: https: //aclanthology.org/2021.emnlp-main.555 z. peng, x. wu, and y. fang, “soft prompt tuning for augmenting dense retrieval with large language models,” arxiv preprint arxiv:2307.08303 , 2023. j. saad-falcon, o. khattab, k. santhanam, r. florian, m. franz, s. roukos, a. sil, m. a. sultan, and c. potts, “udapdr: unsupervised domain adaptation via llm prompting and distillation of rerankers,” in proceedings of the 2023 conference on empirical methods in natural language processing, emnlp 2023, singapore, december 6-10, 2023 , 2023, pp. 11 265–11 279. [online]. available: https://aclanthology.org/2023.emnlp-main.693 v . jeronymo, l. bonifacio, h. abonizio, m. fadaee, r. lotufo, j. zavrel, and r. nogueira, “inpars-v2: large language models as efficient dataset generators for infor- mation retrieval,” arxiv preprint arxiv:2301.01820 , 2023. w. sun, z. chen, x. ma, l. yan, s. wang, p . ren, z. chen, d. yin, and z. ren, “instruction distillation makes large language models efficient zero-shot rankers,” 2023. c. raffel, n. shazeer, a. roberts, k. lee, s. narang, m. matena, y. zhou, w. li, and p . j. liu, “exploring the limits of transfer learning with a unified text-to-text transformer,” j. mach. learn. res. , vol. 21, no. 1, jan 2020. s. bruch, x. wang, m. bendersky, and m. najork, “an analysis of the softmax cross entropy loss for learning- to-rank with binary relevance,” in proceedings of the 2019 acm sigir international conference on theory of information retrieval, ictir 2019, santa clara, ca, usa, october 2-5, 2019 , 2019, pp. 75–78. [online]. available: https://doi.org/10.1145/3341981.3344221 c. burges, t. shaked, e. renshaw, a. lazier, m. deeds, n. hamilton, and g. hullender, “learning to rank using gradient descent,” in proceedings of the 22nd international conference on machine learning , ser. icml ’05. new york, ny, usa: association for computing machinery, 2005, p. 89–96. [online]. available: https: //doi.org/10.1145/1102351.1102363 x. wang, c. li, n. golbandi, m. bendersky, and m. najork, “the lambdaloss framework for ranking metricoptimization,” in proceedings of the 27th acm international conference on information and knowledge management , ser. cikm ’18. new york, ny, usa: association for computing machinery, 2018, p. 1313–1322. [online]. available: https://doi.org/10.1145/3269206.3271784 w. wang, x. lin, f. feng, x. he, and t.-s. chua, “generative recommendation: towards next-generation recommender paradigm,” 2023. s. dai, n. shao, h. zhao, w. yu, z. si, c. xu, z. sun, x. zhang, and j. xu, “uncovering chatgpt’s capabilities in recommender systems,” in proceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 y. xi, w. liu, j. lin, x. cai, h. zhu, j. zhu, b. chen, r. tang, w. zhang, r. zhang, and y. yu, “towards open- world recommendation with knowledge augmentation from large language models,” 2023. x. ren, w. wei, l. xia, l. su, s. cheng, j. wang, d. yin, and c. huang, “representation learning with large language models for recommendation,” 2023. w. wei, x. ren, j. tang, q. wang, l. su, s. cheng, j. wang, d. yin, and c. huang, “llmrec: large language models with graph augmentation for recommendation,” 2024. l. wang, s. zhang, y. wang, e.-p . lim, and y. wang, “llm4vis: explainable visualization recommendation using chatgpt,” in proceedings of the 2023 conference on empirical methods in natural language processing: industry track , m. wang and i. zitouni, eds. singapore: association for computational linguistics, dec. 2023, pp. 675–692. [online]. available: https://aclanthology.org/ 2023.emnlp-industry.64 z. cui, j. ma, c. zhou, j. zhou, and h. yang, “m6-rec: generative pretrained language models are open-ended recommender systems,” 2022. p . liu, l. zhang, and j. a. gulla, “pre-train, prompt and recommendation: a comprehensive survey of language modelling paradigm adaptations in recommender sys- tems,” 2023. k. papineni, s. roukos, t. ward, and w.-j. zhu, “bleu: a method for automatic evaluation of machine translation,” inproceedings of the 40th annual meeting on association for computational linguistics , ser. acl ’02. usa: association for computational linguistics, 2002, p. 311–318. [online]. available: https://doi.org/10.3115/1073083.1073135 c.-y. lin, “rouge: a package for automatic evaluation of summaries,” in text summarization branches out . barcelona, spain: association for computational linguistics, jul. 2004, pp. 74–81. [online]. available: https://aclanthology.org/w04-1013 c. su and c. mcmillan, “distilled gpt for source code summarization,” corr , vol. abs/2308.14731, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308. 14731 w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” corr , vol. abs/2312.15692, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.15692 o. sener and s. savarese, “active learning for convolutional neural networks: a core-set approach,” in 6th international 41 conference on learning representations, iclr 2018, vancouver, bc, canada, april 30 - may 3, 2018, conference track proceedings , 2018. [online]. available: https://openreview.net/forum?id=h1aiuk-rw h. liu, c. li, y. li, and y. j. lee, “improved baselines with visual instruction tuning,” 2023. s. zhang, p . sun, s. chen, m. xiao, w. shao, w. zhang, y. liu, k. chen, and p . luo, “gpt4roi: instruction tuning large language model on region-of-interest,” 2023. openai, “gpt-4v(ision) system card,” 2023. [online]. available: https://api.semanticscholar.org/corpusid: 263218031 b. a. plummer, l. wang, c. m. cervantes, j. c. caicedo, j. hockenmaier, and s. lazebnik, “flickr30k entities: collecting region-to-phrase correspondences for richer image-to-sentence models,” in proceedings of the ieee in- ternational conference on computer vision , 2015, pp. 2641– 2649. l. li, z. xie, m. li, s. chen, p . wang, l. chen, y. yang, b. wang, and l. kong, “silkie: preference distilla- tion for large visual language models,” arxiv preprint arxiv:2312.10665 , 2023. h. ha, p . florence, and s. song, “scaling up and distilling down: language-guided robot skill acquisition,” in con- ference on robot learning . pmlr, 2023, pp. 3766–3777. s. wu, z. liu, z. zhang, z. chen, w. deng, w. zhang, j. yang, z. yao, y. lyu, x. xin, s. gao, p . ren, z. ren, and z. chen, “fuzi.mingcha,” https://github.com/irlab- sdu/fuzi.mingcha, 2023. h. xiong, s. wang, y. zhu, z. zhao, y. liu, q. wang, and d. shen, “doctorglm: fine-tuning your chinese doctor is not a herculean task,” arxiv preprint arxiv:2304.01097 , 2023. x. zhang, c. tian, x. yang, l. chen, z. li, and l. r. pet- zold, “alpacare: instruction-tuned large language models for medical application,” arxiv preprint arxiv:2310.14558 , 2023. y. li, z. li, k. zhang, r. dan, s. jiang, and y. zhang, “chatdoctor: a medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge,” cureus , vol. 15, no. 6, 2023. t. han, l. c. adams, j. papaioannou, p . grundmann, t. oberhauser, a. l ¨oser, d. truhn, and k. k. bressem, “medalpaca - an open-source collection of medical conversational ai models and training data,” corr , vol. abs/2304.08247, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.08247 c. wu, w. lin, x. zhang, y. zhang, y. wang, and w. xie, “pmc-llama: towards building open-source language models for medicine,” arxiv preprint arxiv:2305.10415 , vol. 6, 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consultation,” corr , vol. abs/2308.14346, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.14346 z. gou, z. shao, y. gong, yelong shen, y. yang, m. huang, n. duan, and w. chen, “tora: a tool- integrated reasoning agent for mathematical problem solving,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=ep0ttjvoap e. perkowski, r. pan, t. d. nguyen, y. ting, s. kruk, t. zhang, c. o’neill, m. jablonska, z. sun, m. j. smith, h. liu, k. schawinski, k. iyer, i. ciuca, and universetbd, “astrollama-chat: scaling astrollama with conversational and diverse datasets,” corr , vol. abs/2401.01916, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2401.01916 j. gao, r. pi, j. zhang, j. ye, w. zhong, y. wang, l. hong, j. han, h. xu, z. li, and l. kong, “g-llava: solving geometric problem with multi-modal large language model,” corr , vol. abs/2312.11370, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.11370 h. zhao, s. liu, c. ma, h. xu, j. fu, z.-h. deng, l. kong, and q. liu, “gimlet: a unified graph-text model for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv a. n. rubungo, c. arnold, b. p . rand, and a. b. dieng, “llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions,” corr , vol. abs/2310.14029, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.14029 h. cao, z. liu, x. lu, y. yao, and y. li, “instructmol: multi-modal integration for building a versatile and reliable molecular assistant in drug discovery,” corr , vol. abs/2311.16208, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2311.16208 h. abdine, m. chatzianastasis, c. bouyioukos, and m. vazirgiannis, “prot2text: multimodal protein’s function generation with gnns and transform- ers,” in deep generative models for health workshop neurips 2023 , 2023. [online]. available: https://openreview.net/forum?id=ej7yngwyfj y. luo, j. zhang, s. fan, k. yang, y. wu, m. qiao, and z. nie, “biomedgpt: open multimodal generative pre-trained transformer for biomedicine,” arxiv preprint arxiv:2308.09442 , 2023. b. chen, x. cheng, p . li, y. geng, j. gong, s. li, z. bei, x. tan, b. wang, x. zeng, c. liu, a. zeng, y. dong, j. tang, and l. song, “xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein,” corr , vol. abs/2401.06199, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.06199 c. deng, t. zhang, z. he, y. xu, q. chen, y. shi, l. fu, w. zhang, x. wang, c. zhou, z. lin, and j. he, “k2: a foundation language model for geoscience knowledge understanding and utilization,” 2023. z. bi, n. zhang, y. xue, y. ou, d. ji, g. zheng, and h. chen, “oceangpt: a large language model for ocean science tasks,” corr , vol. abs/2310.02031, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.02031 z. zheng, j. zhang, t. vu, s. diao, y. h. w. tim, and s. yeung, “marinegpt: unlocking secrets of ocean to the public,” corr , vol. abs/2310.13596, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.13596 z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: 42 a scientific large language model in geoscience,” corr , vol. abs/2401.00434, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.00434 d. zhang, a. petrova, d. trautmann, and f. schilder, “un- leashing the power of large language models for legal applications,” in proceedings of the 32nd acm international conference on information and knowledge management , 2023, pp. 5257–5258. z. sun, “a short survey of viewing large language models in legal aspect,” arxiv preprint arxiv:2303.09136 , 2023. j. lai, w. gan, j. wu, z. qi, and p . s. yu, “large language models in law: a survey,” arxiv preprint arxiv:2312.03718 , 2023. s. yue, w. chen, s. wang, b. li, c. shen, s. liu, y. zhou, y. xiao, s. yun, w. lin et al. , “disc-lawllm: fine-tuning large language models for intelligent legal services,” arxiv preprint arxiv:2309.11325 , 2023. h. zhong, c. xiao, c. tu, t. zhang, z. liu, and m. sun, “jec-qa: a legal-domain question answering dataset,” in proceedings of the aaai conference on artificial intelligence , vol. 34, no. 05, 2020, pp. 9701–9708. k. singhal, t. tu, j. gottweis, r. sayres, e. wulczyn, l. hou, k. clark, s. pfohl, h. cole-lewis, d. neal, m. schaekermann, a. wang, m. amin, s. lachgar, p . a. mansfield, s. prakash, b. green, e. dominowska, b. a. y arcas, n. tomasev, y. liu, r. wong, c. semturs, s. s. mahdavi, j. k. barral, d. r. webster, g. s. corrado, y. matias, s. azizi, a. karthikesalingam, and v . natarajan, “towards expert-level medical question answering with large language models,” corr , vol. abs/2305.09617, 2023. [online]. available: https://doi. org/10.48550/arxiv.2305.09617 x. yang, j. gao, w. xue, and e. alexandersson, “pllama: an open-source large language model for plant science,” corr , vol. abs/2401.01600, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.01600 x. wang, g. h. chen, d. song, z. zhang, z. chen, q. xiao, f. jiang, j. li, x. wan, b. wang et al. , “cmb: a compre- hensive medical benchmark in chinese,” arxiv preprint arxiv:2308.08833 , 2023. w. zhu, x. wang, h. zheng, m. chen, and b. tang, “promptcblue: a chinese prompt tuning benchmark for the medical domain,” arxiv preprint arxiv:2310.14151 , 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consulta- tion,” arxiv preprint arxiv:2308.14346 , 2023. c. wu, x. zhang, y. zhang, y. wang, and w. xie, “pmc- llama: further finetuning llama on medical papers,” corr , vol. abs/2304.14454, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.14454 s. xue, f. zhou, y. xu, h. zhao, s. xie, q. dai, c. jiang, j. zhang, j. zhou, d. xiu, and h. mei, “weaverbird: empowering financial decision-making with large language model, knowledge base, and search engine,” corr , vol. abs/2308.05361, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.05361 s. wu, o. irsoy, s. lu, v . dabravolski, m. dredze, s. gehrmann, p . kambadur, d. s. rosenberg, and g. mann, “bloomberggpt: a large language modelfor finance,” corr , vol. abs/2303.17564, 2023. [online]. available: https://doi.org/10.48550/arxiv.2303.17564 d. lu, h. wu, j. liang, y. xu, q. he, y. geng, m. han, y. xin, and y. xiao, “bbt-fin: comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark,” corr , vol. abs/2302.09432, 2023. [online]. available: https://doi.org/10.48550/arxiv.2302. 09432 y. yang, y. tang, and k. y. tam, “investlm: a large language model for investment using financial domain instruction tuning,” corr , vol. abs/2309.13064, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.13064 q. xie, w. han, x. zhang, y. lai, m. peng, a. lopez- lira, and j. huang, “pixiu: a large language model, instruction data and evaluation benchmark for finance,” corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction tuning benchmark for open-source large language models in financial datasets,” corr , vol. abs/2310.04793, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310. 04793 r. taylor, m. kardas, g. cucurull, t. scialom, a. hartshorn, e. saravia, a. poulton, v . kerkez, and r. stojnic, “galactica: a large language model for science,” corr , vol. abs/2211.09085, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09085 j. yin, s. dash, f. wang, and m. shankar, “forge: pre-training open foundation models for science,” inproceedings of the international conference for high performance computing, networking, storage and analysis, sc 2023, denver, co, usa, november 12-17, 2023 , d. arnold, r. m. badia, and k. m. mohror, eds. acm, 2023, pp. 81:1–81:13. [online]. available: https: //doi.org/10.1145/3581784.3613215 z. azerbayev, h. schoelkopf, k. paster, m. d. santos, s. mcaleer, a. q. jiang, j. deng, s. biderman, and s. welleck, “llemma: an open language model for mathematics,” corr , vol. abs/2310.10631, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.10631 f. yu, a. gao, and b. wang, “outcome-supervised verifiers for planning in mathematical reasoning,” corr , vol. abs/2311.09724, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09724 t. d. nguyen, y. ting, i. ciuca, c. o’neill, z. sun, m. jablonska, s. kruk, e. perkowski, j. w. miller, j. li, j. peek, k. iyer, t. r ´ozanski, p . khetarpal, s. zaman, d. brodrick, s. j. r. m ´endez, t. bui, a. goodman, a. accomazzi, j. p . naiman, j. cranney, k. schawinski, and universetbd, “astrollama: towards specialized foundation models in astronomy,” corr , vol. abs/2309.06126, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2309.06126 j. roberts, t. l ¨uddecke, s. das, k. han, and s. albanie, “gpt4geo: how a language model sees the world’s ge- ography,” 2023. z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: a scientific large language model in geoscience,” 2023. 43 c. wang, d. engler, x. li, j. hou, d. j. wald, k. jaiswal, and s. xu, “near-real-time earthquake-induced fatality estimation using crowdsourced data and large-language models,” 2023. l. chen, s. li, j. yan, h. wang, k. gunaratna, v . yadav, z. tang, v . srinivasan, t. zhou, h. huang, and h. jin, “alpagasus: training a better alpaca with fewer data,” 2023. y. cao, y. kang, and l. sun, “instruction mining: high- quality instruction data selection for large language mod- els,” 2023. m. li, y. zhang, z. li, j. chen, l. chen, n. cheng, j. wang, t. zhou, and j. xiao, “from quantity to quality: boosting llm performance with self-guided data selection for instruction tuning,” arxiv , vol. abs/2308.12032, 2023. [online]. available: https://api.semanticscholar. org/corpusid:261076515 q. du, c. zong, and j. zhang, “mods: model-oriented data selection for instruction tuning,” 2023. y. li, b. hui, x. xia, j. yang, m. yang, l. zhang, s. si, j. liu, t. liu, f. huang, and y. li, “one shot learning as instruction data prospector for large language models,” 2023. e. frantar, s. p . singh, and d. alistarh, “optimal brain com- pression: a framework for accurate post-training quanti- zation and pruning,” 2023. t. dettmers, m. lewis, y. belkada, and l. zettlemoyer, “gpt3.int8(): 8-bit matrix multiplication for transformers at scale,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https://openreview.net/ forum?id=dxigwqboxad y. j. kim, r. henry, r. fahim, and h. h. awadalla, “finequant: unlocking efficiency with fine-grained weight-only quantization for llms,” 2023. c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre-trained language models via quantization,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4821–4836. [online]. available: https://aclanthology.org/2022.acl- long.331 z. yao, r. yazdani aminabadi, m. zhang, x. wu, c. li, and y. he, “zeroquant: efficient and affordable post-training quantization for large-scale transformers,” advances in neural information processing systems , vol. 35, pp. 27 168– 27 183, 2022. g. xiao, j. lin, m. seznec, h. wu, j. demouth, and s. han, “smoothquant: accurate and efficient post-training quan- tization for large language models,” 2023. x. ma, g. fang, and x. wang, “llm-pruner: on the struc- tural pruning of large language models,” 2023. m. zhang, h. chen, c. shen, z. yang, l. ou, x. yu, and b. zhuang, “loraprune: pruning meets low-rank parameter-efficient fine-tuning,” 2023. e. frantar and d. alistarh, “sparsegpt: massive language models can be accurately pruned in one-shot,” 2023. m. xu, y. l. xu, and d. p . mandic, “tensorgpt: efficient compression of the embedding layer in llms based on thetensor-train decomposition,” 2023. y. li, y. yu, q. zhang, c. liang, p . he, w. chen, and t. zhao, “losparse: structured compression of large lan- guage models based on low-rank and sparse approxima- tion,” 2023. z. hu, l. wang, y. lan, w. xu, e.-p . lim, l. bing, x. xu, s. poria, and r. k.-w. lee, “llm-adapters: an adapter family for parameter-efficient fine-tuning of large lan- guage models,” 2023. h. liu, d. tam, m. mohammed, j. mohta, t. huang, m. bansal, and c. raffel, “few-shot parameter- efficient fine-tuning is better and cheaper than in- context learning,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https: //openreview.net/forum?id=rbcvmg-jspd y. wang, s. agarwal, s. mukherjee, x. liu, j. gao, a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 5744–5760. [online]. available: https://aclanthology.org/2022.emnlp-main.388 e. j. hu, y. shen, p . wallis, z. allen-zhu, y. li, s. wang, l. wang, and w. chen, “lora: low-rank adaptation of large language models,” 2021. x. l. li and p . liang, “prefix-tuning: optimizing continuous prompts for generation,” in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: long papers) , c. zong, f. xia, w. li, and r. navigli, eds. online: association for computational linguistics, aug. 2021, pp. 4582–4597. [online]. available: https://aclanthology.org/2021.acl- long.353 x. liu, k. ji, y. fu, w. tam, z. du, z. yang, and j. tang, “p- tuning: prompt tuning can be comparable to fine-tuning across scales and tasks,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 2: short papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 61–68. [online]. available: https://aclanthology.org/2022.acl-short.8 t. dettmers, a. pagnoni, a. holtzman, and l. zettlemoyer, “qlora: efficient finetuning of quantized llms,” 2023. j. kim, j. h. lee, s. kim, j. park, k. m. yoo, s. j. kwon, and d. lee, “memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization,” 2023. s. malladi, t. gao, e. nichani, a. damian, j. d. lee, d. chen, and s. arora, “fine-tuning language models with just forward passes,” 2024. z. wan, x. wang, c. liu, s. alam, y. zheng, j. liu, z. qu, s. yan, y. zhu, q. zhang, m. chowdhury, and m. zhang, “efficient large language models: a survey,” 2024. y.-s. lee, m. sultan, y. el-kurdi, t. naseem, a. munawar, r. florian, s. roukos, and r. astudillo, “ensemble- instruct: instruction tuning data generation with a heterogeneous mixture of lms,” in findings of the 44 association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 12 561–12 571. [online]. available: https://aclanthology. org/2023.findings-emnlp.836 w. chen, y. zhou, n. du, y. huang, j. laudon, z. chen, and c. cui, “lifelong language pretraining with distribution- specialized experts,” in international conference on machine learning . pmlr, 2023, pp. 5383–5395. s. kotha, j. m. springer, and a. raghunathan, “under- standing catastrophic forgetting in language models via implicit inference,” arxiv preprint arxiv:2309.10105 , 2023. b. koloski, b. ˇskrlj, m. robnik- ˇsikonja, and s. pollak, “mea- suring catastrophic forgetting in cross-lingual transfer paradigms: exploring tuning strategies,” arxiv preprint arxiv:2309.06089 , 2023. t. wu, l. luo, y.-f. li, s. pan, t.-t. vu, and g. haffari, “continual learning for large language models: a sur- vey,” arxiv preprint arxiv:2402.01364 , 2024. y. luo, z. yang, f. meng, y. li, j. zhou, and y. zhang, “an empirical study of catastrophic forgetting in large language models during continual fine-tuning,” arxiv preprint arxiv:2308.08747 , 2023. j. kirkpatrick, r. pascanu, n. rabinowitz, j. veness, g. des- jardins, a. a. rusu, k. milan, j. quan, t. ramalho, a. grabska-barwinska et al. , “overcoming catastrophic forgetting in neural networks,” proceedings of the national academy of sciences , vol. 114, no. 13, pp. 3521–3526, 2017. m. rostami, s. kolouri, and p . k. pilly, “complementary learning for overcoming catastrophic forgetting using ex- perience replay,” arxiv preprint arxiv:1903.04566 , 2019. d. rolnick, a. ahuja, j. schwarz, t. lillicrap, and g. wayne, “experience replay for continual learning,” advances in neural information processing systems , vol. 32, 2019. s.-w. lee, j.-h. kim, j. jun, j.-w. ha, and b.-t. zhang, “overcoming catastrophic forgetting by incremental mo- ment matching,” advances in neural information processing systems , vol. 30, 2017. a. mallya, d. davis, and s. lazebnik, “piggyback: adapting a single network to multiple tasks by learning to mask weights,” in proceedings of the european conference on com- puter vision (eccv) , 2018, pp. 67–82. z. wang, z. zhang, c.-y. lee, h. zhang, r. sun, x. ren, g. su, v . perot, j. dy, and t. pfister, “learning to prompt for continual learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2022, pp. 139–149. z. hu, y. li, j. lyu, d. gao, and n. vasconcelos, “dense network expansion for class incremental learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2023, pp. 11 858–11 867. x. li, l. lin, s. wang, and c. qian, “unlock the power: competitive distillation for multi-modal large language models,” arxiv preprint arxiv:2311.08213 , 2023. m. zeng, w. xue, q. liu, and y. guo, “continual learning with dirichlet generative-based rehearsal,” arxiv preprint arxiv:2309.06917 , 2023. z. zhang, m. fang, l. chen, and m.-r. namazi-rad, “citb: a benchmark for continual instruction tuning,” arxiv preprint arxiv:2310.14510 , 2023. c. burns, p . izmailov, j. h. kirchner, b. baker, l. gao,l. aschenbrenner, y. chen, a. ecoffet, m. joglekar, j. leike, i. sutskever, and j. wu, “weak-to-strong generalization: eliciting strong capabilities with weak supervision,” corr , vol. abs/2312.09390, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.09390 m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to- strong data filtering for fast instruction-tuning,” corr , vol. abs/2402.00530, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.00530 j. ji, b. chen, h. lou, d. hong, b. zhang, x. pan, j. dai, and y. yang, “aligner: achieving efficient alignment through weak-to-strong correction,” corr , vol. abs/2402.02416, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.02416'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = extract_text_chunks(processed_content, section_headers)\n",
    "chunks\n",
    "# print(\"Extracted chunks:\")\n",
    "# for i, chunk in enumerate(chunks, 1):\n",
    "#     print(f\"\\nChunk {i}:\")\n",
    "#     print(chunk[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 a survey on knowledge distillation of large language models xiaohan xu1, ming li2, chongyang tao3, tao shen4, reynold cheng1, jinyang li1, can xu5, dacheng tao6, tianyi zhou2 1the university of hong kong2university of maryland3microsoft 4university of technology sydney5peking university6the university of sydney {shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk jl0725@connect.hku.hk abstract —in the era of large language models (llms), knowledge distillation (kd) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary llms, such as gpt -4, to their open-source counterparts like llama and mistral. additionally, as open-source llms flourish, kd plays a crucial role in both compressing these models, and facilitating their self- improvement by employing themselves as teachers. this paper presents a comprehensive survey of kd’s role within the realm of llm, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self- improvement. our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization – providing a comprehensive examination of kd mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. crucially, the survey navigates the intricate interplay between data augmentation (da) and kd, illustrating how da emerges as a powerful paradigm within the kd framework to bolster llms’ performance. by leveraging da to generate context- rich, skill-specific training data, kd transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. this work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. by bridging the gap between proprietary and open-source llms, this survey underscores the potential for more accessible, efficient, and powerful ai solutions. most importantly, we firmly advocate for compliance with the legal terms that regulate the use of llms, ensuring ethical and lawful application of kd of llms. an associated github repository is available at https://github.com/tebmer/awesome-knowledge-distillation-of-llms. index terms —large language models, knowledge distillation, data augmentation, skill distillation, supervised fine-tuning ✦ 1 i ntroduction in the evolving landscape of artificial intelligence (ai), proprietary1large language models (llms) such as gpt- 3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude2have emerged as groundbreaking technologies, reshaping our understand- ing of natural language processing (nlp). these models, characterized by their vast scale and complexity, have un- locked new realms of possibility, from generating human- like text to offering sophisticated problem-solving capa- bilities. the core significance of these llms lies in their emergent abilities (wei et al., 2022a,b; xu et al., 2024a), a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. their deep understanding of context, nuance, and the intrica- cies of human language enables them to excel in a wide array of applications, from creative content generation to 1. for simplicity, we use ‘proprietary’ to represent both versatile yet close-source llms like gpt-4 and open-source yet huge llms like llama-2-70b, which encapsulate rich knowledge with a large number of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond current applications, promising to revolutionize industries, augment human creativity, and redefine our interaction with technology. despite the remarkable capabilities of proprietary llms like gpt-4 and gemini, they are not without their shortcom- ings, particularly when viewed in light of the advantages offered by open-source models. a significant drawback is their limited accessibility and higher cost (openai et al., 2023). these proprietary models often come with substantial usage fees and restricted access, making them less attain- able for individuals and smaller organizations. in terms of data privacy and security (wu et al., 2023a), using these proprietary llms frequently entails sending sensitive data to external servers, which raises concerns about data pri- vacy and security. this aspect is especially critical for users handling confidential information. moreover, the general- purpose design of proprietary llms, while powerful, may not always align with the specific needs of niche applica- tions. the constraints of accessibility, cost, and adaptability thus present significant challenges in leveraging the full potential of proprietary llms. in contrast to proprietary llms, open-source modelsarxiv:2402.13116v3 [cs.cl] 8 mar 2024 2 like llama (touvron et al., 2023) and mistral (jiang et al., 2023a) bring several notable advantages. one of the primary benefits of open-source models is their accessibility and adaptability. without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader range of users, from individual re- searchers to smaller organizations. this openness fosters a more collaborative and inclusive ai research environment, encouraging innovation and diverse applications. addition- ally, the customizable nature of open-source llms allows for more tailored solutions, addressing specific needs that generic, large-scale models may not meet. however, the open-source llms also have their own set of drawbacks, primarily stemming from their relatively limited scale and resources compared to their proprietary counterparts. one of the most significant limitations is the smaller model scale, which often results in lower per- formance on real-world tasks with a bunch of instruc- tions (zheng et al., 2023a). these models, with fewer pa- rameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like gpt-4. ad- ditionally, the pre-training investment in these open-source models is typically less substantial. this reduced investment can lead to a narrower range of pre-training data, poten- tially limiting the models’ understanding and handling of diverse or specialized topics (liang et al., 2022; sun et al., 2024a). moreover, open-source models often undergo fewer fine-tuning steps due to resource constraints. fine-tuning is crucial for optimizing a model’s performance for spe- cific tasks or industries, and the lack thereof can hinder the model’s effectiveness in specialized applications. this limitation becomes particularly evident when these models are compared to the highly fine-tuned proprietary llms, which are often tailored to excel in a wide array of complex scenarios (openai et al., 2023). primarily, recognizing the disparities between propri- etary and open-source llms, kd techniques have surged as a means to bridge the performance gap between these models (gou et al., 2021; gupta and agrawal, 2022). knowl- edge distillation, in this context, involves leveraging the more advanced capabilities of leading proprietary models like gpt-4 or gemini as a guiding framework to enhance the competencies of open-source llms. this process is akin to transferring the ‘knowledge’ of a highly skilled teacher to a student, wherein the student (e.g., open-source llm) learns to mimic the performance characteristics of the teacher (e.g., proprietary llm). compared to traditional knowledge distillation algorithms (gou et al., 2021), data augmentation (da) (feng et al., 2021) has emerged as a prevalent paradigm to achieve knowledge distillation of llms, where a small seed of knowledge is used to prompt the llm to generate more data with respect to a specific skill or domain (taori et al., 2023). secondly, kd still retains its fundamental role in compressing llms, making them more efficient without significant loss in performance. (gu et al., 2024; agarwal et al., 2024). more recently, the strategy of employing open-source llms as teachers for their own self-improvement has emerged as a promising approach, enhancing their capabilities significantly (yuan et al., 2024a; chen et al., 2024a). figure 1 provides an illustration of these three key roles played by kd in the context of llms. closed-sourcellmsopen-sourcellmssmallerlmsadvancecompressself-improvement directionofkd ①②③fig. 1: kd plays three key roles in llms: 1) primarily enhancing capabilities, 2) offering traditional compression for efficiency, and 3) an emerging trend of self-improvement via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- struction following (taori et al., 2023)), improved align- ment with user intents (e.g., human values/principles (cui et al., 2023a), and thinking patterns like chain-of-thought (cot) (mukherjee et al., 2023)), and nlp task specialization (e.g., semantic understanding (ding et al., 2023a), and code generation (chaudhary, 2023)). these skills are crucial for the wide array of applications that llms are expected to perform, ranging from casual conversations to com- plex problem-solving in specialized domains. for instance, in vertical domains like healthcare (wang et al., 2023a), law (law, 2023), or science (zhang et al., 2024), where accuracy and context-specific knowledge are paramount, knowledge distillation allows open-source models to sig- nificantly improve their performance by learning from the proprietary models that have been extensively trained and fine-tuned in these areas. the benefits of knowledge distillation in the era of llms are multifaceted and transformative (gu et al., 2024). through a suite of distillation techniques, the gap between proprietary and open-source models is significantly nar- rowed (chiang et al., 2023; xu et al., 2023a) and even filled (zhao et al., 2023a). this process not only streamlines computational requirements but also enhances the environ- mental sustainability of ai operations, as open-source mod- els become more proficient with lesser computational over- head. furthermore, knowledge distillation fosters a more accessible and equitable ai landscape, where smaller enti- ties and individual researchers gain access to state-of-the-art capabilities, encouraging wider participation and diversity in ai advancements. this democratization of technology leads to more robust, versatile, and accessible ai solutions, catalyzing innovation and growth across various industries and research domains. the escalating need for a comprehensive survey on the knowledge distillation of llms stems from the rapidly evolving landscape of ai (openai et al., 2023; team et al., 2023) and the increasing complexity of these models. as ai continues to penetrate various sectors, the ability to effi- ciently and effectively distill knowledge from proprietary llms to open-source ones becomes not just a technical aspiration but a practical necessity. this need is driven by 3 studentmodelllamagptvicunaopt…… seedknowledgesteerdrivegeneratedknowledgedataset demonstrationsrawdatainput set context followingalignmentagentnlp task specializationmulti-modalityskills lawmedical&healthcarefinancesciencemisc.verticaldomains teacherllm gpt-4 claude llama gemini instructions skill domain knowledgeelicitationdistillationalgorithmtraindivergenceandsimilarity feature featureguide reinforcementlearningoutputsreward rm!(·)distill supervisedfine-tuningx,y preferencerankoptimizationy,1y,2y3y1y2y3≻≻rank…… datacuration x,yrawdatasynthesizefeedbackfeedback input outputself-knowledge outputinputinput ylabellabelingexpansion x,ydemonstrationsexpandfeature featureinput,outputextractsec.4sec.5 sec.3.1sec.3.2 fig. 2: an overview of this survey on knowledge distillation of large language models. note that ‘section’ is abbreviated as ‘sec.’ in this figure. rm s(·)denotes the student reward model. the growing demand for more accessible, cost-effective, and adaptable ai solutions that can cater to a diverse range of applications and users. a survey in this field is vital for synthesizing the current methodologies, challenges, and breakthroughs in knowledge distillation. it may serve as a beacon for researchers and practitioners alike, guiding them through the intricate process of distilling complex ai capabilities into more manageable and accessible forms. moreover, such a survey can illuminate the path forward, identifying gaps in current techniques and proposing direc- tions for future research. survey organization. the remainder of this survey is orga- nized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm ofllms. following this intro- duction, §2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of llms and highlighting the role of data augmentation (da) in this context. §3 delves into the approaches to elicit knowledge from teacher llms and core distillation algorithms, examining methods from supervised fine-tuning to more complex strategies involving divergence and similarity, reinforcement learning, and ranking opti- mization. then, §4 focuses on skill distillation, exploring how student models can be enhanced to improve context understanding, alignment with user intentions, and perfor- mance across a variety of nlp tasks. this includes discus- sions on natural language understanding (nlu), genera- tion (nlg), information retrieval, recommendation systems, and the evaluation of text generation. in §5, we ventureinto domain-specific vertical distillation, showcasing how knowledge distillation techniques are applied within spe- cialized fields such as law, healthcare, finance, and science, illustrating the practical implications and transformative impact of these approaches. the survey suggests open problems in §6, identifying current challenges and gaps in knowledge distillation research that offer opportunities for future work. finally, the conclusion and discussion in §7 synthesize the insights gained, reflecting on the implica- tions for the broader ai and nlp research community and proposing directions for future research. figure 2 shows an overview of this survey. 2 o verview 2.1 comparing traditional recipe the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021). this technique is pivotal in mitigating the challenges posed by the computational demands and resource constraints of deploying large-scale models in practical applications. historically, knowledge distillation techniques, prior to the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh et al., 2019; kim and rush, 2016). this process was largely driven by the need to deploy machine learning models in resource-constrained environments, such as mobile devices or edge computing platforms, where the computational 4knowledge distillation of llmskd algorithmsknowledgelabelingannollm (he et al., 2023a), pandalm (wang et al., 2023b), cot-distill (hsieh et al., 2023) orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), baize (xu et al., 2023b), mammoth (yue et al., 2023a), mixed distill (chenglin et al., 2023) expansionself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), code alpaca (chaudhary, 2023) self-align (sun et al., 2024b), wizardlm (xu et al., 2023a), wizardcoder (luo et al., 2023a), wizardmath (luo et al., 2023b), auggpt (dai et al., 2023a), tdg (he et al., 2023b) curationultrachat (ding et al., 2023b), phi-1 (gunasekar et al., 2023), phi-1.5 (li et al., 2023a), phi-2 (mar, 2023), magicoder (wei et al., 2023), wavecoder (yu et al., 2024) zerogen (ye et al., 2022), sungen (gao et al., 2023a), inpars (bonifacio et al., 2022) featurebabyllama (timiryasov and tastet, 2023), minillm (gu et al., 2024), gkd (agarwal et al., 2024), quantgpt (tao et al., 2022a), llm-qat (liu et al., 2023a), feedbackcai (bai et al., 2022a), wizardmath (luo et al., 2023b), ultrafeedback (cui et al., 2023a), zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), rlaif (lee et al., 2023a), lion (jiang et al., 2023b), persd (chen et al., 2023a), gkd (agarwal et al., 2024) self-knowledgeself-instruct (wang et al., 2022a), self-align (sun et al., 2024b), rlcd (yang et al., 2024a), impdistill (jung et al., 2023), lmsi (huang et al., 2023a), rest (gulcehre et al., 2023), self-rewarding (yuan et al., 2024a), baize (xu et al., 2023b), star (zelikman et al., 2022) distillationsupervised fine-tuningalpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), self-instruct (wang et al., 2022a), baize (xu et al., 2023b), star (zelikman et al., 2022), divergence and similaritydistilgpt (sanh et al., 2019), f-distill (wen et al., 2023), minillm (gu et al., 2024) ted (liang et al., 2023a), gkd (agarwal et al., 2024),babyllama(timiryasov and tastet, 2023) reinforcement learningcai (bai et al., 2022a), ultrafeedback (cui et al., 2023a), wizardmath (luo et al., 2023b), minillm (gu et al., 2024), gkd (agarwal et al., 2024), gpt3 reward (kwon et al., 2023) rank optimization zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), skill distillationcontext followinginstruction followingself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), wizardmath (luo et al., 2023b), llama-gpt4 (peng et al., 2023a), multi-turn dialoguevicuna (chiang et al., 2023), baize (xu et al., 2023b), ultrallama (ding et al., 2023b), camel (li et al., 2023b), openchat (wang et al., 2023c), zephyr (tunstall et al., 2023), rag capbility kard (kang et al., 2023a), sail (luo et al., 2023c), self-rag (asai et al., 2023), alignmentthinking patternselfee (ye et al., 2023), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), aft (wang et al., 2023d), adaptllm (cheng et al., 2023), knowpat (zhang et al., 2023a), preferencecai (bai et al., 2022a), gpt-3 reward (kwon et al., 2023), ilf (scheurer et al., 2023), almost (kim et al., 2023a), rlef (roit et al., 2023), rlaif (lee et al., 2023a), zephy (tunstall et al., 2023), ultrafeedback (cui et al., 2023a), valuecai (bai et al., 2022a), align honesty (yang et al., 2023a), sandbox (liu et al., 2023b), self-align (sun et al., 2024b), ultrafeedback (cui et al., 2023a), rlcd (yang et al., 2024a) agenttool usingtoolformer (schick et al., 2023), graph-toolformer (zhang, 2023), gorilla (patil et al., 2023), toolalpaca (tang et al., 2023a), toolllm (qin et al., 2023a), craft (yuan et al., 2023a), confucius (gao et al., 2023b), mllm-tool (wang et al., 2024), α-umi (shen et al., 2024), planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task specializationnluauggpt (dai et al., 2023a), gpt annotation (gilardi et al., 2023), (ding et al., 2023a), tdg (he et al., 2023b), sungen (gao et al., 2023a), mix distill (chenglin et al., 2023), annollm (he et al., 2023a), udg (wang et al., 2021a), zerogen (ye et al., 2022), nlginheritsumm (xu et al., 2023c), recomp (xu et al., 2024b), mario (ramnath et al., 2023), id (jung et al., 2023), gpt-3 labeling (wang et al., 2021b), biogpt (guo et al., 2023a), chatgpt nmt (yang and nicolai, 2023), information retrievalquill (srinivasan et al., 2022), promptgator (dai et al., 2023b), inpars (bonifacio et al., 2022), augtriever (meng et al., 2023), (sun et al., 2023a), rankvicuna (pradeep et al., 2023a), rankzephyr (pradeep et al., 2023b), exaranker (ferraretto et al., 2023), recommendation ndr (mysore et al., 2023), instrcutrec (zhang et al., 2023b), once (liu et al., 2023c), text generation evaluationpandalm (wang et al., 2023b), prometheus (kim et al., 2024), instructscore (xu et al., 2023d), tigerscore (jiang et al., 2023c), auto-j (li et al., 2024a), codecodealpaca (chaudhary, 2023), codellama (rozi `ere et al., 2023), magicoder (wei et al., 2023) phi-1 (gunasekar et al., 2023), persd (chen et al., 2023a), mftcoder (liu et al., 2023d), wavecoder (yu et al., 2024), code clean (jain et al., 2023), multi-modalityllava (liu et al., 2023e), svit (zhao et al., 2023b), lvis-instruct4v (wang et al., 2023e), shikra (chen et al., 2023c), lskd (park et al., 2023), detgpt (pi et al., 2023; zhao et al., 2023c), lrv (liu et al., 2023f), next-gpt (wu et al., 2023b), valley (luo et al., 2023d), iluvui (jiang et al., 2023d), stablellava (li et al., 2023c), pointllm (xu et al., 2023e), verticalization distillationlaw (huang et al., 2023b; cui et al., 2023b); medical & healthcare (zhang et al., 2023c; chen et al., 2023d); finance (zhang and yang, 2023); science (xie et al., 2023a; zhang et al., 2024) and misc. (dan et al., 2023; guo et al., 2023b) fig. 3: taxonomy of knowledge distillation of large language models. the detailed taxonomy of verticalization distillation is shown in figure 7. 5 power and memory are limited. the focus was predomi- nantly on ad-hoc neural architecture selection and training objectives tailored for single tasks. these earlier methods involved training a smaller student network to mimic the output of a larger teacher network, often through techniques like soft target training, where the student learns from the softened softmax output of the teacher. please refer to the survey (gou et al., 2021) for more details on general knowledge distillation techniques in ai and dl. in contrast, the advent of llms has revolutionized the knowledge distillation landscape. the current era of knowledge distillation in llms shifts the focus from mere architecture compression to the more nuanced process of knowledge elicitation and transfer (taori et al., 2023; chaud- hary, 2023; tunstall et al., 2023). this paradigm change is largely due to the expansive and deep-seated knowledge that llms like gpt-4 and gemini possess. and the inacces- sible parameters of llms make it hard to compress them by using pruning (han et al., 2016) or quantization (liu et al., 2023a) techniques. unlike the earlier era, where the goal was to replicate the output behavior of the teacher model or reduce the model size , the current focus in llm-based knowledge distillation is to extract and transfer the rich, nuanced understanding that these models have developed. the key to this modern approach lies in heuristic and carefully designed prompts, which are used to elicit specific knowledge (ding et al., 2023b) or capabilities (chaudhary, 2023) from the llms. these prompts are crafted to tap into the llm’s understanding and capabilities in various domains, ranging from natural language understanding (he et al., 2023a) to more complex cognitive tasks like reason- ing (hsieh et al., 2023) and problem-solving (qiao et al., 2024). the use of prompts as a means of knowledge elici- tation offers a more flexible and dynamic approach to dis- tillation. it allows for a more targeted extraction of knowl- edge, focusing on specific skills or domains of interest. this method is particularly effective in harnessing the emergent abilities of llms, where the models exhibit capabilities beyond their explicit training objectives. furthermore, this era of knowledge distillation also em- phasizes the transfer of more abstract qualities such as reasoning patterns (mitra et al., 2023), preference align- ment (cui et al., 2023a), and value alignment (sun et al., 2024b). this is in stark contrast to the earlier focus on output replication (taori et al., 2023), indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. the current techniques involve not just the replication of outputs, but also the emulation of the thought processes (mitra et al., 2023) and decision-making (asai et al., 2023) patterns of the teacher model. this involves complex strategies like chain-of-thought prompting, where the student model is trained to learn the reasoning process of the teacher, thereby enhancing its problem-solving and decision-making capabilities. 2.2 relation to data augmentation (da) in the era of llms, data augmentation (da) (wang et al., 2022a; ye et al., 2022) emerges as a critical paradigm integral to the process of knowledge distillation. unlike traditional da techniques such as paraphrasing (gangal et al., 2022) orback-translation (longpre et al., 2019), which primarily aim at expanding the training dataset in a somewhat mechanical manner. da within the context of llms focuses on the generation of novel, context-rich training data tailored to specific domains and skills. this innovation is driven by the unique capabilities of llms to generate coherent, diverse, and intricate data samples that closely mimic the nuanced understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed knowledge, kd employs da to prompt llms to produce explicit data that encapsulates specific skills or domain expertise (chaudhary, 2023; west et al., 2022). this method stands out as a potent mechanism for bridging the knowl- edge and capability gap between proprietary and open- source models. through da, llms are prompted to create targeted, high-quality datasets that are not merely larger in volume but are also rich in diversity and specificity. this approach enables the distillation process to be more effec- tive, ensuring that the distilled models not only replicate the teacher model’s output behavior but also embody its deep-seated understanding and cognitive strategies. the significance and necessity of da for achieving kd in the llm era cannot be overstated. da acts as a force multiplier, enabling the distilled models to acquire and re- fine capabilities that would otherwise require exponentially larger datasets and computational resources. it facilitates a more nuanced and effective transfer of knowledge, fo- cusing on the qualitative aspects of learning rather than quantitative expansion. this strategic use of da within kd processes underscores a pivotal shift towards a more efficient, sustainable, and accessible approach to harnessing the power of llms. it empowers open-source models with the ability to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts, thereby democratizing access to advanced ai capabilities and fostering innovation across a broader spectrum of applications and users. 2.3 survey scope building on the discussions introduced earlier, this survey aims to comprehensively explore the landscape of knowl- edge distillation within the context of llms, following a meticulously structured taxonomy as in figure 3. the survey’s scope is delineated through three primary facets: kd algorithms, skill distillation, and verticalization dis- tillation. each facet encapsulates a range of subtopics and methodologies. it’s important to note that kd algorithms provide the technical foundations for skill distillation and verticalization distillation. kd algorithms. this segment focuses on the technical foundations and methodologies of knowledge distillation. it includes an in-depth exploration of the processes involved in constructing knowledge from teacher models (e.g., pro- prietary llms) and integrating this knowledge into student models (e.g., open-source llms). under the umbrella of ‘knowledge ’, we delve into strategies such as labeling (hsieh et al., 2023), expansion (taori et al., 2023), curation (gu- nasekar et al., 2023), feature understanding (agarwal et al., 6 2024), feedback mechanisms (tunstall et al., 2023), and self- knowledge generation (wang et al., 2022a). this exploration seeks to uncover the various ways in which knowledge can be identified, expanded, and curated for effective dis- tillation. the ‘ distillation ’ subsection examines learning ap- proaches like supervised fine-tuning (sft) (wang et al., 2022a), divergence minimization (agarwal et al., 2024), rein- forcement learning techniques (cui et al., 2023a), and rank optimization strategies (tunstall et al., 2023). this analysis aims to illuminate how these algorithms facilitate the trans- fer of knowledge, ensuring that open-source models can replicate and, in some cases, surpass the capabilities of their proprietary counterparts. skill distillation. this facet examines the specific compe- tencies and capabilities enhanced through kd. it encom- passes detailed discussions on context following (taori et al., 2023; luo et al., 2023c), with subtopics like instruction following and retrieval-augmented generation (rag) capa- bility. in the realm of alignment (mitra et al., 2023; tun- stall et al., 2023), the survey investigates thinking patterns, persona/preference modeling, and value alignment. the ‘agent’ category delves into skills such as tool using and planning. nlp task specialization (dai et al., 2023a; jung et al., 2023; chaudhary, 2023) is scrutinized through lenses like natural language understanding (nlu), natural lan- guage generation (nlg), information retrieval, recommen- dation systems, text generation evaluation, and code gen- eration. finally, the survey addresses multi-modality (liu et al., 2023e; zhao et al., 2023b), exploring how kd enhances llms’ ability to interpret and integrate multiple forms of input, enriching their utility and applicability across various contexts. verticalization distillation. this section assesses the ap- plication of kd across diverse vertical domains, offering insights into how distilled llms can be tailored for spe- cialized fields such as law (law, 2023), medical & health- care (wang et al., 2023a), finance (zhang and yang, 2023), science (zhang et al., 2024), among others. this exploration not only showcases the practical implications of kd tech- niques but also highlights their transformative impact on domain-specific ai solutions. through detailed analysis and examples, this part aims to demonstrate the versatility and efficacy of kd in adapting llms to meet the nuanced demands of different industries, thus contributing to the broader ai and ml ecosystem. by navigating through these facets, this survey en- deavors to provide an extensive and nuanced analysis of knowledge distillation in the era of llms. it serves as a guide for researchers, practitioners, and enthusiasts in the field, shedding light on current methodologies, challenges, and opportunities for innovation in this rapidly evolving domain. declaration. this survey represents our earnest effort to provide a comprehensive and insightful overview of knowl- edge distillation techniques applied to llms, focusing on algorithms, skill enhancement, and domain-specific appli- cations. given the vast and rapidly evolving nature of this field, especially with the prevalent practice of elic- iting knowledge from training data across academia, weacknowledge that this manuscript may not encompass every pertinent study or development. nonetheless, it endeavors to introduce the foundational paradigms of knowledge dis- tillation, highlighting key methodologies and their impacts across a range of applications. 2.4 distillation pipeline in llm era seedknowledgeskill/domain teacherllmknowledgeelicitationstudentmodeldistillationalgorithmsteer drivegeneratedknowledgelearningobjectivetrain fig. 4: an illustration of a general pipeline to distill knowl- edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge from a sophisticated teacher model to a less complex student model. this pipeline is integral for leveraging the advanced capabilities of models like gpt-4 or gemini in more acces- sible and efficient open-source counterparts. the outline of this pipeline can be broadly categorized into four distinct stages, each playing a crucial role in the successful distilla- tion of knowledge. an illustration is shown in figure 4. the detailed pipeline could also be seen in figure 2. i. target skill or domain steering teacher llm. the first stage involves directing the teacher llm towards a specific target skill or domain. this is achieved through care- fully crafted instructions or templates that guide the llm’s focus. these instructions are designed to elicit responses that demonstrate the llm’s proficiency in a particular area, be it a specialized domain like healthcare or law, or a skill such as reasoning or language understanding. the objective here is to utilize the teacher llm’s extensive training and nuanced capabilities to generate outputs that are rich in the specific knowledge or skills desired for the student model. ii. seed knowledge as input. once the target area is defined, the next step is to feed the teacher llm with seed knowledge. this seed knowledge typically comprises a small dataset or specific data clues relevant to the elicit skill or domain knowledge from the teacher llm. it acts as a catalyst, prompting the teacher llm to generate more elaborate and detailed outputs based on this initial infor- mation. the seed knowledge is crucial as it provides a foundation upon which the teacher model can build and expand, thereby creating more comprehensive and in-depth knowledge examples. iii. generation of distillation knowledge. in response to the seed knowledge and steering instructions, the teacher llm generates knowledge examples. these examples are predominantly in the form of question-and-answer (qa) dialogues or narrative explanations, aligning with the nat- ural language processing/understanding capabilities of the 7 llm. in certain specialized cases, the outputs may also in- clude logits or hidden features, although this is less common due to the complexity and specific requirements of such data forms. the generated knowledge examples constitute the core of the distillation knowledge, encapsulating the advanced understanding and skills of the teacher llm. iv . training the student model with a specific learn- ing objective. the final stage involves the utilization of the generated knowledge examples to train the student model. this training is guided by a loss function that aligns with the learning objectives. the loss function quantifies the student model’s performance in replicating or adapting the knowledge from the teacher model. by minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. the process involves iteratively adjusting the student model’s parameters to reduce the discrepancy be- tween its outputs and those of the teacher model, ensuring the effective transfer of knowledge. in essential, the above four stages can be abstracted as two formulations. the first formulation represents the process of eliciting knowledge: d(kd) i={parse( o, s)|o∼pt(o|i⊕s),∀s∼ s} , (1) where ⊕denotes fusing two pieces of text, idenotes an instruction or a template for a task, skill, or domain to steer the llm and elicit knowledge, s∼ s denotes an example of the seed knowledge, upon which the llm can explore to generate novel knowledge, parse( o, s)stands for to parse the distillation example ( e.g., (x, y)) from the teacher llm’s output o(plus the input sin some cases), andptrepresents the teacher llm with parameters θt. given the datasets d(kd) ibuilt for distillation, we then define a learning objective as l=x ili(d(kd) i;θs), (2) wherep idenotes there could be multiple tasks or skills being distilled into one student model, li(·;·)stands for a specific learning objective, and θsparameterizes the student model. following our exploration of the distillation pipeline and the foundational concepts underlying knowledge distilla- tion in the llm era, we now turn our focus to the specific algorithms that have gained prominence in this era. 3 k nowledge distillation algorithms this section navigates through the process of knowledge distillation. according to section 2.4, it is categorized into two principal steps: ‘knowledge,’ focusing on eliciting knowledge from teacher llms (eq.1), and ‘distillation,’ centered on injecting this knowledge into student models (eq.2). we will elaborate on these two processes in the subsequent sections. 3.1 knowledge this section focuses on the approaches to elicit knowledge from teacher llms. according to the manners to acquire knowledge, we divided them into labeling ,expansion ,datacuration ,feature ,feedback , and self-knowledge . figure 5 shows an illustration of these knowledge elicitation meth- ods. 3.1.1 labeling labeling knowledge refers to using a teacher llm to label the output yfor a given input xas the seed knowledge, according to the instruction ior demonstrations c, where c= (x1, y1), . . . , (xn, yn). this method of eliciting knowl- edge from teacher llms is straightforward yet effective and has been widely applied across various tasks and appli- cations. it requires only the collection of an input dataset and feeding it into llms to obtain the desired generations. moreover, the generation of yis controllable through the predefined iandc. this process can be formulated as follows: d(lab)={x, y|x∼ x, y∼pt(y|i⊕c⊕x)}. (3) input xcould be sourced from existing nlp task datasets, which serve as typical reservoirs for distillation efforts. numerous works have sought to harness the capa- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- egorize text (gilardi et al., 2023; ding et al., 2023a; he et al., 2023a), while in natural language generation, llms assist in generating sequences for outputs (hsieh et al., 2023; jung et al., 2023; wang et al., 2021b). text generation evaluation tasks leverage llms to label evaluated results (li et al., 2024b; wang et al., 2023b), and reasoning tasks utilize llms for labeling chains of thought (cot) explanations (hsieh et al., 2023; li et al., 2022; ho et al., 2023; magister et al., 2023; fu et al., 2023; ramnath et al., 2023; li et al., 2023d; liu et al., 2023g), among others. rather than concentrating on specific tasks, many current works focus on labeling outputs based on instructions, thereby teaching student models to solve tasks in a more flexible way by following in- structions. collections of various nlp tasks, complemented by instructional templates, serve as valuable input sources forx. for instance, flan-v2 collections (longpre et al., 2023) offers extensive publicly available sets of tasks with instructions, which are labeled with responses generated by teacher llms in orca (mukherjee et al., 2023; mitra et al., 2023). the instructions from these nlp tasks are built from predefined templates, which lack diversity and may have gaps between human’s natural query. the real conversations between humans and chat models provide large-scale data with real queries and generations labeled by powerful llms, like sharegpt. additionally, xu et al. (2023b) and anand et al. (2023) label the real questions sampled from forums like quora and stack overflow. moreover, the process of labeling could be guided by instructions ior demonstrations c. a commonly used in- struction type for guiding labeling is chain-of-thought (cot) prompt (hsieh et al., 2023; fu et al., 2023; magister et al., 2023). mukherjee et al. (2023) add multiple system messages (e.g. “you must generate a detailed and long answer.” or “explain like i’m five, think step-by-step”) to elicit rich signals. yue et al. (2023a) and chenglin et al. (2023) la- bel a hybrid of knowledge of chain-of-thought (cot) and 8 𝑐𝐼labelingexpansion𝑥𝐼𝑦𝑥𝑦expandcompleteupdatedata curation 𝑚meta sources 𝐼𝑥𝑦 input set completecreatesamplegenerate 𝑚meta-information𝑐demonstrations𝑥𝐼 𝑦 filterfeedback extractfeature𝑥𝑦 distributionintermediatefeature 𝑥input𝑦output𝐼instruction𝑦! 𝑦\" 𝑦# 𝑥guidefeedback𝑦#∗ 𝑦# feedback self-knowledge studentteacher generate≻≻𝑦\" 𝑦! 𝑦# 𝑥 𝑥& correctexpand𝑐 fig. 5: an illustration of different knowledge elicitation methods from teacher llms. labeling : the teacher generates the output from the input; expansion : the teacher generates samples similar to the given demonstrations through in- context learning; data curation : the teacher synthesizes data according to meta-information, such as a topic or an entity; feature : feed the data into the teacher and extract its internal knowledge, such as logits and features; feedback : the teacher provides feedback on the student’s generations, such as preferences, corrections, expansions of challenging samples, etc; self-knowledge : the student first generates outputs, which is then filtered for high quality or evaluated by the student itself. program-of-thought (pot) rationales. xu et al. (2023b) pro- pose a self-chat technique that two teacher llms simulate the real conversational to generate multi-turn dialogues for a question from quora and stack overflow. 3.1.2 expansion while the labeling approach is simple and effective, it faces certain limitations. primarily, it is constrained by the scale and variety of the input data. in real-world applications, especially those involving user conversations, there are also concerns regarding the privacy of the data involved. to address these limitations, various expansion methods have been proposed (wang et al., 2022a; taori et al., 2023; chaud- hary, 2023; si et al., 2023; ji et al., 2023a; luo et al., 2023b,a; wu et al., 2023c; sun et al., 2024b; xu et al., 2023a; guo et al., 2023c; rozi `ere et al., 2023; west et al., 2022). these methods take the demonstrations as seed knowledge and aim to expand a large scale and various data by in-context learning. a key characteristic of these expansion methods is the utilization of the in-context learning ability of llms to gen- erate data similar to the provided demonstrations c. unlike in the labeling approach, where the input xis sampled from the existing dataset, in the expansion approach, both x andyare generated by teacher llms. this process can be formulated as follows: d(exp)={(x, y)|x∼pt(x|i⊕c), y∼pt(y|i⊕x)}.(4) in this formulation, xand yrepresent the new input- output pairs generated by the teacher llm. the input x is generated based on a set of input-output demonstrations c. the output yis then generated in response to the new input xunder the guidance of an instruction i. note thatthe demonstrations could be predefined or dynamically updated by adding the newly generated samples. expansion techniques have been widely utilized to extract extensive instruction-following knowledge from teacher llms. wang et al. (2022a) first introduces an iter- ative bootstrapping method, self-instruct, to utilize llms to generate a wide array of instructions based on sev- eral demonstrations sampled from 175 manually-written in- structions. the newly generated instructions are then added back to the initial pool, benefiting subsequent expansion iterations. subsequently, taori et al. (2023) applies this ex- pansion method to a more powerful teacher llm, text- davinci-003, to distill 52k high-quality data. to improve the diversity and coverage during expansion, wu et al. (2023c) and (sun et al., 2024b) prompt the teacher llm to generate instructions corresponding to some specific topics. xu et al. (2023a) propose an evol-instruct method to ex- pand the instructions from two dimensions: difficulty (e.g. rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to expand the distillation of coding (luo et al., 2023a) and math (luo et al., 2023b). additionally, expansion methods can significantly augment nlp task datasets with similar samples, thereby enhancing task performance. for instance, auggpt (dai et al., 2023a) leverages a teacher llm to rephrase each sentence in the training samples into multi- ple conceptually similar, but semantically varied, samples to improve classification performance. similarly, tdg (he et al., 2023b) proposes the targeted data generation (tdg) framework, which automatically identifies challenging sub- groups within data and generates new samples for these subgroups using llms through in-context learning. in summary, the expansion method leverages the in- 9 context learning strengths of llms to produce more var- ied and extensive datasets with both inputs and outputs. however, the quality and diversity of the generated data are heavily reliant on the teacher llms and the initial seed demonstrations. this dependence can lead to a dataset with inherent bias from llms (yu et al., 2023a; wei et al., 2023) and a homogeneity issue where the generations may be prone to similarity ultimately, limiting the diversity this method seeks to achieve (ding et al., 2023b). moreover, the expansion process may inadvertently amplify any biases present in the seed data. 3.1.3 data curation the pursuit of high-quality and scalable data generation in knowledge distillation from llms has led to the emergence of the data curation approach. this method arises in re- sponse to the limitations observed in both the labeling and expansion approaches. these methods often yield data of variable quality and face constraints in quantity. in labeling, the seed knowledge is sourced from task datasets, leading to potential noise and dirty data. meanwhile, in expansion, the input xis derived from seed demonstrations, which can result in homogeneous data when generated in large quantities. to overcome these challenges, the data curation method curates high-quality or large-scale data by extensive meta-information as seed knowledge (ding et al., 2023b; gunasekar et al., 2023; li et al., 2023a; mar, 2023; liu et al., 2023d; wei et al., 2023; yu et al., 2024; ye et al., 2022; gao et al., 2023a; yang and nicolai, 2023). a distinct feature of data curation is its approach to synthesize data from scratch. numerous diverse meta- information, such as topics or knowledge points, could be incorporated into this process to generate controllable x andy. thus, this process can be meticulously controlled to yield datasets that are not only large in scale but also of high quality. the formulation for data curation can be represented as: d(cur)={(x, y)|x∼pt(x|i⊕m), y∼pt(y|i⊕x)}.(5) in this formulation, mrepresents the diverse meta- information used to guide the synthesis of x, and iis the instruction guiding teacher llms to generate xory. different studies primarily vary in their source and method of leveraging meta-information. ultrachat (ding et al., 2023b) effectively demonstrates the process of curating both high-quality and diverse data by distilled knowledge. they collect extensive meta-information across three do- mains: questions about the world, creation and generation , and assistance on existing materials . for example, under questions about the world , they explore 30 meta-topics like ”technology” and ”food and drink.” the teacher llms then use this meta-information to distill a broad array of instructions and conversations, achieving a substantial scale of 1.5 million instances. ultrachat stands out with its lexical and topical diversity. the ultrallama model, fine- tuned on this data, consistently surpasses other open-source models. another notable series, phi(gunasekar et al., 2023; li et al., 2023a; mar, 2023), focuses on distilling smaller, high-quality datasets akin to ”textbooks.” phi-1 (gunasekar et al., 2023) experiments with synthesizing ”textbook qual- ity” data in the coding domain. their approach involvesdistilling clear, self-contained, instructive, and balanced con- tent from llms, guided by random topics or function names to enhance diversity. the distilled data is a synthesis of 1 billion tokens of python textbooks, complete with natural language explanations and code snippets, as well as 180 mil- lion tokens of python exercises with solutions. remarkably, thephi-1 model, despite its smaller size, outperforms nearly all open-source models on coding benchmarks like hu- maneval and mbpp while being 10 times smaller in model size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in contrast, magicoder (wei et al., 2023) and wavecoder (yu et al., 2024) get raw code collections from open-source code datasets, using this as meta-information for generating instructional data. in the context of nlu tasks, certain studies (ye et al., 2022; gao et al., 2023a; wang et al., 2021a) explore the use of labels as meta-information to synthesize corresponding samples for data augmentation. similarly, in information retrieval tasks, there are efforts to utilize docu- ments as meta-information for generating potential queries, thereby constructing large-scale retrieval pairs (bonifacio et al., 2022; meng et al., 2023). in conclusion, data curation through teacher llms has emerged as a promising technique for synthesizing datasets that are not only high-quality and diverse but also large in scale. the success of models like phi-1 in specialized domains underscores the efficacy of this method. the ability to create synthetic datasets will become a crucial technical skill and a key area of focus in ai (li et al., 2023a). 3.1.4 feature the previously discussed knowledge elicitation methods are typically applied to powerful black-box models, which are expensive and somewhat unreproducible due to calling api. in contrast, white-box distillation offers a more trans- parent and accessible approach for researchers. it involves leveraging the output distributions, intermediate features, or activations from teacher llms, which we collectively refer to as feature knowledge. white-box kd approaches have predominantly been studied for smaller encoder-based lms, typically those with fewer than 1 billion parameters (cf. gou et al. (2021) for detail). however, recent research has begun to explore white-box distillation in the context of generative llms (timiryasov and tastet, 2023; liang et al., 2023a; gu et al., 2024; agarwal et al., 2024; liu et al., 2023a; wen et al., 2023; wan et al., 2024a; zhao and zhu, 2023; qin et al., 2023b; boizard et al., 2024; zhong et al., 2024). the typical method for acquiring this feature knowledge involves teacher llms annotating the output sequence y with its internal representations. these annotations are then distilled into the student model using methods such as kullback-leibler divergence (kld). the process of eliciting feature knowledge can be formulated as follows: d(feat)={(x, y, ϕ feat(x, y;θt))|x∼ x, y∼ y} . (6) in this formulation, yis the output set, which can be generated by teacher llms, the student model, or directly sourced from the dataset. ϕfeat(·;θt)represents the opera- tion of extracting feature knowledge (such as output distri- bution) from the teacher llm. 10 the most straightforward method to elicit feature knowl- edge of teacher is to label a fixed dataset of sequences with token-level probability distributions (sanh et al., 2019; wen et al., 2023). to leverage the rich semantic and syntactic knowledge in intermediate layers of the teacher model, ted (liang et al., 2023a) designs task-aware layer-wise distillation. they align the student’s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task. gu et al. (2024) and agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, termed ‘self- generated sequences.’ the student then learns by using feedback (i.e. output distribution) from teacher on these sequences. this method is particularly beneficial when the student model lacks the capacity to mimic teacher’s distri- bution. moreover, various llm-quantization methods with distilling feature knowledge from teacher llms have been proposed (tao et al., 2022a; liu et al., 2023a; kim et al., 2023b). these methods aim to preserve the original output distribution when quantizing the llms, ensuring minimal loss of performance. additionally, feature knowledge could serve as a potent source for multi-teacher knowledge distil- lation. timiryasov and tastet (2023) leverages an ensemble of gpt-2 and llama as teacher models to extract output distributions. similarly, fusellm (wan et al., 2024a) inno- vatively combines the capabilities of various llms through a weighted fusion of their output distributions, integrating them into a singular llm. this approach has the potential to significantly enhance the student model’s capabilities, surpassing those of any individual teacher llm. in summary, feature knowledge offers a more transpar- ent alternative to black-box methods, allowing for deeper insight into and control over the distillation process. by utilizing feature knowledge from teacher llms, such as output distributions and intermediate layer features, white- box approaches enable a more nuanced transfer of informa- tion. while showing promise, especially in smaller models, its application is not suitable for black-box llms where internal parameters are inaccessible. furthermore, student models distilled from white-box llms may underperform compared to their black-box counterparts, as the black-box teacher llms (e.g. gpt-4) tend to be more powerful. 3.1.5 feedback most previous works predominantly focus on one-way knowledge transfer from the teacher to the student for imitation, without considering feedback from the teacher on the student’s generation. the feedback from the teacher typically offers guidance on student-generated outputs by providing preferences, assessments, or corrective informa- tion. for example, a common form of feedback involves teacher ranking the student’s generations and distilling this preference into the student model through reinforcement learning from ai feedback (rlaif) (bai et al., 2022a). here is a generalized formulation for eliciting feedback knowledge: d(fb)={(x, y, ϕ fb(x, y;θt))|x∼ x, y∼ps(y|x)}, (7) where ydenotes the output generated by the student model in response to x, and ϕfb(·;θt))represents providing feedback from teacher llms. this operation evaluates thestudent’s output ygiven the input x, by offering assess- ment, corrective information, or other forms of guidance. this feedback knowledge can not only be distilled into the student to also generate feedback (such as creating a student preference model) but, more importantly, enable the student to refine its responses based on the feedback. various methods have been explored to elicit this advanced knowledge (bai et al., 2022a; luo et al., 2023b; cui et al., 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). preference, as previously discussed, represents a notable form of feedback knowledge from teacher models. various knowledge of preferences could be distilled from teachers by prompting it with specific criteria. bai et al. (2022a) in- troduce rlaif for distilling harmlessness preferences from llms. this involves using an sft-trained llm to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. this dataset is distilled into a preference model (pm), which then guides the rl training of a more harmless llm policy. wizard- math (luo et al., 2023b) places emphasis on mathematical reasoning. they employ chatgpt as teacher to directly provide process supervision and evaluate the correctness of each step in the generated solutions. to scale up high- quality distilled preference data, cui et al. (2023a) develop a large-scale preference dataset for distilling better preference models, ultrafeedback. it compiles various instructions and models to produce comparative data. then, gpt-4 is used to score candidates from various aspects of preference, including instruction-following, truthfulness, honesty and helpfulness. beyond merely assessing student generations, teachers can also furnish extensive feedback on instances where students underperform. in lion (jiang et al., 2023b), teacher model pinpoints instructions that pose challenges to the student model, generating new, more difficult instructions aimed at bolstering the student’s abilities. persd (chen et al., 2023a) showcases a method where teacher offers tailored refinement feedback on incorrect code snippets gen- erated by students, guided by the specific execution errors encountered. similarly, selfee (ye et al., 2023) leverages chatgpt to generate feedback and revise the student’s answer based on the feedback. in contrast, figa (guo et al., 2024) revises the student’s response by comparing it to the ground-truth response. furthermore, teacher model’s distribution over the student’s generations can itself act as a form of feedback. minillm (gu et al., 2024) and gkd (agarwal et al., 2024) present an innovative strategy wherein the student model initially generates sequences, followed by teacher model producing an output distribution as feedback. this method leverages the teacher’s insight to directly inform and refine the student model’s learning process. 3.1.6 self-knowledge the knowledge could also be elicited from the student itself, which we refer to as self-knowledge . in this setting, the same model acts both as the teacher and the student, iteratively improving itself by distilling and refining its own previously 11 generated outputs. this knowledge uniquely circumvents the need for an external, potentially proprietary, powerful teacher model, such as gpt-series llms. furthermore, it allows the model to surpass the limitations or “ceiling” inherent in traditional teacher-student methods. eliciting self-knowledge could be formulated as: d(sk)={(x, y, ϕ sk(x, y))|x∼ s, y∼ps(y|i⊕x)},(8) where ϕsk(·)is a generalized function that represents an additional process to the self-generated outputs y, which could include but is not limited to filtering, rewarding, or any other mechanisms for enhancing or evaluating y. it could be governed by external tools or the student itself θs. recent research in this area has proposed various innovative methodologies to elicit self-knowledge, demonstrating its potential for creating more efficient and autonomous learn- ing systems. (allen-zhu and li, 2020; wang et al., 2022a; sun et al., 2024b; yang et al., 2024a; jung et al., 2023; huang et al., 2023a; gulcehre et al., 2023; yuan et al., 2024a; xu et al., 2023b; zelikman et al., 2022; chen et al., 2024a; zheng et al., 2024; li et al., 2024c; zhao et al., 2024; singh et al., 2023; chen et al., 2024c; hosseini et al., 2024) a notable example of this methodology is self- instruct (wang et al., 2022a), which utilizes gpt-3 for data augmentation through the expansion approach, gen- erating additional data samples to enhance the dataset. this enriched dataset subsequently fine-tunes the original model. other methods aim to elicit targeted knowledge from student models by modifying prompts, and leveraging these data for further refinement. in self-align (sun et al., 2024b), they find that models fine-tuned by self-instruct data tend to generate short or indirect responses. they prompt this model with verbose instruction to produce in- depth and detailed responses. then, they employ context- distillation (askell et al., 2021) to distill these responses paired with non-verbose instructions back to the model. similarly, rlcd (yang et al., 2024a) introduces the use of contrasting prompts to generate preference pairs from an unaligned llm, encompassing both superior and inferior examples. a preference model trained on these pairs then guides the enhancement of the unaligned model through reinforcement learning. several other approaches employ filtering methods to refine self-generated data. for exam- ple, impossible distillation (jung et al., 2023) targets sen- tence summarization tasks, implementing filters based on entailment, length, and diversity to screen self-generated summaries. lmsi (huang et al., 2023a) generates multiple cot reasoning paths and answers for each question, and then retains only those paths that lead to the most consistent answer. note that refined self-knowledge can be iteratively ac- quired as the student model continuously improves, further enhancing the student’s capabilities. this is gulcehre et al. (2023) introduces a reinforced self-training (rest) frame- work that cyclically alternates between grow andimprove stages to progressively obtain better self-knowledge and refine the student model. during the grow stage, the student model generates multiple output predictions. then, in the improve stage, these self-generated outputs are ranked and filtered using a scoring function. subsequently, the lan- guage model undergoes fine-tuning on this curated dataset,employing an offline rl objective. self-play (chen et al., 2024a) introduces a framework resembling iterative dpo, where the language model is fine-tuned to differentiate the self-generated responses from the human-annotated data. these self-generated responses could be seen as “negative knowledge” to promote the student to better align with the target distribution. self-rewarding (yuan et al., 2024a) explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for the self-generated responses. the entire process can then be iterated, improving instruction following and reward modeling capabilities. 3.2 distillation this section focuses on the methodologies for effectively transferring the elicited knowledge from teacher llms into student models. we explore a range of distillation tech- niques, from the strategies that enhance imitation by su- pervised fine-tuning ,divergence and similarity , to advanced methods like reinforcement learning and rank optimization , as shown in figure 3. 3.2.1 supervised fine-tuning supervised fine-tuning (sft), or called sequence-level kd (seqkd) (kim and rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-box llms. sft finetunes student model by maximizing the like- lihood of sequences generated by the teacher llms, aligning the student’s predictions with those of the teacher. this process can be mathematically formulated as minimizing the objective function: lsft=ex∼x,y∼pt(y|x)[−logps(y|x)], (9) where yis the output sequence produced by the teacher model. this simple yet highly effective technique forms the basis of numerous studies in the field. numerous re- searchers have successfully employed sft to train student models using sequences generated by teacher llms (taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; luo et al., 2023b). additionally, sft has been ex- plored in many self-distillation works (wang et al., 2022a; huang et al., 2023c; xu et al., 2023b; zelikman et al., 2022). due to the large number of kd works applying sft, we only list representative ones here. more detailed works can be found in §4. 3.2.2 divergence and similarity this section mainly concentrates on algorithms designed for distilling feature knowledge from white-box teacher llms, including distributions and hidden state features. these algorithms can be broadly categorized into two groups: those minimizing divergence in probability distributions and those aimed at enhancing the similarity of hidden states. divergence. divergence-based methods minimize diver- gence between the probability distributions of the teacher 12 divergence type d(p, q)function forward kldpp(t) logp(t) q(t) reverse kldpq(t) logq(t) p(t) js divergence1 2\\x10pp(t) log2p(t) p(t)+q(t)+pq(t) log2q(t) p(t)+q(t)\\x11 table 1: functional forms of dfor various divergence types. p: reference similarity function lf expression l2-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥2 l1-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥1 cross-entropy loss −pφt(ft(x, y)) log(φ s(fs(x, y))) maximum mean discrepancy mmd (φt(ft(x, y)),φs(fs(x, y))) table 2: summary of similarity functions in knowledge distillation. and student models, represented by a general divergence function d: ldiv= e x∼x,y∼y[d(pt(y|x), ps(y|x))], (10) the specific form of dvaries depending on the type of divergence employed. table 1 outlines the functional forms ofdfor different divergence measures. the commonly-used standard kd objectives essentially minimize the approxi- mated forward kullback-leibler divergence (kld) between the teacher and the student distribution (sanh et al., 2019; wen et al., 2023; timiryasov and tastet, 2023; liang et al., 2023a; chen et al., 2024d) , which forces psto cover all the modes of pt. however, when a student model is unable to learn all modes of a highly complex teacher, the re- sultant “mode-covering” behavior might cause the student to assign probability mass to tokens with low probability under the teacher’s distribution (cf. figure 6 blue curve). this mode-covering phenomenon can potentially lead to hallucinations and low-quality generations. alternatively, mode-seeking divergences like reverse kl prioritize tokens where the teacher assigns high probabilities (cf. figure 6 green curve). this approach can mitigate the risk of low- quality outputs, fostering more accurate generations. how- ever, it often does so at the cost of reduced diversity. gu et al. (2024) adopt reverse kl divergence to prevent students from overestimating low-probability regions of the teacher’s distribution, employing policy gradient methods for opti- mization. both agarwal et al. (2024) and sason and verd ´u (2016) assess the efficacy of different divergence functions in llm distillation, finding the optimal divergence to be task-dependent. for instance, forward kl divergence is more suitable for tasks like machine translation, where the output has fewer modes or variations, while reverse kl divergence is preferable for tasks like dialogue generation and instruction tuning, which involve multiple modes and a wider range of potential responses. thus, the nature of the task significantly influences the selection of the divergence function for optimal performance. similarity. similarity-based methods in knowledge distilla- tion aim to align the hidden states or features of the student pargminqkl(p||q)argminqkl(q||p)fig. 6: comparison of forward and reverse kl diver- gences in approximating a target distribution . forward kl divergence approach tends to cover all modes of the target distribution but is less precise, i.e. “mode-covering” behavior. reverse kl divergence method focuses predom- inantly on the most prominent mode, thereby exhibiting a “mode-seeking” behavior. model with those of the teacher. these methods use various similarity metrics to measure and optimize the congruence of internal representations between the two models. the objective is to ensure that the student model not only produces similar outputs to the teacher but also processes information in a comparable manner. the formulation for a similarity-based objective might look like this: lsim= e x∼x,y∼y[lf(φt(ft(x, y)),φs(fs(x, y)))],(11) where ft(x, y)andfs(x, y)are the feature maps of the teacher and student models, respectively. the transforma- tion functions φtandφsare applied to these feature maps to ensure they are in the same shape, facilitating direct comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based methods in the kd of llms. among them, liang et al. (2023a) propose task-aware layer-wise distillation (ted), a method that utilizes task-aware filters. these filters are designed to selectively capture the most pertinent informa- tion for a specific task from the teacher model. the key objective is to minimize the discrepancy between the filtered representations in both teacher and student models. while similarity-based approaches are common in encoder-based lms (sun et al., 2019, 2020; jiao et al., 2020; hou et al., 2020; zuo et al., 2022; liang et al., 2021), their application in llm knowledge distillation is not as widespread. however, considering their effectiveness, we anticipate an increase in research exploring these methods for llm distillation in the near future. 3.2.3 reinforcement learning this section explores advanced methods of distilling knowl- edge into student models using reinforcement learning (rl). this approach is especially relevant for leveraging the feed- back from teacher to train student models (bai et al., 2022a; cui et al., 2023a; luo et al., 2023b; agarwal et al., 2024; chen et al., 2024b; ma et al., 2023a; pang et al., 2023; du et al., 2023a). the rl-based distillation process typically involves two main stages: 13 distilled reward model training. the first stage involves training a reward model rϕusing the feedback data d(fd) generated by teacher llms. preference data, as one of the typical feedback, is employed to train the student reward model (bai et al., 2022a; cui et al., 2023a; lee et al., 2023a; kim et al., 2023a). they usually consist of input-output pairs (x, yw, yl). here, ywandylrepresent “winning” and “losing” outputs relative to the teacher’s preferences. the loss function for the reward model is defined as: lrm(rϕ,d(fd)) =− e (x,yw,yl)∼d(fd)[logσ(rϕ(x, yw)−rϕ(x, yl))] (12) this formulation guides the reward model to correctly distinguish between more and less preferable outputs based on the teacher’s criteria. instead of learning the instance- level rewards, rlmec (chen et al., 2024b) adopts a dif- ferent approach by training a generative reward model. it is trained on an erroneous solution rewriting data distilled from a teacher llm. this distilled reward model can pro- duce token-level rewards for rl training. reinforcement learning optimization. in the second stage, the student model, represented by a policy πθ, is optimized to maximize the expected reward as per the trained reward model. simultaneously, it minimizes the divergence from a reference policy πref, typically the initial policy of the student model trained by sft, controlled by a factor β. the rl objective is given by: max πθe x∼x,y∼πθ(y|x)[rϕ(x, y)]−βdkl[πθ(y|x)∥πref(y|x)] (13) this rl framework not only ensures that the student model learns the explicit content from the teacher but also effec- tively adopts the teacher’s preference patterns. the use of rl, particularly with the ppo (schulman et al., 2017) algo- rithm, offers a robust mechanism for aligning the student model’s outputs with the teacher. alternatively, the teacher llm can also serve as the reward model to directly assign rewards during rl, circumventing the need for training a reward model (lee et al., 2023a; kwon et al., 2023). while this approach may exhibit superior performance, it comes at a higher computational cost compared to employing a smaller distilled reward model. 3.2.4 ranking optimization ranking optimization presents a stable and computationally efficient alternative to rl for injecting preference feedback into language models (rafailov et al., 2023; song et al., 2023a; yuan et al., 2023b). this method, diverging from traditional rl approaches, directly incorporates ranking information into language models from a fixed preference dataset during fine-tuning. intuitively, it directly updates policy to increase the relative likelihood of preferred over less favored responses. this direct optimization of prefer- ences, without the need for sampling outputs, makes the process more stable and efficient. recently, some works have been proposed to explore using ranking optimization todistill teacher’s preferences into student models (tunstall et al., 2023; hong et al., 2023; yuan et al., 2024a). zephyr (tunstall et al., 2023) utilizes direct preference optimization (dpo) (rafailov et al., 2023) to distill the preference alignment in teacher llms. dpo streamlines the objective of reinforcement learning (as in eq. 13), which involves reward maximization with a kl-divergence constraint, into a single-stage policy training. specifically, dpo’s training goal is to maximize the following expecta- tion: e (x,yw,yl)∼d(fd)\\x14 logσ\\x12 βlogπθ(yw|x) πref(yw|x)−βlogπθ(yl|x) πref(yl|x)\\x13\\x15 , (14) where ywis preferred over ylaccording to the teacher llm. hong et al. (2023) (hong et al., 2023) adopt two ranking-based optimization objectives, rank responses to align human feedback (rrhf) (yuan et al., 2023b) and preference ranking optimization (pro) (song et al., 2023a), for preference distillation. rrhf (yuan et al., 2023b) focuses on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj are their corresponding conditional log probabilities under the policy πθ. this approach emphasizes direct comparison and ranking of responses based on the teacher’s preferences. pro (song et al., 2023a) expands the concept of pairwise comparison to handle preference rankings of any length. for a given instruction xand a sequence of responses ordered by teacher preference as y1≻y2≻...≻yn, the rpo training objective is: lpro=−n−1x k=1logexp (pk)pn i=kexp (pi), (16) where pkrepresents the conditional log probabilities for ykunder the student policy πθ. by iteratively contrasting the likelihood of generating responses, pro optimizes the student lm to prioritize the most preferred response while progressively ranking the rest in the order of diminishing preference. 4 s kill distillation building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms. our exploration will encompass a diverse range of skills exhibited by llms, including context following ,alignment ,agent ,nlp task specializa- tion and multi-modality .context following focuses on the student’s ability to comprehend and respond effectively to input information. alignment delves into the student’s capability to align its output with the teacher’s responses. moving forward, agent underscores the autonomous nature of language models. nlp task specialization highlights the llm’s versatility in specializing across various natural language processing tasks, demonstrating its adaptability. 14 methods skill seed knowledge teacher llm student model knowledge elicitation objective context following self-instruct (wang et al., 2022a) if 175 human-curated tasks gpt3 gpt3 expansion + self-knowledge sft alpaca (taori et al., 2023) if 175 human-curated tasks gpt3 llama expansion + self-knowledge sft lamini-lm (wu et al., 2023c) if3.5k wikipedia categories + mixed datasetchatgpt various models expansion sft wizardlm (xu et al., 2023a) if alpaca data chatgpt llama expansion sft lion (jiang et al., 2023b) if alpaca cata chatgpt llama labeling + expansion + feedback - babyllama (timiryasov and tastet, 2023) if 10m-word babylm dataset gpt-2 + small llama 58m-parameter llama feature d&s minillm (gu et al., 2024) if dolly dataset gpt2 + opt + llama gpt2 + opt + llama feature d&s self-align (sun et al., 2024b) if human-written principles llama llama expansion + self-knowledge sft self-rewarding (yuan et al., 2024a) if human-written samples llama llama self-knowledge sft + rl star (zelikman et al., 2022) if arithmetic + commonsenseqa + gsm8k gpt-j gpt-j self-knowledge sft llama-gpt4 (peng et al., 2023a) if alpaca dataset gpt4 llama labeling sft reflection-tuning (li et al., 2023e) if alpaca/wizardlm dataset chatgpt llama labeling sft selective reflection-tuning (li et al., 2024d) if alpaca/wizardlm dataset chatgpt llama labeling sft vicuna (chiang et al., 2023) if/md human conversation chatgpt + gpt4 llama labeling sft koala (geng et al., 2023) if/md human conversation chatgpt llama labeling sft baize (xu et al., 2023b) if/md quora + stack overflow chatgpt llama expansion + self-knowledge sft ultrachat (ding et al., 2023b) if/md wikidata + text material + c4 chatgpt llama curation sft orca (mukherjee et al., 2023) if/tp flan-v2 chatgpt + gpt4 llama labeling sft orca2 (mitra et al., 2023) if/tp flan-v2 + few-shot/math/synthetic gpt4 llama labeling sft selfee (ye et al., 2023) if/tp human conv, flan/code/math collection chatgpt llama labeling sft cot-distill (hsieh et al., 2023) if/tp e-snli + anli + cqa + svamp palm t5 labeling sft knowpat (zhang et al., 2023a) if/tp cpkg + qa data chatgpt + chatglm + vicuna-7b llama labeling sft debatune (li et al., 2024e) if/tp controversial topics chatgpt llama labeling sft phi-1 (gunasekar et al., 2023) if/code - gpt3.5 phi-1 curation sft phi-1.5 (li et al., 2023a) if/code 20k topics from web gpt3.5 phi-1 curation + labeling sft sail (luo et al., 2023c) if/rag alpaca data + web content gpt4 llama label sft kard (kang et al., 2023b) if/rag medqausmle chatgpt t5 + opt label sft + d&s self-rag (asai et al., 2023) if/rag open-instruct gpt4 llama labeling sft alignment openchat (wang et al., 2023c) if/preference human conversation chatgpt + gpt4 llama labeling sft + rl zephyr (tunstall et al., 2023) if/preference mixed datasets gpt4 mistral labeling + feedback sft + ro almost (kim et al., 2023a) if/preference human-written prompts llama llama expansion + labeling sft + rl rlcd (yang et al., 2024a) if/preference human-written prompts llama llama labeling sft + rl rlaif (lee et al., 2023a) if/preference human-written prompts palm 2 palm 2 labeling + feedback rl gpt3 reward (kwon et al., 2023) preference human-written prompts gpt3 gpt3 labeling rl ilf (scheurer et al., 2023) preference task-specific datasets gpt3 + feedme gpt3 labeling rl ultrafeedback (cui et al., 2023a) preference mixed datasets gpt4 llama labeling rl constitutional ai (bai et al., 2022a) preference/value human-written prompts self-defined student model self-defined model labeling + expansion + feedback sft + rl sandbox (liu et al., 2023b) value simulationtext-davinci-002/-003 + gpt4 + chatgptllama data curation sft + rl agent toolformer (schick et al., 2023) tool ccnet gpt-j gpt-j labeling sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft gpt4tools (yang et al., 2023b) tool image content chatgpt llama curation + expansion sft toolalpaca (tang et al., 2023a) tool public-apis repository chatgpt llama curation sft toolllm (qin et al., 2023a) tool real-world apis chatgpt llama curation sft mllm-tool (wang et al., 2024) tool huggingface model cards gpt4 llama curation sft fireact (chen et al., 2023b) planning mixed qa dataset gpt4 llama labeling sft agenttuning (zeng et al., 2023a) planning 6 agent tasks gpt4 + chatgpt llama labeling + expansion sft lumos (yin et al., 2023a) planning mixed interactive tasks gpt4 llama labeling sft autoact (qiao et al., 2024) planning mixed qa tasks llama llama labeling sft nlp task specialization auggpt (dai et al., 2023a) nlu amazon/symptoms/pubmed20k dataset chatgpt bert label sft tdg (he et al., 2023b) nlu sst + qqp + mnli gpt3 bert expansion sft sungen (gao et al., 2023a) nlu text classification tasks gpt2 distilbert curation sft udg (wang et al., 2021a) nlu nlu tasks gpt3 bert expansion sft inheritsumm (xu et al., 2023c) nlg pile + arxiv + cnn/dm + wikihow gpt3.5 zcode++ label sft dimsum+ (jung et al., 2023) nlg none gpt2 + ctrl + biogpt t5 curation + self-knowledge sft genie (yehudai et al., 2024) nlg eli5 + asqa + nq + cnn/dm falcon + llama flan + llama label sft gkd (agarwal et al., 2024) nlg/nlu/if xsum+wmt14 en-de+gsm8k+flan2021 t5-xl t5 feature + feedback d&s + rl quill (srinivasan et al., 2022) ir ir datasets t5 4-layer transformer internal knowledge d&s rankvicuna (pradeep et al., 2023a) ir ir datasets chatgpt llama labeling sft rankzephyr (pradeep et al., 2023b) ir ir datasets chatgpt + gpt4 mistral labeling sft ndr (mysore et al., 2023) recommendation recommendation datasets gpt3 mpnet-110m labeling sft instrcutrec (zhang et al., 2023b) recommendation 39 instruction templates chatgpt flan-t5 expansion + self-knowledge sft once (liu et al., 2023c) recommendation recommendation dataset chatgpt llama labeling sft pandalm (wang et al., 2023b) evaluation alpaca data chatgpt llama labeling sft prometheus (kim et al., 2024) evaluation 50 seed rubrics gpt4 llama labeling sft instructscore (xu et al., 2023d) evaluation mixed dataset gpt4 llama labeling sft wizardmath (luo et al., 2023b) math gsm8k + math chatgpt llama expansion + feedback sft + rl mammoth (yue et al., 2023a) math/tp mixed math dataset gpt4 llama labeling sft mixed distill (chenglin et al., 2023) math/tp svamp + gsm8k + asdiv + strategyqa chatgpt llama labeling sft wizardcoder (luo et al., 2023a) code code alpaca data chatgpt starcoder expansion sft magicoder (wei et al., 2023) code existing source codes chatgpt llama curation sft wavecoder (yu et al., 2024) code existing source codes gpt4 llama curation sft code alpaca (chaudhary, 2023) code code instructions chatgpt llama expansion + self-knowledge sft code llama (rozi `ere et al., 2023) code human-written instructions llama llama expansion + self-knowledge sft code clean (jain et al., 2023) code code datasets chatgpt llama labeling sft multi-modality llava (liu et al., 2023e) vision-language coco gpt4 llama labeling sft svit (zhao et al., 2023b) vision-language visual genome + coco gpt4 llama labeling sft lvis-instruct4v (wang et al., 2023e) vision-language lvis gpt4v llama labeling sft llavar (zhang et al., 2023d) vision-language laion gpt4 llama labeling sft macaw-llm (lyu et al., 2023) multiple modalities image/video with caption chatgpt llama labeling sft mimic-it (li et al., 2023f) multiple modalities image/video dataset chatgpt llama labeling sft chatbridge (zhao et al., 2023d) multiple modalities task-specific/multimodal-chat data gpt4 + chatgpt llama labeling sft table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, rag: retrieval-augmented generation, nlu: natural language understanding, nlg: natural language generation, ir: information retrieval, sft: supervised fine-tuning, d&s: divergence and similarity, rl: reinforcement learning, ro: ranking optimization. finally, multi-modality encompasses the knowledge trans- fer from teacher llms to multi-modal models. table 3 summarizes the representative works, encompassing details such as the skills involved, seed knowledge, teacher llm, student model, knowledge elicitation method, and training objectives.4.1 context following this part concentrates on the distillation of context follow- ing skills from llms. this process involves transferring the ability of llms to handle a variety of complex contexts — such as few-shot demonstrations, intricate instructions, dia- logue history, and retrieval-augmented information — into smaller models. many research efforts in this domain aim to imbue smaller models with these sophisticated, context- 15 following capabilities. our discussion here will dissect this facet of skill distillation, categorizing it based on different types of context and elaborating on how each is distilled and incorporated into smaller, efficient models. 4.1.1 instruction following instruction-following capacity enables llms to understand and follow user-given instructions. this ability significantly enhances human-ai interaction, allowing for seamless un- derstanding and execution of tasks as directed by users. a primary method for acquiring this skill involves construct- ing instruction-like prompt-response pairs and employing supervised fine tuning (sft) for model training. data for this purpose can be manually curated by human experts or transformed from existing nlp tasks into instructional formats with templates, such as prefacing machine transla- tion data with ”translate this sentence to spanish:” . however, these approaches have limitations. manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input. llms like gpt-4 offer an efficient alternative for creating diverse and controlled sft data by their capabil- ities of in-context learning and instruction following. most relevant works use openai’s gpt series models to generate prompt-response data pairs and then train the student llms by supervised fine-tuning (wang et al., 2022a; taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; mukherjee et al., 2023; mitra et al., 2023; luo et al., 2023b; peng et al., 2023a). basic instructions. self-instruct (wang et al., 2022a) lever- ages the in-context learning capability of gpt-3 to expand a seed pool of 175 tasks to 52k task-agnostic instructions, ensuring a broad spectrum of general instructions. addi- tionally, a filtering and post-processing stage is introduced to eliminate redundant or similar instructions. notably, through training with this enriched dataset, gpt-3 acquires the ability to follow instructions, enabling it to perform comparably to instructgpt in zero-shot instruction tasks and when provided with expert-written instructions for novel tasks. based on the self-instruct method, taori et al. (2023) train an alpaca model using the llama 7b model on 52k instruction-following demonstrations, generated in a similar style as self-instruct but utilizing the more robust text-davinci-003 model. to enhance the diversity of instruc- tional data, wu et al. (2023c) introduce a technique known astopic-guided instruction generation . this method involves gathering 3.5k common topics from wikipedia to serve as guidance during the generation process. complex instructions. some works promote students to solve more complex instructions (xu et al., 2023a; luo et al., 2023b,a; guo et al., 2023c). according to xu et al. (2023a), in- struction datasets derived from human-written seeds often exhibit low to moderate complexity. to enhance the com- plex instruction-following capabilities of smaller models, wizardlm (xu et al., 2023a) introduces evol-instruct . this method gradually transforms instructions into more com- plex forms through a multi-step evolution process, focusing on both increasing difficulty levels and expanding the di- versity of topics. they conducted four rounds of evolution using the openai chatgpt api, resulting in a dataset of250k complex instructions. subsequently, they trained the llama 7b model, referred to as wizardlm, on this dataset. in the high-difficulty section of test instructions, wizardlm even outperformed chatgpt, achieving a win rate 7.9% higher than chatgpt. zhao et al. (2023e) further conduct preliminary studies revealing the effectiveness of increasing instruction complexity. instruction fusion (guo et al., 2023c) further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to distill specific skills such as coding (luo et al., 2023a) and mathematics (luo et al., 2023b). human instructions. in contrast to works that rely on gener- ating instructions from chatgpt, which may lack diversity and have gaps with real human instructions, vicuna (chiang et al., 2023) and koala (geng et al., 2023) showcase impres- sive performance by using human conversations and natu- ral instructions from community-contributed conversations. these conversations, found in platforms like sharegpt, pro- vide a forum for users to share their interactions with chat- gpt. it’s important to note, however, that models trained on such natural conversations might mimic the style but may not fully capture the reasoning process of the original teacher (gudibande et al., 2023; mukherjee et al., 2023). system instructions. to encourage student models to learn the reasoning process, orca and orca 2 (mukherjee et al., 2023; mitra et al., 2023) enhance the prompt, response data pairs by introducing a system message (e.g., ”explain like i’m five, think step-by-step”) to encourage student mod- els to grasp the reasoning process. this system message prompts gpt-4 to provide explanation traces that eluci- date the teacher’s reasoning process. orca 2 (mitra et al., 2023) further trains the student model to identify the most effective solution strategy for each task, guided by orca’s performance. this approach significantly improves the abil- ity of smaller models to follow instructions that involve reasoning. high-quality instructions. as demonstrated in zhou et al. (2023a) and (li et al., 2024f), the data quality is crucial for instruction following training. ultrachat (ding et al., 2023b) distills large-scale data with high-quality and di- verse instructions from teacher llms by various meta- information. the ultrallama model, fine-tuned on this data, consistently surpasses other open-source models. the phi series models (gunasekar et al., 2023; li et al., 2023a; mar, 2023) prioritize data quality and employ synthetic methods to generate data of “textbook quality” to enhance the learning experience for smaller models. notably, phi exhibits the ability to follow instructions effectively even without specific instruction fine-tuning. what’s particularly remarkable is that phi-2, with just 2.7 billion parameters, outperforms mistral and llama-2 models with 7b and 13b parameters across various benchmark evaluations. improved instructions. another line of work focuses on improving the quality of existing instruction data, including both the improvement of instruction and corresponding response. selfee (ye et al., 2023) utilizes the chatgpt to iter- atively improve the quality of responses. expertllama (xu et al., 2023f) improves the quality of responses by augment- 16 ing vanilla instructions with specialized expert identity descriptions. reflection-tuning (li et al., 2023e) improves both the instruction and response sequentially by reflecting on specific criteria. deita (liu et al., 2023h) proposes to enhance and score instructions in three directions includ- ing complexity, quality, and diversity to get high-quality distillation data. muffin (lou et al., 2023) proposes to scale the instruction according to the input by diversifying these tasks with various input facets. selective reflection- tuning (li et al., 2024d) first involves the student model in the data improvement pipeline with a novel student- selection module, in which the student model is able to decide the data learn from. in summary, distilling instruction data from teachers presents a promising avenue for training cheap and re- producible instruction-following language models. cur- rent small models have made strides in enhancing var- ious aspects of instruction-following ability, like diver- sity, complexity and explanation. however, student mod- els trained on instruction data expanded by chatgpt of- ten mimic chatgpt’s style without replicating its factual accuracy (gudibande et al., 2023). achieving a more ca- pable instruction-following capability requires a stronger teacher llm (gudibande et al., 2023) and access to di- verse, high-quality instruction data, such as the one used in orca (mukherjee et al., 2023; mitra et al., 2023), which incorporates extensive task instructions from the flan 2022 collection (longpre et al., 2023). 4.1.2 multi-turn dialogue while instruction following focuses on single-instance com- mand execution, multi-turn dialogue extends this to com- prehend and maintain context through ongoing interactions. this skill is vital for models to engage meaningfully in human-like conversations and respond coherently over suc- cessive dialogue turns. some works have been dedicated to train to small chat models by distilling multi-turn knowl- edge from teacher llms (chiang et al., 2023; xu et al., 2023b; ding et al., 2023b; li et al., 2023b; wang et al., 2023c; tunstall et al., 2023). sharegpt serves as a platform for users to share their conversations with chatgpt, offering a vast repository of multi-turn conversations readily available. some small chat models are trained using this data to acquire the capability for engaging in multi-turn dialogues (chiang et al., 2023; ye et al., 2023; wang et al., 2023c). for example, vicuna (chiang et al., 2023) is a chat model exclusively trained on sharegpt data. despite its sole training source being sharegpt, vi- cuna achieves a high mt-bench (zheng et al., 2023a) score assigned by gpt-43. in the study conducted by wang et al. (2023c), gpt-3.5 and gpt-4 are employed to generate mixed responses using sharegpt data. they assign higher rewards to responses generated by gpt-4, aiming to incentivize student models to produce high-quality responses. addi- tionally, ye et al. (2023) enhance the quality of multi-turn data from sharegpt by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. 3. mt-bench: a multi-turn question set, where the generations of models are evaluated by llm, like gpt-4.to enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversa- tional datasets through self-chat and using them to train smaller models (xu et al., 2023b; ding et al., 2023b; tunstall et al., 2023). for instance, xu et al. (2023b) initiate their work by using questions sourced from quora and stack overflow as seeds, resulting in the collection of 111.5k dialogues through self-chat. subsequently, they employ parameter- efficient tuning to train a chat model named baize. ding et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions and dialogues from chatgpt. notably, ultrachat encom- passes a wide range of topics and instructions. building upon the ultrachat dataset, they fine-tune a llama model, resulting in the creation of a powerful chat model known as ultrallama. ultrallama consistently outperforms other open-source chat models, including vicuna and baize. fur- thermore, ultrachat is employed in conjunction with an ai preference-aligned chat model named zephyr (tunstall et al., 2023). zephyr enhances intent alignment through the application of distilled direct preference optimization (ddpo). 4.1.3 rag capbility llms are known to lack the ability to utilize up-to-date knowledge, and often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge. retrieval-augmented generation (rag) is a promising technique to decrease this issue. handling the augmented context of retrieved information is also a non- trivial skill of llms. several approaches to distill rag capabilities have been proposed (kang et al., 2023a; luo et al., 2023c; asai et al., 2023). sail (luo et al., 2023c) starts by retrieving search results for each training case using search apis, creating search- augmented instructions that include both the instruction and grounding information. to encourage the language model to prioritize informative retrieval results, they input each retrieved passage along with the ground truth response into the entailment model to label each retrieval result for relevance. subsequently, the search-augmented instructions and relevance labels are fed into teacher llms (like gpt- 4) for generating responses. following fine-tuning on this training set, the student model becomes proficient at de- noising search results and generating accurate responses. kard (kang et al., 2023b) distills rationales rfrom the teacher llm in response to questions x. these rationales are then utilized to train two models: a student lm and a reranker. for training the student lm, the rationales serve as a means to retrieve relevant knowledge d, and the student lm is subsequently fine-tuned using the rationales along- side questions and knowledge. however, during inference, only questions are available. to address this, the reranker is trained to mimic how the retriever scores passages with the rationale by minimizing the kl divergence between retriever (d|r)andreranker (d|x). however, the integra- tion of a fixed number of passages in language models, without considering their necessity or relevance, can reduce versatility and lead to the generation of unhelpful responses. to equip student lms with adaptive rag capabilities, self- 17 rag (asai et al., 2023) distills this adaptive ability from teacher llms into a small critic model. this critic model determines whether retrieval is necessary and evaluates the quality of the retrieved results by generating ‘reflection to- kens.’ for instance, self-rag initiates the retrieval operation when generating the reflection token retrieve . to distill this critic data, gpt-4 is prompted to assess the need for retrieval using few-shot demonstrations i, the task input x, and output yto predict a reflection token ras follows: p(r|i, x, y ). 4.2 alignment 4.2.1 thinking pattern most existing methods mainly focus on directly aligning the direct responses of the student models to the responses of teacher models (taori et al., 2023). though effective, these models might suffer the problems that they tend to learn to imitate the response style of the teacher models, but not the reasoning process (mukherjee et al., 2023). thus in order to better distill from the teacher models, methods are proposed that not only imitate the pure responses but some novel thinking patterns (ye et al., 2023; mukherjee et al., 2023; mitra et al., 2023; wang et al., 2023d; cheng et al., 2023; zhang et al., 2023a). motivated by the effectiveness of llms in generat- ing their own feedback without relying on external mod- els (schick et al., 2022; madaan et al., 2023; saunders et al., 2022), selfee (ye et al., 2023) proposes to train a model that has been fine-tuned to continuously revise its own answer until it provides a high-quality response in a single inference. during training, it utilizes both the final response and feedback chain as the fitting target. this pat- tern, response with the revision process, shows a promising performance gain. following selfee, reflection-tuning (li et al., 2023e, 2024d) also utilizes the reflection process as the learning pattern. noticing the lack of reasoning imitation of the previous methods, orca (mukherjee et al., 2023) first proposes explanation tuning, which aims to learn the reasoning steps, including explanation traces, step-by-step thought processes, and other complex instructions, from the teacher model, rather than just the vanilla styles. extensive experiments verify the effectiveness of distilling with this thinking pattern. the following orca2 (mitra et al., 2023) further presents to equip the student models with the ability to utilize different solution strategies for different tasks, mo- tivated by the capability discrepancies between the smaller and larger models. by employing this training pattern, the student models are able to gain a better reasoning ability. be- sides learning with the corresponding revision or reflection process, another thinking pattern that recently appeared is generating both responses and preferences. zhang et al. (2023a) propose to learn both the knowledge and corre- sponding preference for domain-specific qa with llms. recently, debatune (li et al., 2024e) proposes to improve the controllability of llms in generating statements on controversial topics. by engaging two agents in a structured multi-round debate on controversial topics, salient and in- depth statements can be obtained and further distilled into the student models.4.2.2 preference the previously mentioned methods primarily focus on the basic capability of student models to produce outcomes that are strictly accurate but may not align with human preferences, reaching alignment at this level enables these models to aid in various tasks without meeting higher-level demands. early methods mainly utilize human feedback for the alignment of human preferences (ziegler et al., 2019; stiennon et al., 2020; wu et al., 2021; ouyang et al., 2022; bai et al., 2022b; k ¨opf et al., 2023; yuan et al., 2023b). however, obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon et al., 2023; scheurer et al., 2023; kim et al., 2023a; roit et al., 2023; yang et al., 2024a; lee et al., 2023a; tunstall et al., 2023; cui et al., 2023a; wang et al., 2023f). the concept of rlaif, introduced by bai et al. (2022a), involves the integration of preferences labeled by llms with those labeled by humans. this approach is designed to simultaneously optimize two key objectives: ensuring the helpfulness of the output and minimizing any potential harm, making the responses of llms more aligned with human preferences. kwon et al. (2023) develop a proxy reward function using llms like gpt-3, which is created by first providing the llm with a description of the behaviors desired by the user, along with a small number of examples. the llm then produces rewards by evaluating how closely the outputs of a model align with the provided descrip- tions, essentially measuring their relevance to the estab- lished ground truth. scheurer et al. (2023) propose imitation learning from language feedback, in which a language model is utilized to improve various outputs generated by a model. this refinement is based on a reference provided by a human. following this process, the most effectively refined output is chosen to be used in further supervised fine-tuning. as outlined by kim et al. (2023a), almost in- volves condensing human preferences into a set of heuristic guidelines. an example of such a rule is the idea that larger llms that utilize more comprehensive and higher-quality prompts are likely to yield superior responses. based on these established guidelines, comparison data is generated using responses from llms of different sizes and with varying prompts. this data is then used to train a reward model. yang et al. (2024a) propose reinforcement learning from contrast distillation, which aims to align language models without relying on human feedback. this approach involves training a preference model using simulated pairs of preferences, including both high-quality and low-quality examples which are generated through contrasting prompts, positive and negative. lee et al. (2023a) further highlight the effectiveness of rlaif. this work proposes that rlaif not only matches but in some cases surpasses rlhf, and interestingly, rlaif can also enhance the performance of supervised fine-tuning. another notable discovery is that directly prompting the llm for reward scores during reinforcement learning can be more effective than the conventional approach of training a reward model based on llm preferences. wang et al. (2023f) propose conditioned-rlft, which treats different data sources as coarse-grained reward labels and develops 18 a class-conditioned policy to effectively utilize the varying qualities of data, which is a reinforcement learning-free supervised learning approach. cui et al. (2023a) propose a large-scale, high-quality, and diversified preference dataset labeled by gpt4 for comprehensive feedback. tunstall et al. (2023), by proposing distilled direct preference optimiza- tion (rafailov et al., 2023) on ultrafeedback, obtaining a small by powerful llm. 4.2.3 value attaining alignment with human preferences allows large models to optimize human satisfaction by operating in a manner that aligns with human preferences. however, to establish trustworthy llms, the notion of ’aligning llms with human values’ is proposed and the key principles of alignment are often summarized as the “hhh” criteria: helpful, harmless, honest (weidinger et al., 2021; askell et al., 2021). numerous methods have been undertaken for building trustworthy llms. however, due to the intrinsic difficulty of this aim, which is still an unsolved problem for proprietary models (sun et al., 2024a), most existing methods rely on constructing high-quality human prefer- ence datasets (ji et al., 2023b; solaiman and dennison, 2021; bai et al., 2022b; qiu et al., 2022; kiesel et al., 2022; liu et al., 2022a), utilizing human-written rules as constrains (glaese et al., 2022; sun et al., 2023b, 2024b), etc. for detailed progress on trustworthy llms, please further refer to yao et al. (2023a); liu et al. (2023i); sun et al. (2024a). though slightly under-explored, aligning llms with human values by distilling is still possible (bai et al., 2022a; cui et al., 2023a; yang et al., 2024a; sun et al., 2024b). for instance, bai et al. (2022a) propose rlaif, utilizing ai- generated labels to interactively improve both helpfulness and harmlessness. sun et al. (2024b) prompt the student model with 16 principles as guidelines for generating help- ful, ethical, and reliable responses. similarly, both harmless and harmful generations could be elicited by modifying the prompts, and then are used to train the preference model (yang et al., 2024a). cui et al. (2023a) utilize gpt- 4 to rank generations regarding helpfulness, truthfulness, and honesty. liu et al. (2023b) advance the alignment of llms with societal values by incorporating simulated social interactions into the training process. this approach encom- passes a range of elements, including demonstrations that are both in alignment and in conflict with social norms, as well as collective ratings, in-depth feedback, and responses that are revised iteratively. 4.3 agent 4.3.1 tool using while recent llms have shown proficiency in solving var- ious tasks, they still tend to make mistakes when handling large numerical values or executing intricate mathematical calculations (qian et al., 2022; she et al., 2023; manikandan et al., 2023; liang et al., 2023b; mialon et al., 2023). thus equipping llm agents with the capability to utilize tools has been increasingly focused on. commonly used methods mainly relied on human-curated data for training (parisi et al., 2022; nakano et al., 2022; qin et al., 2023c; song et al., 2023b) or prompt designing(cai et al., 2023; shenet al., 2023a; hao et al., 2024). recently, distillation-based methods are also proposed (schick et al., 2023; zhang, 2023; patil et al., 2023; tang et al., 2023a; qin et al., 2023a; yuan et al., 2023a; gao et al., 2023b; wang et al., 2024; shen et al., 2024; yuan et al., 2024b). toolformer (schick et al., 2023) utilizes a self-supervised manner, avoiding large human annotations, to obtain the most required apis to use and further distill this capability to the model itself. the performance of the gpt-j-based toolformer surpasses opt (66b) (zhang et al., 2022) and gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed to enhance llms with graph reasoning skills using exter- nal graph reasoning api tools by adopting chatgpt to annotate and augment a larger graph reasoning statement dataset for training. gorilla (patil et al., 2023) addresses the limitations of current llms in generating accurate input arguments and reduces the problem of ”hallucination” or generating incorrect api usage and it collects thousands of models from platforms like huggingface and torch hub as the api calls and utilizes gpt4 to generate synthetic instruction data for training. gpt4tools (yang et al., 2023b) introduces to enable open-source llms like llama and opt to use multimodal tools, a capability previously limited to advanced proprietary models like chatgpt and gpt-4. the approach involves generating an instruction-following dataset by prompting an advanced teacher model with mul- timodal contexts, using the low-rank adaptation optimiza- tion. toolalpaca (tang et al., 2023a) proposes a framework aimed at enhancing the tool-use capabilities of compact language models for embodied intelligence. it creates a dataset with 3938 instances from over 400 real-world tool apis across 50 categories and utilizes chatgpt to generate documentation for each prompt for later training. toolllm (qin et al., 2023a) proposes a comprehensive framework for enhancing llms with tool-use proficiency, focusing on data creation, model training, and evaluation by distilling from chatgpt. their toolllama shows impressive performance in executing complex instructions and handling new apis, rivaling chatgpt. craft (yuan et al., 2023a) builds a general tool creation and retrieval framework, which uti- lizes gpt4 to generate code snippets as the created tools. during the inference, other small llms could select and retrieve from the generated code snippets to execute or generate other methods conditioned on the given snippets. confucius (gao et al., 2023b) introduces a tiered training strategy for llms to master tool usage through a graduated curriculum and an innovative method called iterative self- instruction from introspective feedback (isif) for dynamic dataset enhancement to handle complex tools. mllm-tool (wang et al., 2024) is a multi-modal tool agent capable of interpreting instructions embedded in visual or audio content through the integration of multi-modal encoders with open-source large language models. as a trainable method, the initial instruction-answer pairs are generated by utilizing gpt4. shen et al. (2024) demonstrate that small llms are weak tool learners and proposes a multi-llm framework that decomposes the tool-use ability of a single model into a planner, caller, and summarizer for the tool using, leading to a supreme performance. the two-stage 19 training strategy introduced by this work is powered by chatgpt and gpt4 for collecting execution trajectories for the training set. yuan et al. (2024b) notice the potential issue of the current lengthy tool documentation, which hinders llms from understanding how to utilize a tool, thus proposing easytool to purify the important infor- mation from extensive documentation. the ground truth summarization of the training documents is obtained by using chatgpt. 4.3.2 planning another important aspect for llm agents is the ability to decompose high-level tasks to a chosen set of actionable steps (huang et al., 2022b), which is especially useful when acting in interactive environments. huang et al. (2022b) first demonstrate that llms can generate plausible goal-driven action plans without training, introduces non-invasive tools to enhance model executability, and assesses these methods through human evaluation to balance executability and semantic accuracy. most existing methods utilize prompting strategies for task planning (singh et al., 2022; zhou et al., 2023b; song et al., 2023c; wang et al., 2023g; yao et al., 2023b; liu et al., 2023j; hao et al., 2023; hu et al., 2023a), or building human-curated data for training (lin et al., 2023a; valmeekam et al., 2023). recently, there have also been some distilling methods emerging (chen et al., 2023b; zeng et al., 2023a; yin et al., 2023a; qiao et al., 2024; kong et al., 2023). fireact (chen et al., 2023b) introduces an innovative ap- proach for refining llms. this method involves fine-tuning smaller-scale llms using agent trajectories that are derived from a variety of tasks and prompting techniques. applying this method with trajectories generated by gpt4 has been shown to consistently enhance performance. agenttuning (zeng et al., 2023a) aims to enhance the performance of llms in executing agent tasks without sacrificing their wide-ranging capabilities. by utilizing a new dataset called agentinstruct, which includes high-quality interaction tra- jectories, it applies a hybrid instruction-tuning approach that merges these trajectories with general domain instruc- tions. lumos (yin et al., 2023a) pertains to a novel frame- work designed to train agents using a unified data format and modular architecture based on open-source llms. this system comprises three key modules: planning, grounding, and execution, enabling the decomposition of tasks into subgoals and actionable steps. tptu-v2 (kong et al., 2023) focuses on improving the task planning and tool usage abili- ties of llms in real-world scenarios, by utilizing data gener- ated by human experts or llms. it introduces a framework comprising three components: an api retriever, an llm finetuner, and a demo selector. autoact (qiao et al., 2024) proposes an agent learning framework that does not require large-scale annotated data or synthetic trajectories from high-resource models like gpt-4. instead, it uses a self- instruct method to generate its own planning trajectories with limited initial data. it then applies a division-of-labor strategy, creating sub-agents specialized in different aspects of the task completion process. distillation also works out for the training of embodied multi-modal agents (sumers et al., 2023; yang et al., 2023c; ma et al., 2023a; du et al., 2023a; sumers et al., 2023). for instance, sumers et al. (2023) aim to enhance the ability ofai agents to follow instructions by using pretrained vision- language models to provide supervision for understanding and acting upon language within their operational environ- ment, leveraging model distillation and hindsight experi- ence replay to teach them contextually relevant interactions in a simulated 3d setting. emma (yang et al., 2023c) evalu- ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using imitation learning, guided by an expert language model (like chatgpt), which operates in a corresponding text- based simulation, focusing on the same tasks. 4.4 nlp task specialization nlp tasks often grapple with challenges like data scarcity, interpretability issues, privacy concerns, and noisy data. the “knowledge” section of our survey illustrates various methods for distilling knowledge from llms, effectively setting the stage for student models to adapt to a range of nlp tasks. this knowledge provides supervision for the training of student models through information aug- mentation (e.g., cot and explanation), data augmentation, and semantic representation. by transferring the distilled knowledge from llms, student models can better handle diverse nlp challenges, improving task performance and addressing data limitations more robustly. 4.4.1 natural language understanding natural language understanding (nlu) is a fundamen- tal nlp task that involves comprehending and interpret- ing human language. the knowledge distilled from llms, such as through data labeling or augmentation, is typi- cally transferred into encoder-based language models like bert (vaswani et al., 2017) and roberta (liu et al., 2019). regarding the task of classification, certain studies have been noteworthy (dai et al., 2023a; gilardi et al., 2023; he et al., 2023b; gao et al., 2023a; chenglin et al., 2023; li et al., 2023g). auggpt (dai et al., 2023a) focuses on both general and clinical domain text classification. to address the limitations of small-scale clinical datasets, which often lack expert annotation and are subject to stringent privacy regulations, auggpt utilizes knowledge from teacher llms to rephrase each sentence in the training samples. this process creates multiple conceptually similar but seman- tically distinct samples, enhancing the dataset’s richness and diversity. another approach is demonstrated by gilardi et al. (2023), who employ chatgpt as an annotator to cate- gorize inputs. this method has been shown to outperform crowd-workers in several tasks, including relevance, stance, topics, and frame detection. furthermore, he et al. (2023b) propose targeted data generation (tdg), a novel approach for identifying challenging subgroups within a dataset. tdg leverages llms, along with human-in-the-loop, to generate new data specifically tailored for these subgroups, thereby enriching the dataset and improving model performance in sentiment analysis and natural language inference tasks. to facilitate the clinical information extraction task, tang et al. (2023b) elicit diverse samples from llms by providing examples and different seeds of clinical entities, i.e. the curation manner. 20 several studies have also focused on multiple nlu tasks (ding et al., 2023a; he et al., 2023a; wang et al., 2021a; he et al., 2022; ye et al., 2022; meng et al., 2022). for example, he et al. (2023a) utilize the knowledge in gpt-3.5 to annotate inputs with labels and explanations for various nlu tasks, including user input and keyword relevance assessment, boolq, and wic. wang et al. (2021a) employ few-shot prompts to expand high-quality training data using gpt-3, i.e. the expansion manner. beyond merely employing a single approach to elicit nlp task knowledge, ding et al. (2023a) explore a combination of labeling ,ex- pansion , and curation methods to extract knowledge from gpt-3 for distilling data for both sequence- and token-level nlp tasks. 4.4.2 natural language generation natural language generation (nlg) is a key aspect of eval- uating the capabilities of llms, encompassing tasks such as summarization, machine translation, and other open-ended text generation tasks. known for their potent generative abilities and creativity, llms excel in these areas, making them prime sources for distilling knowledge into student models tailored for nlg tasks (xu et al., 2023c, 2024b; ramnath et al., 2023; agarwal et al., 2024). additionally, the knowledge distilled from llms can be effectively used for nlg task-specific data augmentation (jung et al., 2023; wang et al., 2021b; guo et al., 2023a; yang and nicolai, 2023; wang et al., 2023h; yang et al., 2023d). while the previous sections have focused on the works about open- ended generation and multi-turn dialogue, this part will specifically highlight the distillation techniques relevant to other nlg tasks. although automatic metrics often favor smaller, fine- tuned models in summarization tasks, human evaluators tend to prefer the summaries generated by llms. address- ing this discrepancy, xu et al. (2023c) develop a student sum- marization model by distilling a gptsumm dataset, which comprises over 4 million paragraph-summary pairs gener- ated by querying gpt-3.5. in a different approach, jung et al. (2023) introduce ‘impossible distillation,’ a method that creates high-quality summarization-specific dataset from weak teacher llms. this method involves training a stu- dent model on the generated dataset and enhancing its capabilities through self-knowledge. turning to the task of machine translation, where creating parallel corpora is tra- ditionally expensive and time-consuming, yang and nicolai (2023) propose a three-step distillation process. this process involves generating seeds of verbs and nouns, forming sen- tences, and then translating these sentences. their findings suggest that while the distilled dataset may lack diversity, it effectively improves the translation signal for training student translation models. to distill high-quality content- grounded data automatically, genie (yehudai et al., 2024) proposes a general methodology containing three key steps: (a) preparation of the content, (b) distillation of responses from a teacher llm corresponding to the content, and (c) filtering mechanism to ensure the quality and faithfulness of the generated data. genie demonstrates that student models trained through this distilled data can match or even surpass models trained on human-generated data.4.4.3 information retrieval information retrieval (ir) represents a crucial branch of computer science, focused on efficiently retrieving infor- mation relevant to user queries from extensive reposito- ries (cai et al., 2022; liu et al., 2022b; feng et al., 2023; shen et al., 2023b). a typical ir system encompasses three main components: the query rewriter, the retriever, and the reranker. recent studies have highlighted the effective- ness of employing llms in ir systems, e.g. in enhancing the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in ir systems faces challenges, primarily due to their slower generation speed, which conflicts with the low-latency re- quirements of ir tasks (sun et al., 2023a). as a result, the kd of llms emerges as a more promising approach for ir, offering a way to infuse the distilled knowledge from llms into various stages of the ir pipeline without compromising on speed. there has been a significant body of work demonstrating how knowledge distilled from llms can benefit each component of the ir system, including the query rewriter (srinivasan et al., 2022; ma et al., 2023c), the retriever (dai et al., 2023b; sachan et al., 2022, 2023; schick and sch ¨utze, 2021; meng et al., 2023; peng et al., 2023b), and thereranker (bonifacio et al., 2022; sun et al., 2023a; pradeep et al., 2023a,b; saad-falcon et al., 2023; ferraretto et al., 2023; jeronymo et al., 2023; sun et al., 2023c). query rewriter. the query rewriter (qr) is a pivotal com- ponent in ir systems, tasked with enhancing the precision and expressiveness of user queries by refining or modifying the initial query to more accurately align with the user’s information needs. one notable approach is quill (srini- vasan et al., 2022), which introduces a two-stage distillation method for query intent understanding. initially, a retrieval- augmented llm, serving as the ‘professor,’ is distilled into a non-retrieval augmented teacher llm, aiming to bolster its understanding capabilities. subsequently, this enhanced teacher llm is distilled into a final student model using a large dataset, further refining the process. incorporating the qr into ir systems, ma et al. (2023c) develop a ’rewrite- retrieve-read’ framework. this process begins with an llm rewriting the queries via prompting, followed by a retrieval-augmented reading stage. to integrate the rewrit- ten queries effectively into the ir system, the knowledge gleaned from the llm is distilled into a compact student rewriter. this rewriter is then fine-tuned using feedback from the llm reader through reinforcement learning. retriever and reranker. in ir systems, the retriever is designed to efficiently locate the top-k relevant texts from a large corpus. it encodes both queries and documents into vector representations and performs retrieval by computing the dot product between these vectors. the reranker further refines the order of the retrieved documents to improve the overall quality of the output. this is achieved in two primary ways, including pointwise reranker and listwise reranker . pointwise reranker takes both the query and a single candidate document as input to directly generate a relevance score. listwise reranker directly reorders a list of input documents in terms of their relevance. 21 retriever and pointwise reranker. for the retriever and pointwise reranker, a common application of kd from llms is the generation of pseudo-queries for given documents. this approach aims to expand the pairwise data, enhancing the training of dense retrievers or rerankers. for example, inpars (bonifacio et al., 2022) utilizes gpt-3 to generate multiple pseudo-queries for an unlabeled document. to ensure the relevance of these queries, the system filters them based on the highest log probabilities of generating a query conditioned on the documents. subsequently, inpars fine-tunes a reranker based on monot5 (raffel et al., 2020). another similar approach, promptagator (dai et al., 2023b), introduces a few-shot dense retrieval method that leverages a small number of demonstrations from the target domain for pseudo-query generation. diverging from the reliance on unlabeled documents, sachan et al. (2022) distill knowl- edge from gpt-4 to curate diverse synthetic data for text embedding tasks across nearly 100 languages. they fine- tune powerful decoder-only llms, such as mistral-7b (jiang et al., 2023a), on this synthetic data using standard con- trastive loss. remarkably, this method demonstrates strong performance on text embedding and multilingual retrieval benchmarks without any labeled data. beyond generating pseudo-queries, teacher llms can also be employed to gen- erate relevance scores as soft labels. these scores are used to train the retriever by minimizing the kl-divergence loss between the teacher and student distributions, as explored by sachan et al. (2023). listwise reranker. a distinct set of studies focuses on listwise reranking, where its advantage lies in compar- ing multiple documents simultaneously to determine the optimal reorder. rankgpt (sun et al., 2023a) leverages gpt-4 to generate permutations for a group of candidate passages. to distill this listwise ranking knowledge into a pointwise student reranker, various training loss functions are employed, such as listwise cross-entropy (bruch et al., 2019), ranknet (burges et al., 2005), and lambdaloss (wang et al., 2018). building upon rankgpt’s framework, rankvi- cuna (pradeep et al., 2023a) and rankzephyr (pradeep et al., 2023b) further refine this approach by directly fine- tuning a listwise reranker using teacher-generated textual permutations. this enables the student reranker to produce sequences of ranked results directly, bypassing the interme- diate step of calculating individual relevance scores. 4.4.4 recommendation recommender systems are integral to enhancing user ex- perience in various online services, providing personalized content based on user preferences and behaviors. many works have demonstrated that llms could be directly used as recommenders without fine-tuning (wang et al., 2023i; dai et al., 2023c) or generate auxiliary textual features to benefit recommender systems (xi et al., 2023; ren et al., 2023; wei et al., 2024). (wang et al., 2023j; ren et al., 2023; wei et al., 2024). however, the real-time nature of online rec- ommender systems demands rapid response times, posing a challenge with the inherent inference latency associated with llms. to address this, several studies have explored ways to distill and integrate the knowledge from llms into recommender systems, thereby leveraging their advanced capabilities while mitigating latency issues for efficient real-time recommendations (mysore et al., 2023; zhang et al., 2023b; liu et al., 2023c). mysore et al. (2023) tackle data scarcity in narrative- driven recommendation (ndr), where users provide de- tailed descriptions of their preferences. they utilize gpt-3 to create synthetic narrative queries from user-item interac- tions via few-shot prompting, then distill this data into re- trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized content, aiding the training of content-based recommenda- tion models. to bridge the gap between language models and recommender systems, some research views behavior modeling as an extension of language modeling (cui et al., 2022; liu et al., 2023k). instructrec (zhang et al., 2023b), for instance, interprets recommendation as instruction fol- lowing. they use chatgpt to distill a wealth of user- personalized instruction data reflecting diverse preferences and intentions based on real historical interactions. this data is then used to fine-tune a 3b student language model specifically for recommendation purposes. 4.4.5 text generation evaluation text generation evaluation, i.e. nlg evaluation, focuses on assessing the quality of generated content. unlike tradi- tional nlg evaluation metrics like bleu (papineni et al., 2002) or rouge (lin, 2004), which primarily rely on surface-level text comparisons, llms, trained on extensive corpora and refined through techniques like rlhf, offer a more nuanced and human-aligned assessment. this so- phistication has led to the increasing use of llms in nlg evaluation (detailed further in (li et al., 2024b)). through kd of llms, student evaluators could enhance inference efficiency and achieve more flexible and highly customized evaluation (wang et al., 2023b; kim et al., 2024; xu et al., 2023d; jiang et al., 2023c; li et al., 2024a). pandalm (wang et al., 2023b) concentrates on a pairwise evaluator designed to compare two pieces of generated content. it utilizes a teacher llm (gpt-3.5) to judge which response is better for a given instruction and input, provid- ing reasons for its decision. addressing the need for cus- tomized and flexible criteria to meet realistic user demands, prometheus (kim et al., 2024) distills gpt-4 to construct a training dataset that includes reference answers and a vari- ety of customized scoring rubrics. this dataset is then used to tune llama for evaluating model-generated responses. instructscore (xu et al., 2023d) takes a more fine-grained ap- proach by using gpt-4 to create detailed analysis data. this data is employed to tune llama, enabling it to perform error analysis on generated texts compared to reference texts. the system further refines its evaluation capabilities through self-training with real model-generated response- reference pairs. for reference-free evaluation across diverse domains, tigerscore (jiang et al., 2023c) samples data from a variety of text generation datasets, such as summariza- tion, translation, and data-to-text. it distills error analysis knowledge from gpt-4 and uses this to fine-tune llama for more nuanced evaluation. lastly, to adapt evaluation to real-world scenarios beyond conventional nlp tasks, auto-j (li et al., 2024a) collects real-world user queries and their evaluations from a teacher llm. this massive dataset 22 of real-world scenarios is then used to distill evaluation knowledge into llama through fine-tuning, enhancing its practical applicability. 4.4.6 code llms, trained on extensive corpora containing code, are highlighted for their proficiency in code-related tasks. their capabilities extend beyond direct code generation to include the provision of external knowledge and data, which is crucial in distilling their expertise into smaller, more effi- cient models. several works have successfully distilled code knowledge from llms into those compact and specialized code models (chaudhary, 2023; rozi `ere et al., 2023; gu- nasekar et al., 2023; wei et al., 2023; chen et al., 2023a; liu et al., 2023d; yu et al., 2024; jain et al., 2023; su and mcmillan, 2023; guo et al., 2023d). a primary focus in these student code models is on code generation, a task of both common utility and practical significance. for instance, code alpaca (chaudhary, 2023) fine-tunes llama using self-instruct with chatgpt-distilled instructions specifically for code generation tasks. similarly, code llama-instruct (rozi `ere et al., 2023) is fine-tuned via self-instruct, prompting llama-2 (touvron et al., 2023) with coding problems, and further refined with unit tests. phi- 1 (gunasekar et al., 2023) aims to enhance the quality of dis- tilled code data by extracting “textbook quality” data from a teacher llm, incorporating python textbook and exercise data. magicoder (wei et al., 2023) addresses potential biases in teacher llms by referencing a wealth of open-source code, yielding more diverse and grounded data for code generation. to consider the capability of the student model and leverage the feedback of the teacher, persd (chen et al., 2023a) introduces a personalized distillation method where the teacher llm refines the student’s generated code based on the execution feedback of the executor. however, these models primarily target the code gener- ation task, lacking generalizability across a broader range of code-related tasks. to address this issue, mftcoder (liu et al., 2023d) utilizes self-instruct to distill diverse code data from teacher llms for various tasks, such as code comple- tion and text-to-code generation, training a student model via multi-task learning. wavecoder (yu et al., 2024), in contrast, creates a comprehensive instruction tuning dataset covering four universal code-related tasks distilled from gpt-3.5-turbo. wavecoder first selects a diverse coreset of raw data using the kcentergreedy (sener and savarese, 2018) clustering method, then employs the teacher llm for generating task definitions and outputs. the teacher model also plays a role in evaluating and filtering this data. notably, wavecoder demonstrates superior generalization across different code-related tasks compared to other open- source models. 4.5 multi-modality multimodal large language models (mllms) surpass tra- ditional language-only llms by understanding and pro- cessing information across multiple modalities, more closely mirroring human perception and enabling a broader range of real-world applications. there is a growing trend towards developing mllms that follow multimodal instructions,facilitating tasks with enhanced levels of interactivity. to ad- dress the scarcity of multimodal instruction-following data and to harness the commonsense and world knowledge embedded in teacher llms, numerous studies have focused on multimodal knowledge distillation from llms (liu et al., 2023e; zhao et al., 2023b; wang et al., 2023e; chen et al., 2023c; park et al., 2023; pi et al., 2023; zhao et al., 2023c; liu et al., 2023f; wu et al., 2023b; luo et al., 2023d; jiang et al., 2023d; li et al., 2023c; xu et al., 2023e). vision-language. in the vision-language domain, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, including captions and bounding boxes, and distills gpt-4 for generating new data in the context of seed examples. this approach creates a llava-instruct-150k dataset, which serves as the foundation for further developments like llava-1.5 (liu et al., 2023l) and gpt4roi (zhang et al., 2023e), enhancing the instruction- following capabilities of mllms. to expand the dataset’s scale, svit (zhao et al., 2023b) introduces a 4.2 million image dataset, distilled from gpt-4 by leveraging manual image annotations. it employs a novel data recipe to select an informative, diverse, and balanced subset of training data. lvis-instruct4v (wang et al., 2023e) leverages gpt- 4v (openai, 2023), a powerful large multimodal model, as a teacher to distill a more accurate and context-aware instruction-following dataset, focusing on fine-grained understanding. further advancements include integrating specific region referencing in image-based instruction following. for instance, shikra (chen et al., 2023c) uses gpt-4 to distill referential question-answer pairs from the flickr30k (plummer et al., 2015) dataset, enhancing the understanding of referential regions within images. lskd (park et al., 2023) introduces localized references to specific image regions, prompting the teacher llm to generate commonsense inferences about these areas. to enhance the visual instruction tuning pipeline with text-rich images, llavar (zhang et al., 2023d) employs the text-only gpt-4 as a teacher, using recognized texts and image captions to generate 16k conversation pairs for text-rich images. the resultant student mllm demonstrates enhanced interaction skills in content that combines both text and imagery. multiple modalities. to extend knowledge distillation of llms to encompass more modalities, such as audio and video, several innovative approaches have been in- troduced. these methods typically involve transforming these modalities into a textual format comprehensible to teacher llms, followed by the distillation of the teacher. macaw-llm (lyu et al., 2023) leverages gpt-4 to generate instruction-response pairs corresponding to the content of images or videos. mimic-it (li et al., 2023f) aims to broaden the scope to language, image, and video understanding, creating a substantial dataset with 2.8 million multimodal instruction-response pairs distilled from chatgpt. chat- bridge (zhao et al., 2023d), on the other hand, represents a novel approach in multimodal language modeling. it translates various non-textual modalities into text, combin- ing fine-grained and global descriptions. this information 23 verticalization distillationlaw lawyerllama (huang et al., 2023b), lawgpt (cui et al., 2023b), fuzi (wu et al., 2023d) medical and healthcarehuatuogpt (zhang et al., 2023c), huatuogpt-ii (chen et al., 2023d), doctorglm (xiong et al., 2023), alpacare (zhang et al., 2023f), huatuo (wang et al., 2023a), chatdoctor (li et al., 2023i), medalpaca (han et al., 2023), pmc-llama (wu et al., 2023e), disc-medllm (bao et al., 2023a) finance xuanyuan (zhang and yang, 2023) sciencedarwin (xie et al., 2023a), sciglm (zhang et al., 2024), wizardmath (luo et al., 2023b), mammoth (yue et al., 2023a), tora (gou et al., 2024), astrollama-chat (perkowski et al., 2024), g-llava (gao et al., 2023c), gimlet (zhao et al., 2023f), llm-prop (rubungo et al., 2023), instructmol (cao et al., 2023a), prot2text (abdine et al., 2023), biomedgpt (luo et al., 2023e), xtrimopglm (chen et al., 2024e), k2 (deng et al., 2023), oceangpt (bi et al., 2023), marinegpt (zheng et al., 2023b), geogalactica (lin et al., 2024), miscellaneous educhat (dan et al., 2023), owl (guo et al., 2023b) fig. 7: taxonomy of verticalization distillation. is then used to distill responses from chatgpt or gpt-4 through an in-context learning process, effectively bridging the gap between different modalities. others. beyond distilling instruction-following data, sev- eral methods have emerged that concentrate on harnessing different aspects of knowledge from llms. for instance, emma (yang et al., 2023c) trains an mllm to act as an embodied reflex agent within a visual environment. it achieves this by distilling gpt-4’s skills in a parallel textual world, generating actions and providing reflective feedback. silkie (li et al., 2023h) takes a unique approach by distilling preferences from gpt-4v , focusing on criteria like helpfulness and visual faithfulness. ha et al. (2023) represent another innovative direction, where it generates, labels, and distills diverse robot-centric exploration experiences by llms into a multi-task visuo-linguo-motor policy. 5 d omain -specified vertical distillation this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc. it delves into cus- tomizing distilled llms for these fields, showing its signifi- cant role in enhancing domain-specific ai applications. the taxonomy of these works is shown in figure 7. 5.1 law law holds a crucial position in molding societies, over- seeing human interactions, and ensuring justice prevails. informed decision-making, legal interpretation, and the pro- vision of legal advice by professionals hinge on precise and current information. legal intelligent applications in different scenarios usually require combinations of multiple fundamental capabilities of legal text retrieval, understand- ing, reasoning and generating (zhang et al., 2023g; sun, 2023; lai et al., 2023). addressing the intricacies of legal ter- minology, subtle interpretations, and the constant evolution of legislation presents distinctive challenges that demand customized resolutions. to handle the above challenges, several studies have investigated the customization of llms for intelligent legal services (cui et al., 2023b; yue et al., 2023b; huang et al., 2023b; wu et al., 2023d). this involves a continued pre-training process on extensive legal corpora, followed by fine-tuning with self-constructed instructions or augmented data using advanced llms.huang et al. (2023b) have unveiled a chinese legal large model named lawyerllama. the model undergoes an initial pre-training phase on an extensive legal corpus, systematically assimilating knowledge of the chinese legal system. subsequently, fine-tuning occurs through the analy- sis of objective questions from the chinese national judicial examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge to specific scenarios. cui et al. (2023b) present lawgpt, built upon the foundation of openllama. the model is trained using a construction process that incorporates real- world legal text, legal regulations, judicial interpretations, and actual legal consultation data. additionally, the authors utilize the chatgpt api for assisted construction, enabling the generation of supplementary data derived from the existing dataset. wu et al. (2023d) have developed a large- scale chinese legal model (named fuzi) with chatglm as its foundation. this model undergoes training on an extensive chinese legal corpus, which incorporates unsu- pervised judicial language data, including diverse judgment documents and legal regulations. additionally, it undergoes supervised judicial fine-tuning with data encompassing le- gal qa and case retrieval. fuzi’s training also involves both general instruction fine-tuning datasets, such as alpaca, and domain-specific instruction fine-tuning datasets from lawyerllama (huang et al., 2023b) and lawgpt (cui et al., 2023b). 5.2 medical and healthcare the integration of llms carries substantial promise in fun- damentally reshaping the landscape of medical data anal- ysis, comprehension, and smart medical services (singhal et al., 2023; yang et al., 2024b). significant research endeav- ors have been dedicated to adapting general-purpose llms to the medical domain, given the ever-expanding wealth of information encompassing electronic health records, med- ical literature, and clinical data. especially in healthcare, llms are revolutionizing patient care, research, and admin- istrative efficiency. they enhance diagnostic accuracy by an- alyzing patient data and medical literature, offering person- alized recommendations, and identifying potential drug in- teractions. llms also streamline administrative tasks by au- tomating patient documentation and processing insurance claims, reducing the burden on healthcare providers and 24 improving patient experiences. furthermore, they facilitate medical research by synthesizing vast amounts of data to uncover new insights into diseases and treatments (will be discussed later). this integration of llms into healthcare is paving the way for more informed clinical decision-making, improved patient outcomes, and more efficient healthcare systems. these adaptations extend across a spectrum, ranging from refining the precision of medical diagnoses (wang et al., 2023k) and providing personalized treatment rec- ommendations (zhu et al., 2023) to automating routine administrative processes within healthcare settings. while existing studies predominantly concentrate on training using dedicated medical dialogue datasets com- prising medical textbooks (wu et al., 2023e), biomedical papers (luo et al., 2023e) medical knowledge-graphs (bao et al., 2023b), or authentic doctor-patient interactions (bao et al., 2023b), an expanding body of research is delving into the augmentation of medical instruction-following data with advanced llms to enhance the alignment of the intricacies within practical user instructions. zhang et al. (2023c) introduce huatuogpt specifically tailored for med- ical consultations. the model leverages both distilled data from chatgpt and real-world data from doctors during the supervised fine-tuning stage. in a parallel effort, xiong et al. (2023) construct a dataset of medical dialogues in chinese, employing chatgpt’s assistance. their method- ology encompassed various techniques to train doctor- glm, an easily deployable llm designed for tasks such as diagnoses, drug recommendations, and other medical advice. zhang et al. (2023f) fine-tune llama-series models using 52k diverse, machine-generated, medical instruction- following data named medinstruct-52k. this effort resulted in the development of alpacare, a model demonstrating robust medical proficiency and generalizability across both general and medical-specific domain free-form instruction evaluations. in a different vein, wang et al. (2023a) propose huatuo, a llama-based model that undergoes supervised fine-tuning with generated qa instances. this refinement process enhances the model’s possession of more reliable medical knowledge. li et al. (2023i) introduce chatdoctor, which was first trained as a generic conversation model based on llama. it utilized 52k instruction-following data from stanford university’s alpaca project (taori et al., 2023). subsequently, the conversation model underwent fine-tuning on a dataset of 100k patient-physician conver- sations collected from an online medical consultation web- site. this two-step training process underscores the model’s adaptability to diverse conversational contexts, particularly those specific to patient-physician interactions. built upon existing datasets, medalpaca (han et al., 2023) proposes to reconstruct the data with gpt-3.5-turbo, which is then used to fine-tune llms for effective medical applications. furthermore, pmc-llama (wu et al., 2023f) proposes a training framework (i.e., continual pre-training and domain-specific multi-task supervised fine-tuning) to adapt a general llm to the medicine domain, where gpt- 4 is leveraged to write synonymous sentences for data augmentation in the sft. to adapt llms to real-world medical consultation, disc-medllm (bao et al., 2023a) leverages gpt-3.5 to 1) construct 50k qa pairs in a few-shot manner and 2) re-generate the 420k dialogues based on real cases, which are then used to train llms in a supervised fine-tuning manner. more recently, huatuogpt- ii (chen et al., 2023d) proposes a one-stage training with instruction-formatting unification of domain data collection for medical adaption upon llms, where gpt-4 is used to formulate medical questions to fine-tuning instructions. these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- ploration of various methodologies, these approaches pro- vide valuable insights into the challenges and potential breakthroughs at the intersection of cutting-edge language models and medical applications. 5.3 finance the application of llms to the finance domain (xue et al., 2023) significantly transforms how financial data is ana- lyzed, decisions are made, and customer interactions are managed. in finance, llms offer unprecedented capabil- ities in understanding complex financial documents, pre- dicting market trends, and automating risk assessment, thus enabling more informed and faster decision-making processes. by processing and analyzing vast amounts of unstructured financial data, such as news articles, reports, and real-time market feeds, llms can identify patterns and insights that were previously inaccessible, leading to more accurate forecasts and strategic financial planning. furthermore, llms enhance customer experiences through personalized financial advice, automated customer service, and sophisticated chatbots that can handle intricate queries. this level of automation and insight has the potential to increase efficiency, reduce operational costs, and improve compliance and risk management practices in financial insti- tutions, making llms a transformative force in the finance sector. knowledge distillation from a proprietary llm is still under-explored, and most existing works focus on adapting llms to finance applications by continual pre-training on finance-specific corpora (wu et al., 2023g; lu et al., 2023) or fine-tuning in a supervised manner on multi-task finance- specific instructions (yang et al., 2023e; xie et al., 2023b; wang et al., 2023l). specifically, xuanyuan (zhang and yang, 2023) lever- ages self-instruct over seed data and self-qa over struc- tured/unstructured data to generate instruction data in the finance domain, which is used to train a finance llm. 5.4 science the integration of llms into the science domain (taylor et al., 2022; yin et al., 2023b) represents a paradigm shift in research, knowledge discovery, and the dissemination of scientific information. in science, llms are leveraged to digest and synthesize vast amounts of literature, aiding in the identification of new research opportunities and the ac- celeration of scientific breakthroughs. they facilitate the un- derstanding of complex scientific concepts by summarizing research papers, generating hypotheses, and even drafting research proposals and manuscripts, thus significantly re- ducing the time researchers spend on literature review and 25 enabling them to focus more on experimental work. llms also democratize access to scientific knowledge by pro- viding layperson summaries of complex research findings, making science more accessible to non-experts and fostering a broader public understanding of scientific advancements. by enhancing the efficiency of research workflows and fostering interdisciplinary collaborations, llms are poised to accelerate the pace of scientific discovery and innovation across various fields. to distill knowledge from an llm, darwin series (xie et al., 2023a) utilizes a semi self- instruct for instruction generation for science papers, which is then used to fine-tune an llm. sciglm (zhang et al., 2024) proposes to train a scientific llm, which prompts a teacher llm to generate detailed answers for unlabelled scientific questions, as well as a self-reflective critic-and- revise to improve data quality. besides the above knowledge distillation methods to adapt llms to science, we will also delve into how the distillation happens in sub-domains, e.g., mathematics, as- tronautics, chemistry, etc. mathematics. the application of llms within the sub- domain of mathematics heralds a transformative era in mathematical research, education, and problem-solving (azerbayev et al., 2023; yu et al., 2023b). llms in mathemat- ics facilitate the exploration and understanding of complex mathematical theories and problems by providing intuitive explanations, proofs, and solutions that can bridge the gap between advanced mathematical concepts and learn- ers at various levels. these models have shown potential in conjecturing new mathematical theorems and patterns, thus opening new avenues for research and discovery that might not have been readily accessible to humans alone. in education, they serve as personalized tutors, offering students step-by-step guidance through mathematical prob- lems and adapting explanations to the learner’s level of un- derstanding. this democratizes access to high-quality math- ematical education and fosters a deeper appreciation and understanding of mathematics among a broader audience. by enhancing collaborative efforts through the generation of new ideas and the simplification of complex concepts, llms are poised to significantly advance the field of math- ematics, making it more accessible, efficient, and innova- tive. wizardmath (luo et al., 2023b) enhances the mathe- matical reasoning capabilities of llama-2 by applying the novel reinforcement learning from evol-instruct feedback (rleif) method, significantly outperforming other open- source llms on the gsm8k and math benchmarks, as well as surpassing several closed-source llms including chatgpt-3.5 and minerva. mammoth (yue et al., 2023a) is a series of open-source llms specifically developed for gen- eral math problem-solving, achieving superior performance on nine mathematical reasoning datasets. utilizing a novel instruction tuning dataset called mathinstruct, which com- bines chain-of-thought and program-of-thought rationales, mammoth models demonstrate substantial improvements over existing models. tora (gou et al., 2024), a series of tool-integrated reasoning agents, significantly advances mathematical problem-solving by combining natural lan- guage reasoning with the use of external computational tools. it markedly outperforms existing open-source modelson 10 mathematical reasoning datasets, showcasing notable improvements over both rationale-based and program- based approaches, and introduces innovative training tech- niques such as output space shaping to enhance model rea- soning capabilities. g-llava (gao et al., 2023c) introduces a significant advancement in geometric problem-solving for llms by leveraging a multimodal approach that combines text and image data. this model, utilizing the geo170k dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. astronautics. the application of llms in astronau- tics (nguyen et al., 2023) propels the field forward. astrollama-chat (perkowski et al., 2024) is an ad- vancement of the astrollama model, leveraging a 7b- parameter llama-2 model and targeted continual pre- training on a curated astronomy corpus to enhance per- formance in astronomy-focused question-answering. this model demonstrates significant improvements in special- ized topic comprehension and introduces a chat-enabled version for the astronomy community, highlighting the effectiveness of domain-specific knowledge distillation in achieving superior performance on specialized topics. chemistry and materials science. the integration of llms into chemistry and materials science has revolutionized the way researchers approach the discovery and develop- ment of new compounds and materials. by analyzing vast datasets and scientific literature, llms can predict the prop- erties and behaviors of substances, significantly accelerating the innovation cycle. gimlet (zhao et al., 2023f), graph instruction based molecule zero-shot learning, is a novel approach to molecule property prediction that integrates graph and text data within a single language model framework, aiming to improve instruction-based zero-shot learning for molec- ular tasks. by leveraging a transformer mechanism with generalized position embedding and decoupled attention, gimlet significantly outperforms traditional molecule-text baselines in zero-shot learning scenarios, demonstrating the model’s effectiveness in generalizing from instructions to a broad range of molecule-related tasks without prior explicit task-specific training. llm-prop (rubungo et al., 2023), leveraging the t5 model, showcases how llms can outperform sota graph neural networks in predicting the physical and electronic properties of crystalline solids from text descriptions. this approach underscores the potential of text-based methods in materials science, offering significant improvements in prediction accuracy while also contribut- ing a benchmark dataset, textedge, to foster further re- search in this emerging field. instructmol (cao et al., 2023a) integrates multi-modal data, aligning molecular structures with natural language instructions for drug discovery tasks. through a novel two-stage instruction-tuning approach, it significantly enhances performance in molecule-related tasks, establishing a reliable molecular assistant that outper- forms existing llms and reduces the performance gap with specialized models. this demonstrates the value of multi- modal integration in developing versatile tools for complex domains like drug discovery. 26 biology. in the field of biology, particularly in the study of pro- teins, dna, and rna, llms are revolutionizing our under- standing of the fundamental molecules of life. by analyzing vast datasets of biological sequences and structures, llms can predict the three-dimensional shapes of proteins, poten- tial functions, and interactions at a scale and speed beyond traditional computational methods. this capability is critical for unraveling the complexities of biological systems, ad- vancing drug discovery by identifying targets and designing molecules with high precision, and understanding genetic diseases through the interpretation of genomic variations. prot2text (abdine et al., 2023) introduces a novel multi- modal framework for generating protein function descrip- tions in free text by combining gnns and llms. this approach, which integrates structural and sequential protein information, highlights the transformative impact of knowl- edge distillation through the fusion of gnns and llms for accurate protein function prediction, potentially revolu- tionizing research in bioinformatics and biological sciences. biomedgpt (luo et al., 2023e) introduces a multimodal generative pre-trained transformer specifically designed for the biomedicine domain, emphasizing the significance of aligning molecular, protein, and natural language modal- ities to enhance biomedical question-answering, molecule, and protein qa tasks. this framework showcases the critical role of knowledge distillation in bridging the gap between complex biological data and human language, thereby fa- cilitating groundbreaking advancements in drug discovery and therapeutic target identification. xtrimopglm (chen et al., 2024e), a unified 100b-scale pre-trained transformer model, addresses both protein understanding and genera- tion tasks by integrating autoencoding and autoregressive pre-training objectives. its significant advancements over existing models in 18 protein understanding benchmarks and its capability in de novo protein sequence generation highlight the model’s importance in advancing the field of protein science through knowledge distillation. geography, geology, and environmental science. the inte- gration of llms into geography, geology, and environmen- tal science is revolutionizing these fields by enhancing data analysis, predictive modeling, and interdisciplinary research (roberts et al., 2023; lin et al., 2023b; wang et al., 2023m). k2 (deng et al., 2023), the first-ever llm specialized in the geoscience domain, demonstrates the significant impact of knowledge distillation in vertical domain specialization. by adapting the general-domain llama-7b model with a 5.5b token geoscience corpus and introducing the geosig- nal instruction tuning dataset, k2 showcases enhanced performance in geoscience knowledge understanding and utilization. the model’s development highlights a novel approach to efficiently gather domain-specific data and align model responses to specialized user queries, underpin- ning the importance of domain-specified vertical distillation in advancing research and applications within geoscience. oceangpt (bi et al., 2023), introduced as the first llm for ocean science tasks, underscores the vital role of knowl- edge distillation in the vertical domain of oceanography. it leverages doinstruct, a novel framework for generating domain-specific instruction data through multi-agent col-laboration, and establishes oceanbench, a benchmark for evaluating llms in the ocean domain. the model’s comprehensive experiments demonstrate its superior capa- bility in understanding and generating knowledge for ocean science, showcasing the significant potential of targeted knowledge distillation in enhancing domain-specific model performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language model tailored for marine science. utilizing the marine-5m dataset, which includes over 5 million marine image-text pairs, marinegpt excels in providing detailed, accurate, and domain-specific responses. this advancement underscores the model’s ability to significantly enhance marine knowl- edge comprehension and application, emphasizing the crit- ical role of domain-specific distillation in bridging the gap between general-purpose models and specialized domain requirements. geogalactica (lin et al., 2024) represents a pioneering step in specializing llms for geoscience, lever- aging a 30 billion parameter model pre-trained on a vast geoscience corpus. this model, notable for being the largest of its kind within the geoscience domain, showcases the significant potential of knowledge distillation in fostering scientific discoveries by bridging artificial intelligence with geoscience research and applications. 5.5 miscellaneous the expansion of llms into various verticals beyond the ones previously discussed showcases their versatility and transformative potential across numerous industries and societal sectors. llms are being tailored to meet the spe- cific needs and challenges of different domains, from legal and governmental to entertainment and beyond, providing sophisticated natural language understanding, generation, and decision-making capabilities. education. educhat (dan et al., 2023) is a large-scale lan- guage model-based chatbot system designed for the educa- tion domain. it aims to revolutionize intelligent education by providing personalized, fair, and compassionate support to teachers, students, and parents. knowledge distillation is emphasized through the pre-training on an educational corpus and fine-tuning on custom instructions to activate education-specific functions like open question answering, essay assessment, and emotional support. educhat demon- strates the importance of domain-specified knowledge dis- tillation in enhancing the performance of llms within specific verticals, offering a significant contribution to in- telligent education technology. it operations. owl (guo et al., 2023b) is a specialized llm tailored for it operations, focusing on enhancing efficiency and analysis within this domain. the model leverages a unique owl-instruct dataset covering a broad range of it-related information, employing a mixture-of- adapter strategy for efficient domain-specific tuning. this approach significantly improves it operation tasks’ perfor- mance, highlighting the critical role of knowledge distilla- tion in adapting general llms to specialized fields such as it operations, thereby pushing forward the frontier in specialized ai applications within this sector. 27 6 o penproblems further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions. in the field of instruction tuning, one of the most commonly used methods for distillation, zhou et al. (2023a) propose that only 1000 human-curated high-quality data is enough for the alignment of llms, hypothesizing that llms have learned the required knowl- edge from pretraining and only a small amount of data is required for the alignment. its finding further raises a new question, how to automatically select the data for better distillation? chen et al. (2023e) directly apply chatgpt to rate each data sample together with explanations, and then the data is selected based on the rating. cao et al. (2023b) split the existing instruction-tuning datasets and trains a linear function to select the most effective data based on their statistical properties. li et al. (2023j) propose a data selection pipeline similar to self-distillation, in which the llm firstly learns from a small subset of the data to get the basic ability, and then further uses this learned model to rate for the original dataset. du et al. (2023b) propose to consider three aspects including quality, coverage, and necessity for the filtering process. li et al. (2023k) select instruction data by evaluating their one-shot improvement on a hold-out set. li et al. (2024f) recently propose superfiltering, which is able to utilize small language models like gpt2 to filter out the high-quality subset from a given high-quality dataset. despite the emergence of these works working on data fil- tering, how to efficiently select the optimal distillation data for llms, and how much data is required for distillation are still unsolved. reduce the distillation cost (lightweight methods) de- spite the remarkable abilities of the latest llms, their sig- nificant resource requirements underscore the urgent need to find efficient solutions to overcome these challenges. common ways to further reduce the distillation cost include model compression and efficient fine-tuning. in the realm of model compression, quantization (frantar et al., 2023; dettmers et al., 2022; kim et al., 2023c; tao et al., 2022b; yao et al., 2022; xiao et al., 2023), parameter pruning (ma et al., 2023d; zhang et al., 2023h; frantar and alistarh, 2023), and low-rank approximation (xu et al., 2023g; li et al., 2023l) are commonly utilized. in the realm of efficient fine-tuning, parameter efficient fine-tuning (hu et al., 2023b; liu et al., 2022c; wang et al., 2022b; hu et al., 2021; li and liang, 2021; liu et al., 2022d), and memory efficient fine-tuning (dettmers et al., 2023; kim et al., 2023d; malladi et al., 2024) are utilized. a detailed survey on efficient large language models can be found here in wan et al. (2024b). the problem that remains is how can we further compress the model and build effective distillation algorithms. multi-teacher distillation most of the existing distilled models are distilled from a single teacher model, how- ever, it is widely accepted that models trained with dif- ferent sources of data have various capabilities. thus a question arises: is it possible to distill knowledge from different teacher models into one student model? babyl- lama (timiryasov and tastet, 2023) proposes to distill the knowledge from both the gpt2 and llama into the small-size student models. ensemble-instruct (lee et al., 2023b) tries to generate both instructions and responses ensembled from several different llms with rougel as the indicator. fusellm (wan et al., 2024a) externalizes the collective knowledge and unique strengths by leveraging the genera- tive distributions of different llms aiming to train a student model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated in table 3, the majority of teacher llms are closed-source due to their advanced capabilities. consequently, current methodologies primarily focus on using the generations from these models as hard labels, training student models through simple supervised fine-tuning. however, beyond the straightforward imitation of output behaviors via hard labels, there is a growing interest in harnessing richer knowledge from teacher llms, including feedback and feature knowledge, as well as exploring diverse combina- tions of knowledge elicitation methods. as highlighted in thefeedback section, teachers can provide various types of feedback based on the student’s outputs (lee et al., 2023a; jiang et al., 2023b; chen et al., 2023a). similarly, the feature section discusses how knowledge based on features, such as logits serving as soft labels, can offer deeper, intrinsic insights into the teacher model (gu et al., 2024; agarwal et al., 2024). these explorations have demonstrated promis- ing outcomes, suggesting that access to a broader spectrum of knowledge can significantly enhance student model per- formance beyond what is achievable through simple sft distillation alone. this highlights the critical need for further research into varied knowledge extraction methods from teacher llms to augment the effectiveness of kd processes. overcoming catastrophic forgetting during distillation previous research has delved into the fine-tuning of llms to acquire the ability to follow instructions or transfer knowledge for forthcoming tasks, skills, or domains, lever- aging advancements in llm technology. nevertheless, in- vestigations have revealed that the continual fine-tuning of llms on particular datasets (skills, domains) can lead to a phenomenon known as catastrophic forgetting, wherein previously acquired knowledge and problem-solving abil- ities for earlier tasks are compromised (chen et al., 2023f; kotha et al., 2023; koloski et al., 2023; wu et al., 2024; luo et al., 2023f). earlier studies in machine learning and deep learning have investigated various techniques to help mitigate forgetting during the fine-tuning or continue learn- ing process, such as rehearsal, which entails periodically revisiting and training on past data (kirkpatrick et al., 2017; rostami et al., 2019; rolnick et al., 2019), as well as reg- ularization methods like elastic weight consolidation (lee et al., 2017), or dynamic architecture methods (mallya et al., 2018; wang et al., 2022c; hu et al., 2023c; chen et al., 2023f). to address the challenges of catastrophic forgetting and to enhance the diversity of generated instructions in knowl- edge distillation for llms, jiang et al. (2023b) randomly sample an instruction from the easy instructions and also prompt the generator to generate a new instruction that belongs to the same domain as the sampled one. in a similar vein, li et al. (2023m) study the problem of instruction- 28 tuning in multi-modal llms knowledge distillation and introduce a competitive distillation framework. the model tries to produce new instructions that differ in content but are similar in difficulty to the original pictures in the multi- modal augmentation phase, so as to alleviate catastrophic forgetting of the model and enhance the diversity of the instruction tuning pool. chen et al. (2023f) propose the lifelong-moe (mixture-of experts) architecture based on general language models, which dynamically adds model capacity via adding experts with regularized pretraining. additionally, the model also introduces implicit regulariza- tion via distillation of the knowledge from old experts and gatings to effectively preserve old knowledge. zeng et al. (2023b) propose a new generative-based rehearsal method as dirichlet continual learning (dcl). this method com- bines task distribution modeling and knowledge distillation to mitigate catastrophic forgetting without requiring access to the old data. to evaluate the effectiveness of instruction tuning in the context of continuous learning tasks, zhang et al. (2023i) introduce a more challenging yet practical problem called continual instruction tuning (cit) and also establish a benchmark suite consisting of learning and eval- uation protocols. although current research has explored some simple methods to alleviate knowledge forgetting dur- ing model fine-tuning or knowledge distillation processes, effectively avoiding catastrophic forgetting across domains and skills remains a challenging issue. how to retain the original model’s capabilities effectively during knowledge distillation or transfer processes is still a challenging prob- lem. trustworthy knowledge distillation trustworthiness in llms is paramount, encompassing attributes such as truth- fulness, safety, fairness, robustness, privacy, and adherence to machine ethics (sun et al., 2024a). the rapid advancement of llms brings to the forefront concerns regarding their trustworthiness, stemming from their complex outputs, the biases present in vast training datasets, and the potential inclusion of private information. current efforts in kd of llms primarily focus on distilling various skills from llms, with relatively little attention paid to trustworthiness aspects. existing studies tend to concentrate on a subset of trustworthiness aspects, such as helpfulness, honesty, and harmlessness (bai et al., 2022a; yang et al., 2024a; cui et al., 2023a). consequently, in the distillation process, student models may inherit issues related to trustworthiness from their teacher llms. as assessed in sun et al. (2024a), smaller open-source llms generally fall short of their proprietary counterparts in trustworthiness metrics. therefore, consid- ering trustworthiness alongside the distillation of capabil- ities into student models is crucial. it is imperative that future research on kd not only enhances the capabilities of student models but also ensures that broader aspects of trustworthiness are meticulously addressed. weak-to-strong distillation. the concept of “weak-to- strong generalization” in llms (burns et al., 2023) empha- sizes the potential to leverage weak supervision to elicit the advanced capabilities of more powerful models. this approach challenges the traditional distillation paradigm by suggesting that even with limited or imperfect supervision, it is possible to enhance the performance of llms sig-nificantly. this necessitates exploring innovative strategies that enable weaker models to guide the learning process of stronger ones effectively, highlighting the importance of developing methods that can bridge the gap between these models. such research could unlock new avenues for improving llms’ efficiency and effectiveness, making the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision can unlock the full capabilities of much stronger models. through experiments with pre-trained language models in the gpt-4 family across nlp , chess, and reward modeling tasks, it finds that finetuning strong models on weak labels leads to better performance than their weak supervisors, demonstrating weak-to-strong generalization. then, li et al. (2024g) introduce superfiltering, a method that employs smaller, weaker models like gpt-2 to select high-quality data for fine-tuning larger, more capable models such as llama2. this approach is rooted in discovering a strong consistency in evaluating instruction tuning data difficulty across models of varying sizes. more recently, ji et al. (2024) introduce aligner, a novel approach for aligning llms with human values and intentions by utilizing weak supervisory signals from smaller models to improve the performance of larger models. however, burns et al. (2023) find that achieving the full capabilities of strong models requires more than naive finetuning, suggesting the need for further research in this area. therefore, open questions still remain about 1) what are the theoretical and practical limits of weak-to-strong distillation? can weak supervision reliably extract and enhance the full spectrum of capabilities in stronger models across all domains, or are there inherent limitations based on model architecture or task specificity? 2) how do we identify or design the optimal weak su- pervisors for distilling knowledge into stronger models? is there a framework or criteria to predict which weak models would be most effective in guiding the learning process of more complex models for specific tasks? 3) to what extent are weak-to-strong distillation techniques transferable and scalable across different sizes and types of models? how can these methods be adapted to ensure efficacy and ef- ficiency in distilling knowledge from very large models to significantly smaller ones, especially in resource-constrained environments? self-alignment. aligning llms traditionally relies heavily on human or teacher llms to supply extensive preference data. consequently, the alignment of the student model is limited by the quantity of distilled preference data and the teacher’s capabilities. self-alignment offers a promising alternative, aiming to enhance alignment beyond the con- straints of teacher-provided preferences. in self-alignment, the student model endeavors to autonomously improve and align its responses with desired behaviors, including generating model-written feedback, critiques, and explana- tions. several studies have explored utilizing the student model’s inherent capabilities to generate knowledge for alignment (bai et al., 2022a; sun et al., 2024b; li et al., 2024c; yuan et al., 2024a). beyond merely producing improved responses (bai et al., 2022a; sun et al., 2024b), implemen- tations of self-alignment include employing the student as 29 its reward model to offer feedback (yuan et al., 2024a), a strategy that merges self-knowledge with feedback methods of eliciting knowledge. we advocate for increasingly lever- aging the student model itself to provide feedback, thereby enhancing self-alignment capabilities. this approach not only facilitates moving beyond traditional human/teacher preference-based rewards but also opens avenues for con- tinual self-improvement and alignment. 7 c onclusion and discussion this survey has traversed the expansive domain of knowl- edge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field. we have underscored the pivotal role of kd in democratizing access to the advanced capabilities of proprietary llms, thereby fostering a more equitable ai landscape. through meticulous examination, we have highlighted how kd serves as a bridge, enabling resource- constrained entities to benefit from the profound advance- ments in llms without the prohibitive costs associated with training and deploying state-of-the-art models. our exploration delineates the multifaceted approaches to kd, ranging from algorithmic innovations and skill en- hancement to domain-specific distillations. each segment reveals the nuanced complexities and potentialities inherent in tailoring distilled models to emulate the sophisticated un- derstandings and functionalities of their more cumbersome counterparts. notably, the integration of data augmentation strategies within kd processes emerges as a critical lever for enhancing distillation in this llm era, underscoring the synergistic potential between generating context-rich training data and the distillation endeavor. as we project into the future, several avenues for re- search beckon. the evolving landscape of ai, marked by rapid advancements in model architectures and training methodologies, presents both challenges and opportunities for kd. the quest for more efficient, transparent, and ethical ai models necessitates continued innovation in kd tech- niques, especially those that can navigate the delicate bal- ance between model fidelity, computational efficiency, and ethical considerations. furthermore, the exploration of kd in nascent areas such as weak-to-strong generalization, self- alignment, multi-modal llms, real-time adaptation, and personalized ai services promises to expand the horizons of what distilled models can achieve. therefore, knowledge distillation of llms stands at a critical juncture, embodying the potential to significantly influence the trajectory of ai development and application. as this survey elucidates, the concerted efforts of the re- search community in pushing the boundaries of kd will be instrumental in realizing the vision of accessible, efficient, and responsible ai for all. legal considerations for using llm outputs: impor- tantly, it’s crucial to note the legal implications of utilizing llm outputs, such as those from chatgpt4, llama5, etc. we strongly advocate compliance with the terms of use specified by the model providers, such as the restrictions on developing competitive products, and so on. 4. https://openai.com/policies/business-terms 5. https://llama.meta.com/llama-downloads/references l. ouyang, j. wu, x. jiang, d. almeida, c. wainwright, p . mishkin, c. zhang, s. agarwal, k. slama, a. ray et al. , “training language models to follow instructions with human feedback,” advances in neural information processing systems , vol. 35, pp. 27 730–27 744, 2022. openai, :, j. achiam, s. adler, s. agarwal, l. ahmad, i. akkaya, f. l. aleman, d. almeida, j. altenschmidt, s. altman, s. anadkat, r. avila, i. babuschkin, s. balaji, v . balcom, p . baltescu, h. bao, m. bavarian, j. belgum, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, a. cann, b. carey, c. carlson, r. carmichael, b. chan, c. chang, f. chantzis, d. chen, s. chen, r. chen, j. chen, m. chen, b. chess, c. cho, c. chu, h. w. chung, d. cummings, j. currier, y. dai, c. decareaux, t. degry, n. deutsch, d. deville, a. dhar, d. dohan, s. dowling, s. dunning, a. ecoffet, a. eleti, t. eloundou, d. farhi, l. fedus, n. felix, s. p . fishman, j. forte, i. fulford, l. gao, e. georges, c. gibson, v . goel, t. gogineni, g. goh, r. gontijo-lopes, j. gordon, m. grafstein, s. gray, r. greene, j. gross, s. s. gu, y. guo, c. hallacy, j. han, j. harris, y. he, m. heaton, j. heidecke, c. hesse, a. hickey, w. hickey, p . hoeschele, b. houghton, k. hsu, s. hu, x. hu, j. huizinga, s. jain, s. jain, j. jang, a. jiang, r. jiang, h. jin, d. jin, s. jomoto, b. jonn, h. jun, t. kaf- tan, łukasz kaiser, a. kamali, i. kanitscheider, n. s. keskar, t. khan, l. kilpatrick, j. w. kim, c. kim, y. kim, h. kirchner, j. kiros, m. knight, d. kokotajlo, łukasz kondraciuk, a. kondrich, a. konstantinidis, k. kosic, g. krueger, v . kuo, m. lampe, i. lan, t. lee, j. leike, j. leung, d. levy, c. m. li, r. lim, m. lin, s. lin, m. litwin, t. lopez, r. lowe, p . lue, a. makanju, k. mal- facini, s. manning, t. markov, y. markovski, b. mar- tin, k. mayer, a. mayne, b. mcgrew, s. m. mckin- ney, c. mcleavey, p . mcmillan, j. mcneil, d. medina, a. mehta, j. menick, l. metz, a. mishchenko, p . mishkin, v . monaco, e. morikawa, d. mossing, t. mu, m. murati, o. murk, d. m ´ely, a. nair, r. nakano, r. nayak, a. nee- lakantan, r. ngo, h. noh, l. ouyang, c. o’keefe, j. pa- chocki, a. paino, j. palermo, a. pantuliano, g. parascan- dolo, j. parish, e. parparita, a. passos, m. pavlov, a. peng, a. perelman, f. de avila belbute peres, m. petrov, h. p . de oliveira pinto, michael, pokorny, m. pokrass, v . pong, t. powell, a. power, b. power, e. proehl, r. puri, a. rad- ford, j. rae, a. ramesh, c. raymond, f. real, k. rimbach, c. ross, b. rotsted, h. roussez, n. ryder, m. saltarelli, t. sanders, s. santurkar, g. sastry, h. schmidt, d. schnurr, j. schulman, d. selsam, k. sheppard, t. sherbakov, j. shieh, s. shoker, p . shyam, s. sidor, e. sigler, m. simens, j. sitkin, k. slama, i. sohl, b. sokolowsky, y. song, n. staudacher, f. p . such, n. summers, i. sutskever, j. tang, n. tezak, m. thompson, p . tillet, a. tootoonchian, e. tseng, p . tuggle, n. turley, j. tworek, j. f. c. uribe, a. vallone, a. vijayvergiya, c. voss, c. wainwright, j. j. wang, a. wang, b. wang, j. ward, j. wei, c. weinmann, a. welihinda, p . welinder, j. weng, l. weng, m. wiethoff, d. willner, c. winter, s. wolrich, h. wong, l. workman, s. wu, j. wu, m. wu, k. xiao, t. xu, s. yoo, k. yu, 30 q. yuan, w. zaremba, r. zellers, c. zhang, m. zhang, s. zhao, t. zheng, j. zhuang, w. zhuk, and b. zoph, “gpt- 4 technical report,” 2023. g. team, r. anil, s. borgeaud, y. wu, j.-b. alayrac, j. yu, r. soricut, j. schalkwyk, a. m. dai, a. hauth et al. , “gemini: a family of highly capable multimodal models,” arxiv preprint arxiv:2312.11805 , 2023. j. wei, y. tay, r. bommasani, c. raffel, b. zoph, s. borgeaud, d. yogatama, m. bosma, d. zhou, d. metzler, e. h. chi, t. hashimoto, o. vinyals, p . liang, j. dean, and w. fedus, “emergent abilities of large language models,” trans. mach. learn. res. , vol. 2022, 2022. [online]. available: https://openreview.net/forum?id=yzksu5zdwd j. wei, x. wang, d. schuurmans, m. bosma, f. xia, e. chi, q. v . le, d. zhou et al. , “chain-of-thought prompting elicits reasoning in large language models,” advances in neural information processing systems , vol. 35, pp. 24 824– 24 837, 2022. x. xu, c. tao, t. shen, c. xu, h. xu, g. long, and j. guang lou, “re-reading improves reasoning in large language models,” 2024. p . liang, r. bommasani, t. lee, d. tsipras, d. soylu, m. yasunaga, y. zhang, d. narayanan, y. wu, a. kumar, b. newman, b. yuan, b. yan, c. zhang, c. cosgrove, c. d. manning, c. r ´e, d. acosta-navas, d. a. hudson, e. zelikman, e. durmus, f. ladhak, f. rong, h. ren, h. yao, j. wang, k. santhanam, l. j. orr, l. zheng, m. y ¨uksekg ¨on¨ul, m. suzgun, n. kim, n. guha, n. s. chatterji, o. khattab, p . henderson, q. huang, r. chi, s. m. xie, s. santurkar, s. ganguli, t. hashimoto, t. icard, t. zhang, v . chaudhary, w. wang, x. li, y. mai, y. zhang, and y. koreeda, “holistic evaluation of language models,” corr , vol. abs/2211.09110, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09110 x. wu, r. duan, and j. ni, “unveiling security, privacy, and ethical concerns of chatgpt,” journal of information and intelligence , 2023. h. touvron, l. martin, k. stone, p . albert, a. almahairi, y. babaei, n. bashlykov, s. batra, p . bhargava, s. bhosale, d. bikel, l. blecher, c. c. ferrer, m. chen, g. cucurull, d. esiobu, j. fernandes, j. fu, w. fu, b. fuller, c. gao, v . goswami, n. goyal, a. hartshorn, s. hosseini, r. hou, h. inan, m. kardas, v . kerkez, m. khabsa, i. kloumann, a. korenev, p . s. koura, m.-a. lachaux, t. lavril, j. lee, d. liskovich, y. lu, y. mao, x. martinet, t. mihaylov, p . mishra, i. molybog, y. nie, a. poulton, j. reizen- stein, r. rungta, k. saladi, a. schelten, r. silva, e. m. smith, r. subramanian, x. e. tan, b. tang, r. taylor, a. williams, j. x. kuan, p . xu, z. yan, i. zarov, y. zhang, a. fan, m. kambadur, s. narang, a. rodriguez, r. stojnic, s. edunov, and t. scialom, “llama 2: open foundation and fine-tuned chat models,” 2023. a. q. jiang, a. sablayrolles, a. mensch, c. bamford, d. s. chaplot, d. de las casas, f. bressand, g. lengyel, g. lam- ple, l. saulnier, l. r. lavaud, m.-a. lachaux, p . stock, t. l. scao, t. lavril, t. wang, t. lacroix, and w. e. sayed, “mistral 7b,” 2023. l. zheng, w. chiang, y. sheng, s. zhuang, z. wu, y. zhuang, z. lin, z. li, d. li, e. p . xing, h. zhang, j. e. gonzalez, and i. stoica, “judging llm-as-a-judge with mt-bench and chatbot arena,” corr , vol. abs/2306.05685, 2023. [online].available: https://doi.org/10.48550/arxiv.2306.05685 l. sun, y. huang, h. wang, s. wu, q. zhang, c. gao, y. huang, w. lyu, y. zhang, x. li, z. liu, y. liu, y. wang, z. zhang, b. kailkhura, c. xiong, c. xiao, c. li, e. xing, f. huang, h. liu, h. ji, h. wang, h. zhang, h. yao, m. kellis, m. zitnik, m. jiang, m. bansal, j. zou, j. pei, j. liu, j. gao, j. han, j. zhao, j. tang, j. wang, j. mitchell, k. shu, k. xu, k.-w. chang, l. he, l. huang, m. backes, n. z. gong, p . s. yu, p .-y. chen, q. gu, r. xu, r. ying, s. ji, s. jana, t. chen, t. liu, t. zhou, w. wang, x. li, x. zhang, x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. j. gou, b. yu, s. j. maybank, and d. tao, “knowledge distillation: a survey,” international journal of computer vision , vol. 129, pp. 1789–1819, 2021. m. gupta and p . agrawal, “compression of deep learning models for text: a survey,” acm transactions on knowledge discovery from data (tkdd) , vol. 16, no. 4, pp. 1–55, 2022. s. y. feng, v . gangal, j. wei, s. chandar, s. vosoughi, t. mi- tamura, and e. hovy, “a survey of data augmentation approaches for nlp,” arxiv preprint arxiv:2105.03075 , 2021. r. taori, i. gulrajani, t. zhang, y. dubois, x. li, c. guestrin, p . liang, and t. b. hashimoto, “stanford alpaca: an instruction-following llama model,” https://github.com/ tatsu-lab/stanford alpaca, 2023. y. gu, l. dong, f. wei, and m. huang, “minillm: knowledge distillation of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=5h0qf7ibzz r. agarwal, n. vieillard, y. zhou, p . stanczyk, s. r. garea, m. geist, and o. bachem, “on-policy distillation of language models: learning from self-generated mistakes,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=3zktaqxlhw w. yuan, r. y. pang, k. cho, s. sukhbaatar, j. xu, and j. weston, “self-rewarding language models,” 2024. z. chen, y. deng, h. yuan, k. ji, and q. gu, “self-play fine-tuning converts weak language models to strong language models,” 2024. y. huang, y. chen, z. yu, and k. mckeown, “in-context learning distillation: transferring few-shot learning abil- ity of pre-trained language models,” 2022. g. cui, l. yuan, n. ding, g. yao, w. zhu, y. ni, g. xie, z. liu, and m. sun, “ultrafeedback: boosting lan- guage models with high-quality feedback,” arxiv preprint arxiv:2310.01377 , 2023. s. mukherjee, a. mitra, g. jawahar, s. agarwal, h. palangi, and a. awadallah, “orca: progressive learning from complex explanation traces of gpt-4,” arxiv preprint arxiv:2306.02707 , 2023. b. ding, c. qin, l. liu, y. k. chia, b. li, s. joty, and l. bing, “is gpt-3 a good data annotator?” in acl (1) . asso- ciation for computational linguistics, 2023, pp. 11 173– 11 195. s. chaudhary, “code alpaca: an instruction-following llama model for code generation,” https://github.com/ sahil280114/codealpaca, 2023. h. wang, c. liu, n. xi, z. qiang, s. zhao, b. qin, and 31 t. liu, “huatuo: tuning llama model with chinese medi- cal knowledge,” arxiv preprint arxiv:2304.06975 , 2023. lawgpt . github, 2023. d. zhang, z. hu, s. zhoubian, z. du, k. yang, z. wang, y. yue, y. dong, and j. tang, “sciglm: training scientific language models with self-reflective instruction annotation and tuning,” corr , vol. abs/2401.07950, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401. 07950 w.-l. chiang, z. li, z. lin, y. sheng, z. wu, h. zhang, l. zheng, s. zhuang, y. zhuang, j. e. gonzalez, i. stoica, and e. p . xing, “vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” march 2023. [online]. available: https://lmsys.org/blog/2023-03-30- vicuna/ c. xu, q. sun, k. zheng, x. geng, p . zhao, j. feng, c. tao, and d. jiang, “wizardlm: empowering large language models to follow complex instructions,” arxiv preprint arxiv:2304.12244 , 2023. w. x. zhao, k. zhou, j. li, t. tang, x. wang, y. hou, y. min, b. zhang, j. zhang, z. dong, y. du, c. yang, y. chen, z. chen, j. jiang, r. ren, y. li, x. tang, z. liu, p . liu, j.-y. nie, and j.-r. wen, “a survey of large language models,” 2023. x. he, z. lin, y. gong, a. jin, h. zhang, c. lin, j. jiao, s. m. yiu, n. duan, w. chen et al. , “annollm: making large language models to be better crowdsourced annotators,” arxiv preprint arxiv:2303.16854 , 2023. y. wang, z. yu, z. zeng, l. yang, c. wang, h. chen, c. jiang, r. xie, j. wang, x. xie, w. ye, s. zhang, and y. zhang, “pandalm: an automatic evaluation benchmark for llm instruction tuning optimization,” 2023. c. hsieh, c. li, c. yeh, h. nakhost, y. fujii, a. ratner, r. krishna, c. lee, and t. pfister, “distilling step-by-step! outperforming larger language models with less training data and smaller model sizes,” in acl (findings) . associ- ation for computational linguistics, 2023, pp. 8003–8017. a. mitra, l. d. corro, s. mahajan, a. codas, c. simoes, s. agarwal, x. chen, a. razdaibiedina, e. jones, k. aggar- wal, h. palangi, g. zheng, c. rosset, h. khanpour, and a. awadallah, “orca 2: teaching small language models how to reason,” 2023. c. xu, d. guo, n. duan, and j. j. mcauley, “baize: an open- source chat model with parameter-efficient tuning on self- chat data,” in emnlp . association for computational linguistics, 2023, pp. 6268–6278. x. yue, x. qu, g. zhang, y. fu, w. huang, h. sun, y. su, and w. chen, “mammoth: building math generalist mod- els through hybrid instruction tuning,” arxiv preprint arxiv:2309.05653 , 2023. l. chenglin, c. qianglong, w. caiyu, and z. yin, “mixed distillation helps smaller language model better reason- ing,” 2023. y. wang, y. kordi, s. mishra, a. liu, n. a. smith, d. khashabi, and h. hajishirzi, “self-instruct: aligning language model with self generated instructions,” arxiv preprint arxiv:2212.10560 , 2022. z. sun, y. shen, q. zhou, h. zhang, z. chen, d. cox, y. yang, and c. gan, “principle-driven self-alignment of language models from scratch with minimal human supervision,” advances in neural information processingsystems , vol. 36, 2024. z. luo, c. xu, p . zhao, q. sun, x. geng, w. hu, c. tao, j. ma, q. lin, and d. jiang, “wizardcoder: empowering code large language models with evol-instruct,” arxiv preprint arxiv:2306.08568 , 2023. h. luo, q. sun, c. xu, p . zhao, j. lou, c. tao, x. geng, q. lin, s. chen, and d. zhang, “wizardmath: empower- ing mathematical reasoning for large language models via reinforced evol-instruct,” arxiv preprint arxiv:2308.09583 , 2023. h. dai, z. liu, w. liao, x. huang, y. cao, z. wu, l. zhao, s. xu, w. liu, n. liu, s. li, d. zhu, h. cai, l. sun, q. li, d. shen, t. liu, and x. li, “auggpt: leveraging chatgpt for text data augmentation,” 2023. z. he, m. t. ribeiro, and f. khani, “targeted data generation: finding and fixing model weaknesses,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8506–8520. [online]. available: https://aclanthology.org/2023.acl-long.474 n. ding, y. chen, b. xu, y. qin, s. hu, z. liu, m. sun, and b. zhou, “enhancing chat language models by scaling high-quality instructional conversations,” in emnlp . as- sociation for computational linguistics, 2023, pp. 3029– 3051. s. gunasekar, y. zhang, j. aneja, c. c. t. mendes, a. d. giorno, s. gopi, m. javaheripi, p . kauffmann, g. de rosa, o. saarikivi, a. salim, s. shah, h. s. behl, x. wang, s. bubeck, r. eldan, a. t. kalai, y. t. lee, and y. li, “textbooks are all you need,” 2023. y. li, s. bubeck, r. eldan, a. del giorno, s. gunasekar, and y. t. lee, “textbooks are all you need ii: phi-1.5 technical report,” arxiv preprint arxiv:2309.05463 , 2023. phi-2: the surprising power of small lan- guage models , december 2023. [online]. avail- able: https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/ y. wei, z. wang, j. liu, y. ding, and l. zhang, “magicoder: source code is all you need,” 2023. z. yu, x. zhang, n. shang, y. huang, c. xu, y. zhao, w. hu, and q. yin, “wavecoder: widespread and versatile en- hanced instruction tuning with refined data generation,” 2024. j. ye, j. gao, q. li, h. xu, j. feng, z. wu, t. yu, and l. kong, “zerogen: efficient zero-shot learning via dataset generation,” in emnlp . association for computational linguistics, 2022, pp. 11 653–11 669. j. gao, r. pi, y. lin, h. xu, j. ye, z. wu, w. zhang, x. liang, z. li, and l. kong, “self-guided noise-free data generation for efficient zero-shot learning,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=h5opjgd lo6 l. h. bonifacio, h. q. abonizio, m. fadaee, and r. f. nogueira, “inpars: data augmentation for information retrieval using large language models,” corr , vol. abs/2202.05144, 2022. [online]. available: https://arxiv. org/abs/2202.05144 i. timiryasov and j.-l. tastet, “baby llama: knowledge 32 distillation from an ensemble of teachers trained on a small dataset with no performance penalty,” in proceedings of the babylm challenge at the 27th conference on computational natural language learning , a. warstadt, a. mueller, l. choshen, e. wilcox, c. zhuang, j. ciro, r. mosquera, b. paranjabe, a. williams, t. linzen, and r. cotterell, eds. singapore: association for computational linguistics, dec. 2023, pp. 279–289. [online]. available: https://aclanthology.org/2023.conll- babylm.24 c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre- trained language models via quantization,” arxiv preprint arxiv:2203.10705 , 2022. z. liu, b. oguz, c. zhao, e. chang, p . stock, y. mehdad, y. shi, r. krishnamoorthi, and v . chandra, “llm-qat: data-free quantization aware training for large language models,” arxiv preprint arxiv:2305.17888 , 2023. y. bai, s. kadavath, s. kundu, a. askell, j. kernion, a. jones, a. chen, a. goldie, a. mirhoseini, c. mckinnon, c. chen, c. olsson, c. olah, d. hernandez, d. drain, d. gan- guli, d. li, e. tran-johnson, e. perez, j. kerr, j. mueller, j. ladish, j. landau, k. ndousse, k. lukosuite, l. lovitt, m. sellitto, n. elhage, n. schiefer, n. mercado, n. das- sarma, r. lasenby, r. larson, s. ringer, s. johnston, s. kravec, s. e. showk, s. fort, t. lanham, t. telleen- lawton, t. conerly, t. henighan, t. hume, s. r. bow- man, z. hatfield-dodds, b. mann, d. amodei, n. joseph, s. mccandlish, t. brown, and j. kaplan, “constitutional ai: harmlessness from ai feedback,” 2022. l. tunstall, e. beeching, n. lambert, n. rajani, k. rasul, y. belkada, s. huang, l. von werra, c. fourrier, n. habib et al. , “zephyr: direct distillation of lm alignment,” arxiv preprint arxiv:2310.16944 , 2023. j. hong, q. tu, c. chen, x. gao, j. zhang, and r. yan, “cyclealign: iterative distillation from black-box llm to white-box models for better human alignment,” arxiv preprint arxiv:2310.16271 , 2023. h. lee, s. phatale, h. mansoor, k. lu, t. mesnard, c. bishop, v . carbune, and a. rastogi, “rlaif: scaling reinforcement learning from human feedback with ai feedback,” arxiv preprint arxiv:2309.00267 , 2023. y. jiang, c. chan, m. chen, and w. wang, “lion: adversarial distillation of closed-source large language model,” arxiv preprint arxiv:2305.12870 , 2023. h. chen, a. saha, s. hoi, and s. joty, “personalized distillation: empowering open-sourced llms with adaptive learning for code generation,” in the 2023 conference on empirical methods in natural language processing , 2023. [online]. available: https://openreview. net/forum?id=alxwmbcnvn k. yang, d. klein, a. celikyilmaz, n. peng, and y. tian, “rlcd: reinforcement learning from contrastive distilla- tion for lm alignment,” in the twelfth international confer- ence on learning representations , 2024. [online]. available: https://openreview.net/forum?id=v3xxtxwki6 j. jung, p . west, l. jiang, f. brahman, x. lu, j. fisher, t. sorensen, and y. choi, “impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing,” 2023. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, andj. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 c. gulcehre, t. l. paine, s. srinivasan, k. konyushkova, l. weerts, a. sharma, a. siddhant, a. ahern, m. wang, c. gu, w. macherey, a. doucet, o. firat, and n. de freitas, “reinforced self-training (rest) for language modeling,” 2023. e. zelikman, y. wu, j. mu, and n. d. goodman, “star: boot- strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,” arxiv preprint arxiv:1910.01108 , 2019. y. wen, z. li, w. du, and l. mou, “f-divergence minimization for sequence-level knowledge distillation,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 10 817–10 834. [online]. available: https: //aclanthology.org/2023.acl-long.605 c. liang, s. zuo, q. zhang, p . he, w. chen, and t. zhao, “less is more: task-aware layer-wise distillation for lan- guage model compression,” in international conference on machine learning . pmlr, 2023, pp. 20 852–20 867. m. kwon, s. m. xie, k. bullard, and d. sadigh, “reward de- sign with language models,” in iclr . openreview.net, 2023. b. peng, c. li, p . he, m. galley, and j. gao, “instruction tuning with gpt-4,” 2023. g. li, h. a. a. k. hammoud, h. itani, d. khizbullin, and b. ghanem, “camel: communicative agents for” mind” exploration of large scale language model society,” arxiv preprint arxiv:2303.17760 , 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, “openchat: advancing open-source language models with mixed-quality data,” sep. 2023, arxiv:2309.11235 [cs]. [online]. available: http://arxiv.org/abs/2309.11235 m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” arxiv preprint arxiv:2305.18395 , 2023. h. luo, y.-s. chuang, y. gong, t. zhang, y. kim, x. wu, d. fox, h. meng, and j. glass, “sail: search-augmented in- struction learning,” arxiv preprint arxiv:2305.15225 , 2023. a. asai, z. wu, y. wang, a. sil, and h. hajishirzi, “self- rag: learning to retrieve, generate, and critique through self-reflection,” arxiv preprint arxiv:2310.11511 , 2023. s. ye, y. jo, d. kim, s. kim, h. hwang, and m. seo, “selfee: iterative self-revising llm empowered by self-feedback generation,” blog post, may 2023. [online]. available: https://kaistai.github.io/selfee/ p . wang, l. li, l. chen, f. song, b. lin, y. cao, t. liu, and z. sui, “making large language models better reasoners with alignment,” 2023. d. cheng, s. huang, and f. wei, “adapting large language models via reading comprehension,” 2023. y. zhang, z. chen, y. fang, l. cheng, y. lu, f. li, w. zhang, 33 and h. chen, “knowledgeable preference alignment for llms in domain-specific question answering,” 2023. j. scheurer, j. a. campos, t. korbak, j. s. chan, a. chen, k. cho, and e. perez, “training language models with language feedback at scale,” 2023. s. kim, s. bae, j. shin, s. kang, d. kwak, k. yoo, and m. seo, “aligning large language models through synthetic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 677–13 700. [online]. available: https://aclanthology. org/2023.emnlp-main.844 p . roit, j. ferret, l. shani, r. aharoni, g. cideron, r. dadashi, m. geist, s. girgin, l. hussenot, o. keller, n. momchev, s. ramos garea, p . stanczyk, n. vieillard, o. bachem, g. elidan, a. hassidim, o. pietquin, and i. szpektor, “factually consistent summarization via reinforcement learning with textual entailment feedback,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 6252–6272. [online]. available: https://aclanthology.org/2023.acl-long.344 y. yang, e. chern, x. qiu, g. neubig, and p . liu, “alignment for honesty,” arxiv preprint arxiv:2312.07000 , 2023. r. liu, r. yang, c. jia, g. zhang, d. zhou, a. m. dai, d. yang, and s. vosoughi, “training socially aligned lan- guage models on simulated social interactions,” 2023. t. schick, j. dwivedi-yu, r. dess `ı, r. raileanu, m. lomeli, l. zettlemoyer, n. cancedda, and t. scialom, “tool- former: language models can teach themselves to use tools,” 2023. j. zhang, “graph-toolformer: to empower llms with graph reasoning ability via prompt augmented by chatgpt,” arxiv preprint arxiv:2304.11116 , 2023. s. g. patil, t. zhang, x. wang, and j. e. gonzalez, “gorilla: large language model connected with massive apis,” 2023. q. tang, z. deng, h. lin, x. han, q. liang, b. cao, and l. sun, “toolalpaca: generalized tool learning for lan- guage models with 3000 simulated cases,” 2023. y. qin, s. liang, y. ye, k. zhu, l. yan, y. lu, y. lin, x. cong, x. tang, b. qian, s. zhao, l. hong, r. tian, r. xie, j. zhou, m. gerstein, d. li, z. liu, and m. sun, “toolllm: facilitating large language models to master 16000+ real- world apis,” 2023. l. yuan, y. chen, x. wang, y. r. fung, h. peng, and h. ji, “craft: customizing llms by creating and retrieving from specialized toolsets,” 2023. s. gao, z. shi, m. zhu, b. fang, x. xin, p . ren, z. chen, j. ma, and z. ren, “confucius: iterative tool learning from introspection feedback by easy-to-difficult curriculum,” 2023. c. wang, w. luo, q. chen, h. mai, j. guo, s. dong, xiaohua, xuan, z. li, l. ma, and s. gao, “mllm-tool: a multimodal large language model for tool agent learning,” 2024. w. shen, c. li, h. chen, m. yan, x. quan, h. chen, j. zhang, and f. huang, “small llms are weak tool learners: a multi- llm agent,” 2024.b. chen, c. shu, e. shareghi, n. collier, k. narasimhan, and s. yao, “fireact: toward language agent fine-tuning,” 2023. a. zeng, m. liu, r. lu, b. wang, x. liu, y. dong, and j. tang, “agenttuning: enabling generalized agent abilities for llms,” 2023. d. yin, f. brahman, a. ravichander, k. chandu, k.-w. chang, y. choi, and b. y. lin, “lumos: learning agents with unified data, modular design, and open-source llms,” 2023. s. qiao, n. zhang, r. fang, y. luo, w. zhou, y. e. jiang, c. lv, and h. chen, “autoact: automatic agent learning from scratch via self-planning,” 2024. y. kong, j. ruan, y. chen, b. zhang, t. bao, s. shi, g. du, x. hu, h. mao, z. li, x. zeng, and r. zhao, “tptu-v2: boosting task planning and tool usage of large language model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” proceedings of the national academy of sciences , vol. 120, no. 30, jul. 2023. [online]. available: http: //dx.doi.org/10.1073/pnas.2305016120 z. wang, a. w. yu, o. firat, and y. cao, “towards zero-label language learning,” 2021. y. xu, r. xu, d. iter, y. liu, s. wang, c. zhu, and m. zeng, “inheritsumm: a general, versatile and compact summarizer by distilling from gpt,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 879–13 892. [online]. available: https: //aclanthology.org/2023.findings-emnlp.927 f. xu, w. shi, and e. choi, “recomp: improving retrieval- augmented lms with context compression and selective augmentation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=mljlvignhp s. ramnath, b. joshi, s. hallinan, x. lu, l. h. li, a. chan, j. hessel, y. choi, and x. ren, “tailoring self-rationalizers with multi-reward distillation,” 2023. s. wang, y. liu, y. xu, c. zhu, and m. zeng, “want to reduce labeling cost? gpt-3 can help,” in findings of the association for computational linguistics: emnlp 2021 , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 4195–4205. [online]. available: https: //aclanthology.org/2021.findings-emnlp.354 z. guo, p . wang, y. wang, and s. yu, “improving small language models on pubmedqa via generative data aug- mentation,” 2023. w. yang and g. nicolai, “neural machine translation data generation and augmentation using chatgpt,” 2023. k. srinivasan, k. raman, a. samanta, l. liao, l. bertelli, and m. bendersky, “quill: query intent with large language models using retrieval augmentation and multi-stage distillation,” in proceedings of the 2022 conference on empirical methods in natural language processing: industry track , y. li and a. lazaridou, eds. abu dhabi, uae: association for computational linguistics, dec. 2022, pp. 492–501. [online]. available: 34 https://aclanthology.org/2022.emnlp-industry.50 z. dai, v . y. zhao, j. ma, y. luan, j. ni, j. lu, a. bakalov, k. guu, k. b. hall, and m. chang, “promptagator: few-shot dense retrieval from 8 examples,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=gml46ympu2j r. meng, y. liu, s. yavuz, d. agarwal, l. tu, n. yu, j. zhang, m. bhat, and y. zhou, “augtriever: unsupervised dense retrieval by scalable data augamentation,” 2023. w. sun, l. yan, x. ma, s. wang, p . ren, z. chen, d. yin, and z. ren, “is chatgpt good at search? investigating large language models as re-ranking agents,” 2023. r. pradeep, s. sharifymoghaddam, and j. lin, “rankvicuna: zero-shot listwise document reranking with open-source large language models,” 2023. ——, “rankzephyr: effective and robust zero-shot listwise reranking is a breeze!” 2023. f. ferraretto, t. laitz, r. lotufo, and r. nogueira, “exaranker: synthetic explanations improve neural rankers,” in proceedings of the 46th international acm sigir conference on research and development in information retrieval , ser. sigir ’23. new york, ny, usa: association for computing machinery, 2023, p. 2409–2414. [online]. available: https://doi.org/10.1145/3539618.3592067 s. mysore, a. mccallum, and h. zamani, “large language model augmented narrative driven recommendations,” inproceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 777–783. [online]. available: https://doi.org/10.1145/3604915.3608829 j. zhang, r. xie, y. hou, w. x. zhao, l. lin, and j.-r. wen, “recommendation as instruction following: a large language model empowered recommendation approach,” 2023. q. liu, n. chen, t. sakai, and x.-m. wu, “once: boost- ing content-based recommendation with both open- and closed-source large language models,” 2023. s. kim, j. shin, y. cho, j. jang, s. longpre, h. lee, s. yun, s. shin, s. kim, j. thorne, and m. seo, “prometheus: inducing evaluation capability in language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=8eujatvekw w. xu, d. wang, l. pan, z. song, m. freitag, w. wang, and l. li, “instructscore: towards explainable text generation evaluation with automatic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5967–5994. [online]. available: https://aclanthology.org/2023.emnlp-main.365 d. jiang, y. li, g. zhang, w. huang, b. y. lin, and w. chen, “tigerscore: towards building explainable metric for all text generation tasks,” 2023. j. li, s. sun, w. yuan, r.-z. fan, hai zhao, and p . liu, “generative judge for evaluating alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=gtkfw6szgsb. rozi `ere, j. gehring, f. gloeckle, s. sootla, i. gat, x. e. tan, y. adi, j. liu, t. remez, j. rapin, a. kozhevnikov, i. evtimov, j. bitton, m. bhatt, c. c. ferrer, a. grattafiori, w. xiong, a. d ´efossez, j. copet, f. azhar, h. touvron, l. martin, n. usunier, t. scialom, and g. synnaeve, “code llama: open foundation models for code,” 2023. b. liu, c. chen, c. liao, z. gong, h. wang, z. lei, m. liang, d. chen, m. shen, h. zhou, h. yu, and j. li, “mftcoder: boosting code llms with multitask fine-tuning,” 2023. n. jain, t. zhang, w. chiang, j. e. gonzalez, k. sen, and i. stoica, “llm-assisted code cleaning for training accurate code generators,” corr , vol. abs/2311.14904, 2023. [online]. available: https://doi.org/10.48550/ arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up visual instruction tuning,” 2023. j. wang, l. meng, z. weng, b. he, z. wu, and y.-g. jiang, “to see is to believe: prompting gpt-4v for better visual instruction tuning,” 2023. k. chen, z. zhang, w. zeng, r. zhang, f. zhu, and r. zhao, “shikra: unleashing multimodal llm’s referential dialogue magic,” 2023. j. s. park, j. hessel, k. r. chandu, p . p . liang, x. lu, p . west, y. yu, q. huang, j. gao, a. farhadi, and y. choi, “localized symbolic knowledge distillation for visual commonsense models,” 2023. r. pi, j. gao, s. diao, r. pan, h. dong, j. zhang, l. yao, j. han, h. xu, l. kong, and t. zhang, “detgpt: detect what you need via reasoning,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 14 172–14 189. [online]. available: https: //aclanthology.org/2023.emnlp-main.876 l. zhao, e. yu, z. ge, j. yang, h. wei, h. zhou, j. sun, y. peng, r. dong, c. han, and x. zhang, “chatspot: bootstrapping multimodal llms via precise referring in- struction tuning,” 2023. f. liu, k. lin, l. li, j. wang, y. yacoob, and l. wang, “mitigating hallucination in large multi-modal models via robust instruction tuning,” 2023. s. wu, h. fei, l. qu, w. ji, and t.-s. chua, “next-gpt: any- to-any multimodal llm,” 2023. r. luo, z. zhao, m. yang, j. dong, d. li, p . lu, t. wang, l. hu, m. qiu, and z. wei, “valley: video assistant with large language model enhanced ability,” 2023. y. jiang, e. schoop, a. swearngin, and j. nichols, “iluvui: instruction-tuned language-vision modeling of uis from machine conversations,” 2023. y. li, c. zhang, g. yu, z. wang, b. fu, g. lin, c. shen, l. chen, and y. wei, “stablellava: enhanced visual in- struction tuning with synthesized image-dialogue data,” 2023. r. xu, x. wang, t. wang, y. chen, j. pang, and d. lin, “pointllm: empowering large language models to under- stand point clouds,” 2023. q. huang, m. tao, z. an, c. zhang, c. jiang, z. chen, z. wu, and y. feng, “lawyer llama technical report,” arxiv preprint arxiv:2305.15062 , 2023. 35 j. cui, z. li, y. yan, b. chen, and l. yuan, “chatlaw: open- source legal large language model with integrated ex- ternal knowledge bases,” arxiv preprint arxiv:2306.16092 , 2023. h. zhang, j. chen, f. jiang, f. yu, z. chen, g. chen, j. li, x. wu, z. zhiyi, q. xiao, x. wan, b. wang, and h. li, “huatuogpt, towards taming language model to be a doctor,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 10 859– 10 885. [online]. available: https://aclanthology.org/ 2023.findings-emnlp.725 j. chen, x. wang, a. gao, f. jiang, s. chen, h. zhang, d. song, w. xie, c. kong, j. li, x. wan, h. li, and b. wang, “huatuogpt-ii, one-stage training for medical adaption of llms,” corr , vol. abs/2311.09774, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09774 x. zhang and q. yang, “xuanyuan 2.0: a large chinese financial chat model with hundreds of billions parameters,” in proceedings of the 32nd acm international conference on information and knowledge management, cikm 2023, birmingham, united kingdom, october 21- 25, 2023 , i. frommholz, f. hopfgartner, m. lee, m. oakes, m. lalmas, m. zhang, and r. l. t. santos, eds. acm, 2023, pp. 4435–4439. [online]. available: https://doi.org/10.1145/3583780.3615285 t. xie, y. wan, w. huang, z. yin, y. liu, s. wang, q. linghu, c. kit, c. grazian, w. zhang, i. razzak, and b. hoex, “darwin series: domain specific large language models for natural science,” corr , vol. abs/2308.13565, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2308.13565 y. dan, z. lei, y. gu, y. li, j. yin, j. lin, l. ye, z. tie, y. zhou, y. wang, a. zhou, z. zhou, q. chen, j. zhou, l. he, and x. qiu, “educhat: a large-scale language model-based chatbot system for intelligent education,” corr , vol. abs/2308.02773, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.02773 h. guo, j. yang, j. liu, l. yang, l. chai, j. bai, j. peng, x. hu, c. chen, d. zhang, x. shi, t. zheng, l. zheng, b. zhang, k. xu, and z. li, “owl: a large language model for it operations,” corr , vol. abs/2309.09298, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.09298 y. kim and a. m. rush, “sequence-level knowledge distil- lation,” arxiv preprint arxiv:1606.07947 , 2016. s. han, h. mao, and w. j. dally, “deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding,” international confer- ence on learning representations (iclr) , 2016. v . gangal, s. y. feng, m. alikhani, t. mitamura, and e. hovy, “nareor: the narrative reordering problem,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 10 645–10 653. s. longpre, y. lu, z. tu, and c. dubois, “an exploration of data augmentation and sampling techniques for domain- agnostic question answering,” in proceedings of the 2nd workshop on machine reading for question answering , a. fisch, a. talmor, r. jia, m. seo, e. choi, and d. chen, eds. hong kong, china: association for computational linguistics, nov. 2019, pp. 220–227. [online]. available:https://aclanthology.org/d19-5829 p . west, c. bhagavatula, j. hessel, j. hwang, l. jiang, r. le bras, x. lu, s. welleck, and y. choi, “symbolic knowledge distillation: from general language models to commonsense models,” in proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 4602–4625. [online]. available: https://aclanthology.org/2022.naacl-main.341 z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, j. qian, b. peng, y. mao, w. chen, and x. yan, “explana- tions from large language models make small reasoners better,” 2022. n. ho, l. schmid, and s. yun, “large language models are reasoning teachers,” in acl (1) . association for computational linguistics, 2023, pp. 14 852–14 882. l. c. magister, j. mallinson, j. adamek, e. malmi, and a. severyn, “teaching small language models to reason,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: short papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 1773–1781. [online]. available: https://aclanthology.org/2023.acl-short.151 y. fu, h. peng, l. ou, a. sabharwal, and t. khot, “specializ- ing smaller language models towards multi-step reason- ing,” 2023. l. h. li, j. hessel, y. yu, x. ren, k.-w. chang, and y. choi, “symbolic chain-of-thought distillation: small models can also “think” step-by-step,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2665–2679. [online]. available: https://aclanthology.org/2023.acl- long.150 w. liu, g. li, k. zhang, b. du, q. chen, x. hu, h. xu, j. chen, and j. wu, “mind’s mirror: distilling self- evaluation capability and comprehensive thinking from large language models,” 2023. s. longpre, l. hou, t. vu, a. webson, h. w. chung, y. tay, d. zhou, q. v . le, b. zoph, j. wei et al. , “the flan collec- tion: designing data and methods for effective instruction tuning,” arxiv preprint arxiv:2301.13688 , 2023. y. anand, z. nussbaum, b. duderstadt, b. schmidt, and a. mulyar, “gpt4all: training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo,” github , 2023. q. si, t. wang, z. lin, x. zhang, y. cao, and w. wang, “an empirical study of instruction-tuning large language models in chinese,” in emnlp (findings) . association for computational linguistics, 2023, pp. 4086–4107. y. ji, y. deng, y. gong, y. peng, q. niu, l. zhang, b. ma, and x. li, “exploring the impact of instruction data scaling on large language models: an empirical study on real-world use cases,” 2023. m. wu, a. waheed, c. zhang, m. abdul-mageed, and a. f. 36 aji, “lamini-lm: a diverse herd of distilled models from large-scale instructions,” 2023. w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” 2023. y. yu, y. zhuang, j. zhang, y. meng, a. ratner, r. krishna, j. shen, and c. zhang, “large language model as at- tributed training data generator: a tale of diversity and bias,” 2023. f. wan, x. huang, d. cai, x. quan, w. bi, and s. shi, “knowledge fusion of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=jidsk12qcz q. zhao and b. zhu, “towards the fundamental limits of knowledge transfer over finite domains,” inneurips 2023 workshop on mathematics of modern machine learning , 2023. [online]. available: https: //openreview.net/forum?id=9qxoxqxa0n c. qin, w. xia, f. jiao, and s. joty, “improving in-context learning via bidirectional alignment,” 2023. n. boizard, k. el-haddad, c. hudelot, and p . colombo, “towards cross-tokenizer distillation: the universal logit distillation loss for llms,” arxiv preprint arxiv:2402.12030 , 2024. q. zhong, l. ding, l. shen, j. liu, b. du, and d. tao, “revis- iting knowledge distillation for autoregressive language models,” 2024. m. kim, s. lee, j. lee, s. hong, d.-s. chang, w. sung, and j. choi, “token-scaled logit distillation for ternary weight generative language models,” arxiv preprint arxiv:2308.06744 , 2023. z. chen, k. zhou, w. x. zhao, j. wan, f. zhang, d. zhang, and j.-r. wen, “improving large language models via fine- grained reinforcement learning with minimum editing constraint,” 2024. g. guo, r. zhao, t. tang, x. zhao, and j.-r. wen, “beyond imitation: leveraging fine-grained quality signals for alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=lnlju5c5dk z. allen-zhu and y. li, “towards understanding ensemble, knowledge distillation and self-distillation in deep learn- ing,” arxiv preprint arxiv:2012.09816 , 2020. t. zheng, s. guo, x. qu, j. guo, w. zhang, x. du, c. lin, w. huang, w. chen, j. fu et al. , “kun: answer polish- ment for chinese self-alignment with instruction back- translation,” arxiv preprint arxiv:2401.06477 , 2024. x. li, p . yu, c. zhou, t. schick, o. levy, l. zettlemoyer, j. e. weston, and m. lewis, “self-alignment with instruction backtranslation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=1oijhjbrst b. zhao, h. hajishirzi, and q. cao, “apt: adaptive pruning and tuning pretrained language models for efficient train- ing and inference,” arxiv preprint arxiv:2401.12200 , 2024. a. singh, j. d. co-reyes, r. agarwal, a. anand, p . patil, p . j. liu, j. harrison, j. lee, k. xu, a. parisi et al. , “beyond hu- man data: scaling self-training for problem-solving with language models,” arxiv preprint arxiv:2312.06585 , 2023. w. chen, d. song, and b. li, “grath: gradual self-truthifyingfor large language models,” 2024. a. hosseini, x. yuan, n. malkin, a. courville, a. sordoni, and r. agarwal, “v-star: training verifiers for self-taught reasoners,” 2024. a. askell, y. bai, a. chen, d. drain, d. ganguli, t. henighan, a. jones, n. joseph, b. mann, n. dassarma, n. elhage, z. hatfield-dodds, d. hernandez, j. kernion, k. ndousse, c. olsson, d. amodei, t. brown, j. clark, s. mccandlish, c. olah, and j. kaplan, “a general lan- guage assistant as a laboratory for alignment,” 2021. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, and j. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 h. chen, x. quan, h. chen, m. yan, and j. zhang, “knowl- edge distillation for closed-source language models,” arxiv preprint arxiv:2401.07013 , 2024. i. sason and s. verd ´u, “f-divergence inequalities,” ieee transactions on information theory , vol. 62, no. 11, pp. 5973– 6006, 2016. s. sun, y. cheng, z. gan, and j. liu, “patient knowledge distillation for bert model compression,” 2019. z. sun, h. yu, x. song, r. liu, y. yang, and d. zhou, “mobilebert: a compact task-agnostic bert for resource-limited devices,” in proceedings of the 58th annual meeting of the association for computational linguistics , d. jurafsky, j. chai, n. schluter, and j. tetreault, eds. online: association for computational linguistics, jul. 2020, pp. 2158–2170. [online]. available: https://aclanthology.org/2020.acl-main.195 x. jiao, y. yin, l. shang, x. jiang, x. chen, l. li, f. wang, and q. liu, “tinybert: distilling bert for natural language understanding,” in findings of the association for computational linguistics: emnlp 2020 , t. cohn, y. he, and y. liu, eds. online: association for computational linguistics, nov. 2020, pp. 4163–4174. [online]. available: https://aclanthology.org/2020.findings-emnlp.372 l. hou, z. huang, l. shang, x. jiang, x. chen, and q. liu, “dynabert: dynamic bert with adaptive width and depth,” advances in neural information processing systems , vol. 33, pp. 9782–9793, 2020. s. zuo, q. zhang, c. liang, p . he, t. zhao, and w. chen, “moebert: from bert to mixture-of-experts via importance- guided adaptation,” arxiv preprint arxiv:2204.07675 , 2022. k. j. liang, w. hao, d. shen, y. zhou, w. chen, c. chen, and l. carin, “mixkd: towards efficient distillation of large- scale language models,” in 9th international conference on learning representations, iclr 2021, virtual event, austria, may 3-7, 2021 . openreview.net, 2021. [online]. available: https://openreview.net/forum?id=ufgeeljklu5 y. j. ma, w. liang, g. wang, d.-a. huang, o. bastani, d. ja- yaraman, y. zhu, l. fan, and a. anandkumar, “eureka: human-level reward design via coding large language models,” 2023. j.-c. pang, p . wang, k. li, x.-h. chen, j. xu, z. zhang, and y. yu, “language model self-improvement by reinforce- ment learning contemplation,” 2023. y. du, o. watkins, z. wang, c. colas, t. darrell, p . abbeel, 37 a. gupta, and j. andreas, “guiding pretraining in reinforcement learning with large language models,” inproceedings of the 40th international conference on machine learning , ser. proceedings of machine learning research, a. krause, e. brunskill, k. cho, b. engelhardt, s. sabato, and j. scarlett, eds., vol. 202. pmlr, 23–29 jul 2023, pp. 8657–8677. [online]. available: https://proceedings.mlr.press/v202/du23f.html j. schulman, f. wolski, p . dhariwal, a. radford, and o. klimov, “proximal policy optimization algorithms,” 2017. r. rafailov, a. sharma, e. mitchell, s. ermon, c. d. man- ning, and c. finn, “direct preference optimization: your language model is secretly a reward model,” 2023. f. song, b. yu, m. li, h. yu, f. huang, y. li, and h. wang, “preference ranking optimization for human alignment,” arxiv preprint arxiv:2306.17492 , 2023. z. yuan, h. yuan, c. tan, w. wang, s. huang, and f. huang, “rrhf: rank responses to align language mod- els with human feedback without tears,” arxiv preprint arxiv:2304.05302 , 2023. m. li, l. chen, j. chen, s. he, and t. zhou, “reflection-tuning: recycling data for better instruction- tuning,” in neurips 2023 workshop on instruction tuning and instruction following , 2023. [online]. available: https://openreview.net/forum?id=xaqozzqkpu m. li, l. chen, j. chen, s. he, j. gu, and t. zhou, “selective reflection-tuning: student-selected data recycling for llm instruction-tuning,” 2024. [online]. available: https: //api.semanticscholar.org/corpusid:267682220 x. geng, a. gudibande, h. liu, e. wallace, p . abbeel, s. levine, and d. song, “koala: a dialogue model for academic research,” blog post, april 2023. [online]. available: https://bair.berkeley.edu/blog/2023/04/03/ koala/ m. li, j. chen, l. chen, and t. zhou, “can llms speak for diverse people? tuning llms via debate to generate controllable controversial statements,” 2024. m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” 2023. r. yang, l. song, y. li, s. zhao, y. ge, x. li, and y. shan, “gpt4tools: teaching large language model to use tools via self-instruction,” 2023. a. yehudai, b. carmeli, y. mass, o. arviv, n. mills, a. toledo, e. shnarch, and l. choshen, “genie: achieving human parity in content-grounded datasets generation,” 2024. y. zhang, r. zhang, j. gu, y. zhou, n. lipka, d. yang, and t. sun, “llavar: enhanced visual instruction tuning for text-rich image understanding,” 2023. c. lyu, m. wu, l. wang, x. huang, b. liu, z. du, s. shi, and z. tu, “macaw-llm: multi-modal language modeling with image, audio, video, and text integration,” arxiv preprint arxiv:2306.09093 , 2023. b. li, y. zhang, l. chen, j. wang, f. pu, j. yang, c. li, and z. liu, “mimic-it: multi-modal in-context instruction tuning,” 2023. z. zhao, l. guo, t. yue, s. chen, s. shao, x. zhu, z. yuan, and j. liu, “chatbridge: bridging modalities with large language model as a language catalyst,” 2023.y. zhao, b. yu, b. hui, h. yu, f. huang, y. li, and n. l. zhang, “a preliminary study of the intrinsic relationship between complexity and alignment,” 2023. a. gudibande, e. wallace, c. snell, x. geng, h. liu, p . abbeel, s. levine, and d. song, “the false promise of imitating proprietary llms,” arxiv preprint arxiv:2305.15717 , 2023. c. zhou, p . liu, p . xu, s. iyer, j. sun, y. mao, x. ma, a. efrat, p . yu, l. yu, s. zhang, g. ghosh, m. lewis, l. zettlemoyer, and o. levy, “lima: less is more for alignment,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview.net/forum?id=kbmokmx2he m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to-strong data filtering for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and z. mao, “expertprompting: instructing large language models to be distinguished experts,” 2023. w. liu, w. zeng, k. he, y. jiang, and j. he, “what makes good data for alignment? a comprehensive study of auto- matic data selection in instruction tuning,” 2023. r. lou, k. zhang, j. xie, y. sun, j. ahn, h. xu, y. su, and w. yin, “muffin: curating multi-faceted instructions for improving instruction-following,” 2023. t. schick, j. dwivedi-yu, z. jiang, f. petroni, p . lewis, g. izacard, q. you, c. nalmpantis, e. grave, and s. riedel, “peer: a collaborative language model,” 2022. a. madaan, n. tandon, p . gupta, s. hallinan, l. gao, s. wiegreffe, u. alon, n. dziri, s. prabhumoye, y. yang, s. gupta, b. p . majumder, k. hermann, s. welleck, a. yaz- danbakhsh, and p . clark, “self-refine: iterative refinement with self-feedback,” 2023. w. saunders, c. yeh, j. wu, s. bills, l. ouyang, j. ward, and j. leike, “self-critiquing models for assisting human evaluators,” 2022. d. m. ziegler, n. stiennon, j. wu, t. b. brown, a. radford, d. amodei, p . christiano, and g. irving, “fine-tuning language models from human preferences,” arxiv preprint arxiv:1909.08593 , 2019. n. stiennon, l. ouyang, j. wu, d. ziegler, r. lowe, c. voss, a. radford, d. amodei, and p . f. christiano, “learning to summarize with human feedback,” advances in neu- ral information processing systems , vol. 33, pp. 3008–3021, 2020. j. wu, l. ouyang, d. m. ziegler, n. stiennon, r. lowe, j. leike, and p . christiano, “recursively summarizing books with human feedback,” 2021. y. bai, a. jones, k. ndousse, a. askell, a. chen, n. das- sarma, d. drain, s. fort, d. ganguli, t. henighan et al. , “training a helpful and harmless assistant with rein- forcement learning from human feedback,” arxiv preprint arxiv:2204.05862 , 2022. a. k ¨opf, y. kilcher, d. von r ¨utte, s. anagnostidis, z.-r. tam, k. stevens, a. barhoum, n. m. duc, o. stanley, r. nagyfi, s. es, s. suri, d. glushkov, a. dantuluri, a. maguire, c. schuhmann, h. nguyen, and a. mattick, “openassis- tant conversations – democratizing large language model alignment,” 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, 38 “openchat: advancing open-source language models with mixed-quality data,” 2023. l. weidinger, j. mellor, m. rauh, c. griffin, j. uesato, p .- s. huang, m. cheng, m. glaese, b. balle, a. kasirzadeh, z. kenton, s. brown, w. hawkins, t. stepleton, c. biles, a. birhane, j. haas, l. rimell, l. a. hendricks, w. isaac, s. legassick, g. irving, and i. gabriel, “ethical and social risks of harm from language models,” 2021. j. ji, m. liu, j. dai, x. pan, c. zhang, c. bian, c. zhang, r. sun, y. wang, and y. yang, “beavertails: towards improved safety alignment of llm via a human-preference dataset,” 2023. i. solaiman and c. dennison, “process for adapting lan- guage models to society (palms) with values-targeted datasets,” advances in neural information processing sys- tems , vol. 34, pp. 5861–5873, 2021. l. qiu, y. zhao, j. li, p . lu, b. peng, j. gao, and s.-c. zhu, “valuenet: a new dataset for human value driven dialogue system,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 11 183– 11 191. j. kiesel, m. alshomary, n. handke, x. cai, h. wachsmuth, and b. stein, “identifying the human values behind arguments,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4459–4471. [online]. available: https://aclanthology.org/2022.acl-long.306 r. liu, g. zhang, x. feng, and s. vosoughi, “aligning generative language models with human values,” in findings of the association for computational linguistics: naacl 2022 , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 241– 252. [online]. available: https://aclanthology.org/2022. findings-naacl.18 a. glaese, n. mcaleese, m. trebacz, j. aslanides, v . firoiu, t. ewalds, m. rauh, l. weidinger, m. chadwick, p . thacker et al. , “improving alignment of dialogue agents via targeted human judgements,” arxiv preprint arxiv:2209.14375 , 2022. h. sun, z. zhang, f. mi, y. wang, w. liu, j. cui, b. wang, q. liu, and m. huang, “moraldial: a framework to train and evaluate moral dialogue systems via moral discussions,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2213–2230. [online]. available: https://aclanthology.org/2023.acl-long.123 j. yao, x. yi, x. wang, j. wang, and x. xie, “from instructions to intrinsic human values – a survey of alignment goals for big models,” 2023. y. liu, y. yao, j.-f. ton, x. zhang, r. g. h. cheng, y. klochkov, m. f. taufiq, and h. li, “trustworthy llms: a survey and guideline for evaluating large language models’ alignment,” arxiv preprint arxiv:2308.05374 , 2023. j. qian, h. wang, z. li, s. li, and x. yan, “limitations of language models in arithmetic and symbolic induction,” 2022.x. she, y. liu, y. zhao, y. he, l. li, c. tantithamthavorn, z. qin, and h. wang, “pitfalls in language models for code intelligence: a taxonomy and survey,” 2023. h. manikandan, y. jiang, and j. z. kolter, “language models are weak learners,” 2023. y. liang, c. wu, t. song, w. wu, y. xia, y. liu, y. ou, s. lu, l. ji, s. mao, y. wang, l. shou, m. gong, and n. duan, “taskmatrix.ai: completing tasks by connecting foundation models with millions of apis,” 2023. g. mialon, r. dess `ı, m. lomeli, c. nalmpantis, r. pa- sunuru, r. raileanu, b. rozi `ere, t. schick, j. dwivedi- yu, a. celikyilmaz, e. grave, y. lecun, and t. scialom, “augmented language models: a survey,” 2023. a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, k. cobbe, t. eloundou, g. krueger, k. button, m. knight, b. chess, and j. schulman, “webgpt: browser-assisted question-answering with human feedback,” 2022. y. qin, z. cai, d. jin, l. yan, s. liang, k. zhu, y. lin, x. han, n. ding, h. wang, r. xie, f. qi, z. liu, m. sun, and j. zhou, “webcpm: interactive web search for chinese long-form question answering,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8968–8988. [online]. available: https://aclanthology.org/2023.acl-long.499 y. song, w. xiong, d. zhu, w. wu, h. qian, m. song, h. huang, c. li, k. wang, r. yao, y. tian, and s. li, “restgpt: connecting large language models with real- world restful apis,” 2023. t. cai, x. wang, t. ma, x. chen, and d. zhou, “large language models as tool makers,” 2023. y. shen, k. song, x. tan, d. li, w. lu, and y. zhuang, “hugginggpt: solving ai tasks with chatgpt and its friends in hugging face,” 2023. s. hao, t. liu, z. wang, and z. hu, “toolkengpt: augment- ing frozen language models with massive tools via tool embeddings,” 2024. s. yuan, k. song, j. chen, x. tan, y. shen, r. kan, d. li, and d. yang, “easytool: enhancing llm-based agents with concise tool instruction,” 2024. s. zhang, s. roller, n. goyal, m. artetxe, m. chen, s. chen, c. dewan, m. diab, x. li, x. v . lin, t. mihaylov, m. ott, s. shleifer, k. shuster, d. simig, p . s. koura, a. sridhar, t. wang, and l. zettlemoyer, “opt: open pre-trained transformer language models,” 2022. t. brown, b. mann, n. ryder, m. subbiah, j. d. ka- plan, p . dhariwal, a. neelakantan, p . shyam, g. sastry, a. askell et al. , “language models are few-shot learners,” advances in neural information processing systems , vol. 33, pp. 1877–1901, 2020. w. huang, p . abbeel, d. pathak, and i. mordatch, “lan- guage models as zero-shot planners: extracting actionable knowledge for embodied agents,” in international confer- ence on machine learning . pmlr, 2022, pp. 9118–9147. i. singh, v . blukis, a. mousavian, a. goyal, d. xu, j. trem- blay, d. fox, j. thomason, and a. garg, “progprompt: 39 generating situated robot task plans using large language models,” 2022. d. zhou, n. sch ¨arli, l. hou, j. wei, n. scales, x. wang, d. schuurmans, c. cui, o. bousquet, q. le, and e. chi, “least-to-most prompting enables complex reasoning in large language models,” 2023. c. h. song, j. wu, c. washington, b. m. sadler, w.-l. chao, and y. su, “llm-planner: few-shot grounded planning for embodied agents with large language models,” in proceed- ings of the ieee/cvf international conference on computer vision , 2023, pp. 2998–3009. z. wang, s. cai, a. liu, x. ma, and y. liang, “describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents,” arxiv preprint arxiv:2302.01560 , 2023. s. yao, d. yu, j. zhao, i. shafran, t. l. griffiths, y. cao, and k. narasimhan, “tree of thoughts: deliberate prob- lem solving with large language models,” arxiv preprint arxiv:2305.10601 , 2023. b. liu, y. jiang, x. zhang, q. liu, s. zhang, j. biswas, and p . stone, “llm+ p: empowering large language mod- els with optimal planning proficiency,” arxiv preprint arxiv:2304.11477 , 2023. s. hao, y. gu, h. ma, j. j. hong, z. wang, d. z. wang, and z. hu, “reasoning with language model is planning with world model,” arxiv preprint arxiv:2305.14992 , 2023. m. hu, y. mu, x. yu, m. ding, s. wu, w. shao, q. chen, b. wang, y. qiao, and p . luo, “tree-planner: efficient close-loop task planning with large language models,” arxiv preprint arxiv:2310.08582 , 2023. b. y. lin, c. huang, q. liu, w. gu, s. sommerer, and x. ren, “on grounded planning for embodied tasks with language models,” in proceedings of the aaai conference on artificial intelligence , vol. 37, no. 11, 2023, pp. 13 192– 13 200. k. valmeekam, m. marquez, s. sreedharan, and s. kambhampati, “on the planning abilities of large language models - a critical investigation,” in thirty-seventh conference on neural informa- tion processing systems , 2023. [online]. available: https://openreview.net/forum?id=x6deqxisew t. sumers, k. marino, a. ahuja, r. fergus, and i. dasgupta, “distilling internet-scale vision-language models into em- bodied agents,” in proceedings of the 40th international conference on machine learning , ser. icml’23. jmlr.org, 2023. y. yang, t. zhou, k. li, d. tao, l. li, l. shen, x. he, j. jiang, and y. shi, “embodied multi-modal agent trained by an llm from a parallel textworld,” 2023. a. vaswani, n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez, ł. kaiser, and i. polosukhin, “attention is all you need,” advances in neural information processing systems , vol. 30, 2017. y. liu, m. ott, n. goyal, j. du, m. joshi, d. chen, o. levy, m. lewis, l. zettlemoyer, and v . stoyanov, “roberta: a robustly optimized bert pretraining approach,” 2019. j. li, l. gui, y. zhou, d. west, c. aloisi, and y. he, “dis- tilling chatgpt for explainable automated student answer assessment,” in emnlp (findings) . association for com- putational linguistics, 2023, pp. 6007–6026. r. tang, x. han, x. jiang, and x. hu, “does syntheticdata generation of llms help clinical text mining?” arxiv preprint arxiv:2303.04360 , 2023. x. he, i. nassar, j. kiros, g. haffari, and m. norouzi, “generate, annotate, and learn: nlp with synthetic text,” trans. assoc. comput. linguistics , vol. 10, pp. 826–842, 2022. [online]. available: https://transacl.org/ojs/index. php/tacl/article/view/3811 y. meng, j. huang, y. zhang, and j. han, “generating training data with language models: towards zero-shot language understanding,” in advances in neural information processing systems 35: annual conference on neural information processing systems 2022, neurips 2022, new orleans, la, usa, november 28 - december 9, 2022 , 2022. [online]. available: http://papers.nips.cc/paper files/ paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms generate high-quality synthetic note-oriented doctor- patient conversations?” in proceedings of the 5th clinical natural language processing workshop , t. naumann, a. ben abacha, s. bethard, k. roberts, and a. rumshisky, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 460–471. [online]. available: https://aclanthology.org/2023.clinicalnlp-1.49 z. yang, s. cherian, and s. vucetic, “data augmentation for radiology report simplification,” in findings of the association for computational linguistics: eacl 2023 , a. vlachos and i. augenstein, eds. dubrovnik, croatia: association for computational linguistics, may 2023, pp. 1922–1932. [online]. available: https: //aclanthology.org/2023.findings-eacl.144 z. cai, c. tao, t. shen, c. xu, x. geng, x. a. lin, l. he, and d. jiang, “hyper: multitask hyper-prompted training en- ables large-scale retrieval generalization,” in the eleventh international conference on learning representations , 2022. c. liu, c. tao, x. geng, t. shen, d. zhao, c. xu, b. jiao, and d. jiang, “adam: dense retrieval distillation with adaptive dark examples,” arxiv preprint arxiv:2212.10192 , 2022. j. feng, c. tao, x. geng, t. shen, c. xu, g. long, d. zhao, and d. jiang, “knowledge refinement via interaction be- tween search engines and large language models,” arxiv preprint arxiv:2305.07402 , 2023. t. shen, g. long, x. geng, c. tao, t. zhou, and d. jiang, “large language models are strong zero-shot retriever,” arxiv preprint arxiv:2304.14233 , 2023. x. ma, x. zhang, r. pradeep, and j. lin, “zero-shot listwise document reranking with a large language model,” 2023. z. qin, r. jagerman, k. hui, h. zhuang, j. wu, j. shen, t. liu, j. liu, d. metzler, x. wang, and m. bendersky, “large language models are effective text rankers with pairwise ranking prompting,” 2023. x. ma, y. gong, p . he, h. zhao, and n. duan, “query rewriting in retrieval-augmented large language models,” inproceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5303–5315. [online]. available: https://aclanthology.org/2023.emnlp-main.322 40 d. sachan, m. lewis, m. joshi, a. aghajanyan, w.- t. yih, j. pineau, and l. zettlemoyer, “improving passage retrieval with zero-shot question generation,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 3781–3797. [online]. available: https://aclanthology.org/2022.emnlp-main.249 d. s. sachan, m. lewis, d. yogatama, l. zettlemoyer, j. pineau, and m. zaheer, “questions are all you need to train a dense passage retriever,” transactions of the association for computational linguistics , vol. 11, pp. 600–616, 2023. [online]. available: https://aclanthology. org/2023.tacl-1.35 t. schick and h. sch ¨utze, “generating datasets with pretrained language models,” in proceedings of the 2021 conference on empirical methods in natural language processing , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. online and punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 6943–6951. [online]. available: https: //aclanthology.org/2021.emnlp-main.555 z. peng, x. wu, and y. fang, “soft prompt tuning for augmenting dense retrieval with large language models,” arxiv preprint arxiv:2307.08303 , 2023. j. saad-falcon, o. khattab, k. santhanam, r. florian, m. franz, s. roukos, a. sil, m. a. sultan, and c. potts, “udapdr: unsupervised domain adaptation via llm prompting and distillation of rerankers,” in proceedings of the 2023 conference on empirical methods in natural language processing, emnlp 2023, singapore, december 6-10, 2023 , 2023, pp. 11 265–11 279. [online]. available: https://aclanthology.org/2023.emnlp-main.693 v . jeronymo, l. bonifacio, h. abonizio, m. fadaee, r. lotufo, j. zavrel, and r. nogueira, “inpars-v2: large language models as efficient dataset generators for infor- mation retrieval,” arxiv preprint arxiv:2301.01820 , 2023. w. sun, z. chen, x. ma, l. yan, s. wang, p . ren, z. chen, d. yin, and z. ren, “instruction distillation makes large language models efficient zero-shot rankers,” 2023. c. raffel, n. shazeer, a. roberts, k. lee, s. narang, m. matena, y. zhou, w. li, and p . j. liu, “exploring the limits of transfer learning with a unified text-to-text transformer,” j. mach. learn. res. , vol. 21, no. 1, jan 2020. s. bruch, x. wang, m. bendersky, and m. najork, “an analysis of the softmax cross entropy loss for learning- to-rank with binary relevance,” in proceedings of the 2019 acm sigir international conference on theory of information retrieval, ictir 2019, santa clara, ca, usa, october 2-5, 2019 , 2019, pp. 75–78. [online]. available: https://doi.org/10.1145/3341981.3344221 c. burges, t. shaked, e. renshaw, a. lazier, m. deeds, n. hamilton, and g. hullender, “learning to rank using gradient descent,” in proceedings of the 22nd international conference on machine learning , ser. icml ’05. new york, ny, usa: association for computing machinery, 2005, p. 89–96. [online]. available: https: //doi.org/10.1145/1102351.1102363 x. wang, c. li, n. golbandi, m. bendersky, and m. najork, “the lambdaloss framework for ranking metricoptimization,” in proceedings of the 27th acm international conference on information and knowledge management , ser. cikm ’18. new york, ny, usa: association for computing machinery, 2018, p. 1313–1322. [online]. available: https://doi.org/10.1145/3269206.3271784 w. wang, x. lin, f. feng, x. he, and t.-s. chua, “generative recommendation: towards next-generation recommender paradigm,” 2023. s. dai, n. shao, h. zhao, w. yu, z. si, c. xu, z. sun, x. zhang, and j. xu, “uncovering chatgpt’s capabilities in recommender systems,” in proceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 y. xi, w. liu, j. lin, x. cai, h. zhu, j. zhu, b. chen, r. tang, w. zhang, r. zhang, and y. yu, “towards open- world recommendation with knowledge augmentation from large language models,” 2023. x. ren, w. wei, l. xia, l. su, s. cheng, j. wang, d. yin, and c. huang, “representation learning with large language models for recommendation,” 2023. w. wei, x. ren, j. tang, q. wang, l. su, s. cheng, j. wang, d. yin, and c. huang, “llmrec: large language models with graph augmentation for recommendation,” 2024. l. wang, s. zhang, y. wang, e.-p . lim, and y. wang, “llm4vis: explainable visualization recommendation using chatgpt,” in proceedings of the 2023 conference on empirical methods in natural language processing: industry track , m. wang and i. zitouni, eds. singapore: association for computational linguistics, dec. 2023, pp. 675–692. [online]. available: https://aclanthology.org/ 2023.emnlp-industry.64 z. cui, j. ma, c. zhou, j. zhou, and h. yang, “m6-rec: generative pretrained language models are open-ended recommender systems,” 2022. p . liu, l. zhang, and j. a. gulla, “pre-train, prompt and recommendation: a comprehensive survey of language modelling paradigm adaptations in recommender sys- tems,” 2023. k. papineni, s. roukos, t. ward, and w.-j. zhu, “bleu: a method for automatic evaluation of machine translation,” inproceedings of the 40th annual meeting on association for computational linguistics , ser. acl ’02. usa: association for computational linguistics, 2002, p. 311–318. [online]. available: https://doi.org/10.3115/1073083.1073135 c.-y. lin, “rouge: a package for automatic evaluation of summaries,” in text summarization branches out . barcelona, spain: association for computational linguistics, jul. 2004, pp. 74–81. [online]. available: https://aclanthology.org/w04-1013 c. su and c. mcmillan, “distilled gpt for source code summarization,” corr , vol. abs/2308.14731, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308. 14731 w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” corr , vol. abs/2312.15692, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.15692 o. sener and s. savarese, “active learning for convolutional neural networks: a core-set approach,” in 6th international 41 conference on learning representations, iclr 2018, vancouver, bc, canada, april 30 - may 3, 2018, conference track proceedings , 2018. [online]. available: https://openreview.net/forum?id=h1aiuk-rw h. liu, c. li, y. li, and y. j. lee, “improved baselines with visual instruction tuning,” 2023. s. zhang, p . sun, s. chen, m. xiao, w. shao, w. zhang, y. liu, k. chen, and p . luo, “gpt4roi: instruction tuning large language model on region-of-interest,” 2023. openai, “gpt-4v(ision) system card,” 2023. [online]. available: https://api.semanticscholar.org/corpusid: 263218031 b. a. plummer, l. wang, c. m. cervantes, j. c. caicedo, j. hockenmaier, and s. lazebnik, “flickr30k entities: collecting region-to-phrase correspondences for richer image-to-sentence models,” in proceedings of the ieee in- ternational conference on computer vision , 2015, pp. 2641– 2649. l. li, z. xie, m. li, s. chen, p . wang, l. chen, y. yang, b. wang, and l. kong, “silkie: preference distilla- tion for large visual language models,” arxiv preprint arxiv:2312.10665 , 2023. h. ha, p . florence, and s. song, “scaling up and distilling down: language-guided robot skill acquisition,” in con- ference on robot learning . pmlr, 2023, pp. 3766–3777. s. wu, z. liu, z. zhang, z. chen, w. deng, w. zhang, j. yang, z. yao, y. lyu, x. xin, s. gao, p . ren, z. ren, and z. chen, “fuzi.mingcha,” https://github.com/irlab- sdu/fuzi.mingcha, 2023. h. xiong, s. wang, y. zhu, z. zhao, y. liu, q. wang, and d. shen, “doctorglm: fine-tuning your chinese doctor is not a herculean task,” arxiv preprint arxiv:2304.01097 , 2023. x. zhang, c. tian, x. yang, l. chen, z. li, and l. r. pet- zold, “alpacare: instruction-tuned large language models for medical application,” arxiv preprint arxiv:2310.14558 , 2023. y. li, z. li, k. zhang, r. dan, s. jiang, and y. zhang, “chatdoctor: a medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge,” cureus , vol. 15, no. 6, 2023. t. han, l. c. adams, j. papaioannou, p . grundmann, t. oberhauser, a. l ¨oser, d. truhn, and k. k. bressem, “medalpaca - an open-source collection of medical conversational ai models and training data,” corr , vol. abs/2304.08247, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.08247 c. wu, w. lin, x. zhang, y. zhang, y. wang, and w. xie, “pmc-llama: towards building open-source language models for medicine,” arxiv preprint arxiv:2305.10415 , vol. 6, 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consultation,” corr , vol. abs/2308.14346, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.14346 z. gou, z. shao, y. gong, yelong shen, y. yang, m. huang, n. duan, and w. chen, “tora: a tool- integrated reasoning agent for mathematical problem solving,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=ep0ttjvoap e. perkowski, r. pan, t. d. nguyen, y. ting, s. kruk, t. zhang, c. o’neill, m. jablonska, z. sun, m. j. smith, h. liu, k. schawinski, k. iyer, i. ciuca, and universetbd, “astrollama-chat: scaling astrollama with conversational and diverse datasets,” corr , vol. abs/2401.01916, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2401.01916 j. gao, r. pi, j. zhang, j. ye, w. zhong, y. wang, l. hong, j. han, h. xu, z. li, and l. kong, “g-llava: solving geometric problem with multi-modal large language model,” corr , vol. abs/2312.11370, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.11370 h. zhao, s. liu, c. ma, h. xu, j. fu, z.-h. deng, l. kong, and q. liu, “gimlet: a unified graph-text model for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv a. n. rubungo, c. arnold, b. p . rand, and a. b. dieng, “llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions,” corr , vol. abs/2310.14029, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.14029 h. cao, z. liu, x. lu, y. yao, and y. li, “instructmol: multi-modal integration for building a versatile and reliable molecular assistant in drug discovery,” corr , vol. abs/2311.16208, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2311.16208 h. abdine, m. chatzianastasis, c. bouyioukos, and m. vazirgiannis, “prot2text: multimodal protein’s function generation with gnns and transform- ers,” in deep generative models for health workshop neurips 2023 , 2023. [online]. available: https://openreview.net/forum?id=ej7yngwyfj y. luo, j. zhang, s. fan, k. yang, y. wu, m. qiao, and z. nie, “biomedgpt: open multimodal generative pre-trained transformer for biomedicine,” arxiv preprint arxiv:2308.09442 , 2023. b. chen, x. cheng, p . li, y. geng, j. gong, s. li, z. bei, x. tan, b. wang, x. zeng, c. liu, a. zeng, y. dong, j. tang, and l. song, “xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein,” corr , vol. abs/2401.06199, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.06199 c. deng, t. zhang, z. he, y. xu, q. chen, y. shi, l. fu, w. zhang, x. wang, c. zhou, z. lin, and j. he, “k2: a foundation language model for geoscience knowledge understanding and utilization,” 2023. z. bi, n. zhang, y. xue, y. ou, d. ji, g. zheng, and h. chen, “oceangpt: a large language model for ocean science tasks,” corr , vol. abs/2310.02031, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.02031 z. zheng, j. zhang, t. vu, s. diao, y. h. w. tim, and s. yeung, “marinegpt: unlocking secrets of ocean to the public,” corr , vol. abs/2310.13596, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.13596 z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: 42 a scientific large language model in geoscience,” corr , vol. abs/2401.00434, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.00434 d. zhang, a. petrova, d. trautmann, and f. schilder, “un- leashing the power of large language models for legal applications,” in proceedings of the 32nd acm international conference on information and knowledge management , 2023, pp. 5257–5258. z. sun, “a short survey of viewing large language models in legal aspect,” arxiv preprint arxiv:2303.09136 , 2023. j. lai, w. gan, j. wu, z. qi, and p . s. yu, “large language models in law: a survey,” arxiv preprint arxiv:2312.03718 , 2023. s. yue, w. chen, s. wang, b. li, c. shen, s. liu, y. zhou, y. xiao, s. yun, w. lin et al. , “disc-lawllm: fine-tuning large language models for intelligent legal services,” arxiv preprint arxiv:2309.11325 , 2023. h. zhong, c. xiao, c. tu, t. zhang, z. liu, and m. sun, “jec-qa: a legal-domain question answering dataset,” in proceedings of the aaai conference on artificial intelligence , vol. 34, no. 05, 2020, pp. 9701–9708. k. singhal, t. tu, j. gottweis, r. sayres, e. wulczyn, l. hou, k. clark, s. pfohl, h. cole-lewis, d. neal, m. schaekermann, a. wang, m. amin, s. lachgar, p . a. mansfield, s. prakash, b. green, e. dominowska, b. a. y arcas, n. tomasev, y. liu, r. wong, c. semturs, s. s. mahdavi, j. k. barral, d. r. webster, g. s. corrado, y. matias, s. azizi, a. karthikesalingam, and v . natarajan, “towards expert-level medical question answering with large language models,” corr , vol. abs/2305.09617, 2023. [online]. available: https://doi. org/10.48550/arxiv.2305.09617 x. yang, j. gao, w. xue, and e. alexandersson, “pllama: an open-source large language model for plant science,” corr , vol. abs/2401.01600, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.01600 x. wang, g. h. chen, d. song, z. zhang, z. chen, q. xiao, f. jiang, j. li, x. wan, b. wang et al. , “cmb: a compre- hensive medical benchmark in chinese,” arxiv preprint arxiv:2308.08833 , 2023. w. zhu, x. wang, h. zheng, m. chen, and b. tang, “promptcblue: a chinese prompt tuning benchmark for the medical domain,” arxiv preprint arxiv:2310.14151 , 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consulta- tion,” arxiv preprint arxiv:2308.14346 , 2023. c. wu, x. zhang, y. zhang, y. wang, and w. xie, “pmc- llama: further finetuning llama on medical papers,” corr , vol. abs/2304.14454, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.14454 s. xue, f. zhou, y. xu, h. zhao, s. xie, q. dai, c. jiang, j. zhang, j. zhou, d. xiu, and h. mei, “weaverbird: empowering financial decision-making with large language model, knowledge base, and search engine,” corr , vol. abs/2308.05361, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.05361 s. wu, o. irsoy, s. lu, v . dabravolski, m. dredze, s. gehrmann, p . kambadur, d. s. rosenberg, and g. mann, “bloomberggpt: a large language modelfor finance,” corr , vol. abs/2303.17564, 2023. [online]. available: https://doi.org/10.48550/arxiv.2303.17564 d. lu, h. wu, j. liang, y. xu, q. he, y. geng, m. han, y. xin, and y. xiao, “bbt-fin: comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark,” corr , vol. abs/2302.09432, 2023. [online]. available: https://doi.org/10.48550/arxiv.2302. 09432 y. yang, y. tang, and k. y. tam, “investlm: a large language model for investment using financial domain instruction tuning,” corr , vol. abs/2309.13064, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.13064 q. xie, w. han, x. zhang, y. lai, m. peng, a. lopez- lira, and j. huang, “pixiu: a large language model, instruction data and evaluation benchmark for finance,” corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction tuning benchmark for open-source large language models in financial datasets,” corr , vol. abs/2310.04793, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310. 04793 r. taylor, m. kardas, g. cucurull, t. scialom, a. hartshorn, e. saravia, a. poulton, v . kerkez, and r. stojnic, “galactica: a large language model for science,” corr , vol. abs/2211.09085, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09085 j. yin, s. dash, f. wang, and m. shankar, “forge: pre-training open foundation models for science,” inproceedings of the international conference for high performance computing, networking, storage and analysis, sc 2023, denver, co, usa, november 12-17, 2023 , d. arnold, r. m. badia, and k. m. mohror, eds. acm, 2023, pp. 81:1–81:13. [online]. available: https: //doi.org/10.1145/3581784.3613215 z. azerbayev, h. schoelkopf, k. paster, m. d. santos, s. mcaleer, a. q. jiang, j. deng, s. biderman, and s. welleck, “llemma: an open language model for mathematics,” corr , vol. abs/2310.10631, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.10631 f. yu, a. gao, and b. wang, “outcome-supervised verifiers for planning in mathematical reasoning,” corr , vol. abs/2311.09724, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09724 t. d. nguyen, y. ting, i. ciuca, c. o’neill, z. sun, m. jablonska, s. kruk, e. perkowski, j. w. miller, j. li, j. peek, k. iyer, t. r ´ozanski, p . khetarpal, s. zaman, d. brodrick, s. j. r. m ´endez, t. bui, a. goodman, a. accomazzi, j. p . naiman, j. cranney, k. schawinski, and universetbd, “astrollama: towards specialized foundation models in astronomy,” corr , vol. abs/2309.06126, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2309.06126 j. roberts, t. l ¨uddecke, s. das, k. han, and s. albanie, “gpt4geo: how a language model sees the world’s ge- ography,” 2023. z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: a scientific large language model in geoscience,” 2023. 43 c. wang, d. engler, x. li, j. hou, d. j. wald, k. jaiswal, and s. xu, “near-real-time earthquake-induced fatality estimation using crowdsourced data and large-language models,” 2023. l. chen, s. li, j. yan, h. wang, k. gunaratna, v . yadav, z. tang, v . srinivasan, t. zhou, h. huang, and h. jin, “alpagasus: training a better alpaca with fewer data,” 2023. y. cao, y. kang, and l. sun, “instruction mining: high- quality instruction data selection for large language mod- els,” 2023. m. li, y. zhang, z. li, j. chen, l. chen, n. cheng, j. wang, t. zhou, and j. xiao, “from quantity to quality: boosting llm performance with self-guided data selection for instruction tuning,” arxiv , vol. abs/2308.12032, 2023. [online]. available: https://api.semanticscholar. org/corpusid:261076515 q. du, c. zong, and j. zhang, “mods: model-oriented data selection for instruction tuning,” 2023. y. li, b. hui, x. xia, j. yang, m. yang, l. zhang, s. si, j. liu, t. liu, f. huang, and y. li, “one shot learning as instruction data prospector for large language models,” 2023. e. frantar, s. p . singh, and d. alistarh, “optimal brain com- pression: a framework for accurate post-training quanti- zation and pruning,” 2023. t. dettmers, m. lewis, y. belkada, and l. zettlemoyer, “gpt3.int8(): 8-bit matrix multiplication for transformers at scale,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https://openreview.net/ forum?id=dxigwqboxad y. j. kim, r. henry, r. fahim, and h. h. awadalla, “finequant: unlocking efficiency with fine-grained weight-only quantization for llms,” 2023. c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre-trained language models via quantization,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4821–4836. [online]. available: https://aclanthology.org/2022.acl- long.331 z. yao, r. yazdani aminabadi, m. zhang, x. wu, c. li, and y. he, “zeroquant: efficient and affordable post-training quantization for large-scale transformers,” advances in neural information processing systems , vol. 35, pp. 27 168– 27 183, 2022. g. xiao, j. lin, m. seznec, h. wu, j. demouth, and s. han, “smoothquant: accurate and efficient post-training quan- tization for large language models,” 2023. x. ma, g. fang, and x. wang, “llm-pruner: on the struc- tural pruning of large language models,” 2023. m. zhang, h. chen, c. shen, z. yang, l. ou, x. yu, and b. zhuang, “loraprune: pruning meets low-rank parameter-efficient fine-tuning,” 2023. e. frantar and d. alistarh, “sparsegpt: massive language models can be accurately pruned in one-shot,” 2023. m. xu, y. l. xu, and d. p . mandic, “tensorgpt: efficient compression of the embedding layer in llms based on thetensor-train decomposition,” 2023. y. li, y. yu, q. zhang, c. liang, p . he, w. chen, and t. zhao, “losparse: structured compression of large lan- guage models based on low-rank and sparse approxima- tion,” 2023. z. hu, l. wang, y. lan, w. xu, e.-p . lim, l. bing, x. xu, s. poria, and r. k.-w. lee, “llm-adapters: an adapter family for parameter-efficient fine-tuning of large lan- guage models,” 2023. h. liu, d. tam, m. mohammed, j. mohta, t. huang, m. bansal, and c. raffel, “few-shot parameter- efficient fine-tuning is better and cheaper than in- context learning,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https: //openreview.net/forum?id=rbcvmg-jspd y. wang, s. agarwal, s. mukherjee, x. liu, j. gao, a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 5744–5760. [online]. available: https://aclanthology.org/2022.emnlp-main.388 e. j. hu, y. shen, p . wallis, z. allen-zhu, y. li, s. wang, l. wang, and w. chen, “lora: low-rank adaptation of large language models,” 2021. x. l. li and p . liang, “prefix-tuning: optimizing continuous prompts for generation,” in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: long papers) , c. zong, f. xia, w. li, and r. navigli, eds. online: association for computational linguistics, aug. 2021, pp. 4582–4597. [online]. available: https://aclanthology.org/2021.acl- long.353 x. liu, k. ji, y. fu, w. tam, z. du, z. yang, and j. tang, “p- tuning: prompt tuning can be comparable to fine-tuning across scales and tasks,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 2: short papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 61–68. [online]. available: https://aclanthology.org/2022.acl-short.8 t. dettmers, a. pagnoni, a. holtzman, and l. zettlemoyer, “qlora: efficient finetuning of quantized llms,” 2023. j. kim, j. h. lee, s. kim, j. park, k. m. yoo, s. j. kwon, and d. lee, “memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization,” 2023. s. malladi, t. gao, e. nichani, a. damian, j. d. lee, d. chen, and s. arora, “fine-tuning language models with just forward passes,” 2024. z. wan, x. wang, c. liu, s. alam, y. zheng, j. liu, z. qu, s. yan, y. zhu, q. zhang, m. chowdhury, and m. zhang, “efficient large language models: a survey,” 2024. y.-s. lee, m. sultan, y. el-kurdi, t. naseem, a. munawar, r. florian, s. roukos, and r. astudillo, “ensemble- instruct: instruction tuning data generation with a heterogeneous mixture of lms,” in findings of the 44 association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 12 561–12 571. [online]. available: https://aclanthology. org/2023.findings-emnlp.836 w. chen, y. zhou, n. du, y. huang, j. laudon, z. chen, and c. cui, “lifelong language pretraining with distribution- specialized experts,” in international conference on machine learning . pmlr, 2023, pp. 5383–5395. s. kotha, j. m. springer, and a. raghunathan, “under- standing catastrophic forgetting in language models via implicit inference,” arxiv preprint arxiv:2309.10105 , 2023. b. koloski, b. ˇskrlj, m. robnik- ˇsikonja, and s. pollak, “mea- suring catastrophic forgetting in cross-lingual transfer paradigms: exploring tuning strategies,” arxiv preprint arxiv:2309.06089 , 2023. t. wu, l. luo, y.-f. li, s. pan, t.-t. vu, and g. haffari, “continual learning for large language models: a sur- vey,” arxiv preprint arxiv:2402.01364 , 2024. y. luo, z. yang, f. meng, y. li, j. zhou, and y. zhang, “an empirical study of catastrophic forgetting in large language models during continual fine-tuning,” arxiv preprint arxiv:2308.08747 , 2023. j. kirkpatrick, r. pascanu, n. rabinowitz, j. veness, g. des- jardins, a. a. rusu, k. milan, j. quan, t. ramalho, a. grabska-barwinska et al. , “overcoming catastrophic forgetting in neural networks,” proceedings of the national academy of sciences , vol. 114, no. 13, pp. 3521–3526, 2017. m. rostami, s. kolouri, and p . k. pilly, “complementary learning for overcoming catastrophic forgetting using ex- perience replay,” arxiv preprint arxiv:1903.04566 , 2019. d. rolnick, a. ahuja, j. schwarz, t. lillicrap, and g. wayne, “experience replay for continual learning,” advances in neural information processing systems , vol. 32, 2019. s.-w. lee, j.-h. kim, j. jun, j.-w. ha, and b.-t. zhang, “overcoming catastrophic forgetting by incremental mo- ment matching,” advances in neural information processing systems , vol. 30, 2017. a. mallya, d. davis, and s. lazebnik, “piggyback: adapting a single network to multiple tasks by learning to mask weights,” in proceedings of the european conference on com- puter vision (eccv) , 2018, pp. 67–82. z. wang, z. zhang, c.-y. lee, h. zhang, r. sun, x. ren, g. su, v . perot, j. dy, and t. pfister, “learning to prompt for continual learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2022, pp. 139–149. z. hu, y. li, j. lyu, d. gao, and n. vasconcelos, “dense network expansion for class incremental learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2023, pp. 11 858–11 867. x. li, l. lin, s. wang, and c. qian, “unlock the power: competitive distillation for multi-modal large language models,” arxiv preprint arxiv:2311.08213 , 2023. m. zeng, w. xue, q. liu, and y. guo, “continual learning with dirichlet generative-based rehearsal,” arxiv preprint arxiv:2309.06917 , 2023. z. zhang, m. fang, l. chen, and m.-r. namazi-rad, “citb: a benchmark for continual instruction tuning,” arxiv preprint arxiv:2310.14510 , 2023. c. burns, p . izmailov, j. h. kirchner, b. baker, l. gao,l. aschenbrenner, y. chen, a. ecoffet, m. joglekar, j. leike, i. sutskever, and j. wu, “weak-to-strong generalization: eliciting strong capabilities with weak supervision,” corr , vol. abs/2312.09390, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.09390 m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to- strong data filtering for fast instruction-tuning,” corr , vol. abs/2402.00530, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.00530 j. ji, b. chen, h. lou, d. hong, b. zhang, x. pan, j. dai, and y. yang, “aligner: achieving efficient alignment through weak-to-strong correction,” corr , vol. abs/2402.02416, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.02416'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_object():\n",
    "    client_val = AzureOpenAI(\n",
    "        api_key=os.getenv('AZURE_OPENAI_KEY'),\n",
    "        api_version='2023-12-01-preview',\n",
    "        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "    )\n",
    "    return client_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completion(client, model, messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3,\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('main_summaries.json', 'r') as f:\n",
    "    file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create an engaging, technical summary of the following AI research paper tailored for ML engineers and AI professionals. Your summary should:\\n\\nOpening:\\n\\nBegin with a powerful statement that captures the paper\\'s most innovative or impactful idea.\\nFollow with a brief explanation of the paper\\'s main contribution to the field of AI/ML and why it matters.\\nUse language that would intrigue technical practitioners and immediately convey the significance of the research.\\n\\n\\nKey Takeaways:\\nPresent 2-3 key takeaways. For each:\\n\\nExplain the main idea in clear terms, avoiding excessive technical jargon.\\nMention its potential impact on current ML practices or future research.\\nEnsure clear linkages between concepts, showing how ideas connect or build upon each other.\\nInclude enough detail to convey the importance and novelty of each takeaway, while maintaining overall brevity.\\n\\nImpactful Quote:\\nInclude one short, impactful quote from the paper. This should be the only instance of using the paper\\'s exact words. Explain its significance succinctly, focusing on its potential implications for AI/ML development.\\nConclusion:\\nHighlight the potential future impact of this research and why it\\'s exciting for the field, drawing on the ideas presented in the takeaways.\\n\\nThroughout the summary:\\n\\nUse your own words to convey the paper\\'s ideas. Do not copy exact sentences or substantial phrases from the paper, except for the single quote in section 3.\\nAim for a tone that balances technical insight with accessibility and enthusiasm.\\nFocus on sparking curiosity and conveying the potential importance of the research.\\nEnsure clear connections between different concepts and ideas presented.\\n                         \\nAim for a tone that balances technical precision with enthusiasm for the research\\'s potential. Focus on aspects that would most interest ML practitioners and researchers, always ensuring clear explanations of how concepts interact or build upon each other. Your goal is to leave technical readers thinking, \"This approach could significantly advance our current methods - I need to explore the full paper.\"\\nKeep the entire summary under 300 words to maintain engagement, remembering that detailed section-by-section summaries will follow.\\nNow, here\\'s the full paper:\\n1 a survey on knowledge distillation of large language models xiaohan xu1, ming li2, chongyang tao3, tao shen4, reynold cheng1, jinyang li1, can xu5, dacheng tao6, tianyi zhou2 1the university of hong kong2university of maryland3microsoft 4university of technology sydney5peking university6the university of sydney {shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk jl0725@connect.hku.hk abstract —in the era of large language models (llms), knowledge distillation (kd) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary llms, such as gpt -4, to their open-source counterparts like llama and mistral. additionally, as open-source llms flourish, kd plays a crucial role in both compressing these models, and facilitating their self- improvement by employing themselves as teachers. this paper presents a comprehensive survey of kd’s role within the realm of llm, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self- improvement. our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization – providing a comprehensive examination of kd mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. crucially, the survey navigates the intricate interplay between data augmentation (da) and kd, illustrating how da emerges as a powerful paradigm within the kd framework to bolster llms’ performance. by leveraging da to generate context- rich, skill-specific training data, kd transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. this work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. by bridging the gap between proprietary and open-source llms, this survey underscores the potential for more accessible, efficient, and powerful ai solutions. most importantly, we firmly advocate for compliance with the legal terms that regulate the use of llms, ensuring ethical and lawful application of kd of llms. an associated github repository is available at https://github.com/tebmer/awesome-knowledge-distillation-of-llms. index terms —large language models, knowledge distillation, data augmentation, skill distillation, supervised fine-tuning ✦ 1 i ntroduction in the evolving landscape of artificial intelligence (ai), proprietary1large language models (llms) such as gpt- 3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude2have emerged as groundbreaking technologies, reshaping our understand- ing of natural language processing (nlp). these models, characterized by their vast scale and complexity, have un- locked new realms of possibility, from generating human- like text to offering sophisticated problem-solving capa- bilities. the core significance of these llms lies in their emergent abilities (wei et al., 2022a,b; xu et al., 2024a), a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. their deep understanding of context, nuance, and the intrica- cies of human language enables them to excel in a wide array of applications, from creative content generation to 1. for simplicity, we use ‘proprietary’ to represent both versatile yet close-source llms like gpt-4 and open-source yet huge llms like llama-2-70b, which encapsulate rich knowledge with a large number of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond current applications, promising to revolutionize industries, augment human creativity, and redefine our interaction with technology. despite the remarkable capabilities of proprietary llms like gpt-4 and gemini, they are not without their shortcom- ings, particularly when viewed in light of the advantages offered by open-source models. a significant drawback is their limited accessibility and higher cost (openai et al., 2023). these proprietary models often come with substantial usage fees and restricted access, making them less attain- able for individuals and smaller organizations. in terms of data privacy and security (wu et al., 2023a), using these proprietary llms frequently entails sending sensitive data to external servers, which raises concerns about data pri- vacy and security. this aspect is especially critical for users handling confidential information. moreover, the general- purpose design of proprietary llms, while powerful, may not always align with the specific needs of niche applica- tions. the constraints of accessibility, cost, and adaptability thus present significant challenges in leveraging the full potential of proprietary llms. in contrast to proprietary llms, open-source modelsarxiv:2402.13116v3 [cs.cl] 8 mar 2024 2 like llama (touvron et al., 2023) and mistral (jiang et al., 2023a) bring several notable advantages. one of the primary benefits of open-source models is their accessibility and adaptability. without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader range of users, from individual re- searchers to smaller organizations. this openness fosters a more collaborative and inclusive ai research environment, encouraging innovation and diverse applications. addition- ally, the customizable nature of open-source llms allows for more tailored solutions, addressing specific needs that generic, large-scale models may not meet. however, the open-source llms also have their own set of drawbacks, primarily stemming from their relatively limited scale and resources compared to their proprietary counterparts. one of the most significant limitations is the smaller model scale, which often results in lower per- formance on real-world tasks with a bunch of instruc- tions (zheng et al., 2023a). these models, with fewer pa- rameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like gpt-4. ad- ditionally, the pre-training investment in these open-source models is typically less substantial. this reduced investment can lead to a narrower range of pre-training data, poten- tially limiting the models’ understanding and handling of diverse or specialized topics (liang et al., 2022; sun et al., 2024a). moreover, open-source models often undergo fewer fine-tuning steps due to resource constraints. fine-tuning is crucial for optimizing a model’s performance for spe- cific tasks or industries, and the lack thereof can hinder the model’s effectiveness in specialized applications. this limitation becomes particularly evident when these models are compared to the highly fine-tuned proprietary llms, which are often tailored to excel in a wide array of complex scenarios (openai et al., 2023). primarily, recognizing the disparities between propri- etary and open-source llms, kd techniques have surged as a means to bridge the performance gap between these models (gou et al., 2021; gupta and agrawal, 2022). knowl- edge distillation, in this context, involves leveraging the more advanced capabilities of leading proprietary models like gpt-4 or gemini as a guiding framework to enhance the competencies of open-source llms. this process is akin to transferring the ‘knowledge’ of a highly skilled teacher to a student, wherein the student (e.g., open-source llm) learns to mimic the performance characteristics of the teacher (e.g., proprietary llm). compared to traditional knowledge distillation algorithms (gou et al., 2021), data augmentation (da) (feng et al., 2021) has emerged as a prevalent paradigm to achieve knowledge distillation of llms, where a small seed of knowledge is used to prompt the llm to generate more data with respect to a specific skill or domain (taori et al., 2023). secondly, kd still retains its fundamental role in compressing llms, making them more efficient without significant loss in performance. (gu et al., 2024; agarwal et al., 2024). more recently, the strategy of employing open-source llms as teachers for their own self-improvement has emerged as a promising approach, enhancing their capabilities significantly (yuan et al., 2024a; chen et al., 2024a). figure 1 provides an illustration of these three key roles played by kd in the context of llms. closed-sourcellmsopen-sourcellmssmallerlmsadvancecompressself-improvement directionofkd ①②③fig. 1: kd plays three key roles in llms: 1) primarily enhancing capabilities, 2) offering traditional compression for efficiency, and 3) an emerging trend of self-improvement via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- struction following (taori et al., 2023)), improved align- ment with user intents (e.g., human values/principles (cui et al., 2023a), and thinking patterns like chain-of-thought (cot) (mukherjee et al., 2023)), and nlp task specialization (e.g., semantic understanding (ding et al., 2023a), and code generation (chaudhary, 2023)). these skills are crucial for the wide array of applications that llms are expected to perform, ranging from casual conversations to com- plex problem-solving in specialized domains. for instance, in vertical domains like healthcare (wang et al., 2023a), law (law, 2023), or science (zhang et al., 2024), where accuracy and context-specific knowledge are paramount, knowledge distillation allows open-source models to sig- nificantly improve their performance by learning from the proprietary models that have been extensively trained and fine-tuned in these areas. the benefits of knowledge distillation in the era of llms are multifaceted and transformative (gu et al., 2024). through a suite of distillation techniques, the gap between proprietary and open-source models is significantly nar- rowed (chiang et al., 2023; xu et al., 2023a) and even filled (zhao et al., 2023a). this process not only streamlines computational requirements but also enhances the environ- mental sustainability of ai operations, as open-source mod- els become more proficient with lesser computational over- head. furthermore, knowledge distillation fosters a more accessible and equitable ai landscape, where smaller enti- ties and individual researchers gain access to state-of-the-art capabilities, encouraging wider participation and diversity in ai advancements. this democratization of technology leads to more robust, versatile, and accessible ai solutions, catalyzing innovation and growth across various industries and research domains. the escalating need for a comprehensive survey on the knowledge distillation of llms stems from the rapidly evolving landscape of ai (openai et al., 2023; team et al., 2023) and the increasing complexity of these models. as ai continues to penetrate various sectors, the ability to effi- ciently and effectively distill knowledge from proprietary llms to open-source ones becomes not just a technical aspiration but a practical necessity. this need is driven by 3 studentmodelllamagptvicunaopt…… seedknowledgesteerdrivegeneratedknowledgedataset demonstrationsrawdatainput set context followingalignmentagentnlp task specializationmulti-modalityskills lawmedical&healthcarefinancesciencemisc.verticaldomains teacherllm gpt-4 claude llama gemini instructions skill domain knowledgeelicitationdistillationalgorithmtraindivergenceandsimilarity feature featureguide reinforcementlearningoutputsreward rm!(·)distill supervisedfine-tuningx,y preferencerankoptimizationy,1y,2y3y1y2y3≻≻rank…… datacuration x,yrawdatasynthesizefeedbackfeedback input outputself-knowledge outputinputinput ylabellabelingexpansion x,ydemonstrationsexpandfeature featureinput,outputextractsec.4sec.5 sec.3.1sec.3.2 fig. 2: an overview of this survey on knowledge distillation of large language models. note that ‘section’ is abbreviated as ‘sec.’ in this figure. rm s(·)denotes the student reward model. the growing demand for more accessible, cost-effective, and adaptable ai solutions that can cater to a diverse range of applications and users. a survey in this field is vital for synthesizing the current methodologies, challenges, and breakthroughs in knowledge distillation. it may serve as a beacon for researchers and practitioners alike, guiding them through the intricate process of distilling complex ai capabilities into more manageable and accessible forms. moreover, such a survey can illuminate the path forward, identifying gaps in current techniques and proposing direc- tions for future research. survey organization. the remainder of this survey is orga- nized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm ofllms. following this intro- duction, §2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of llms and highlighting the role of data augmentation (da) in this context. §3 delves into the approaches to elicit knowledge from teacher llms and core distillation algorithms, examining methods from supervised fine-tuning to more complex strategies involving divergence and similarity, reinforcement learning, and ranking opti- mization. then, §4 focuses on skill distillation, exploring how student models can be enhanced to improve context understanding, alignment with user intentions, and perfor- mance across a variety of nlp tasks. this includes discus- sions on natural language understanding (nlu), genera- tion (nlg), information retrieval, recommendation systems, and the evaluation of text generation. in §5, we ventureinto domain-specific vertical distillation, showcasing how knowledge distillation techniques are applied within spe- cialized fields such as law, healthcare, finance, and science, illustrating the practical implications and transformative impact of these approaches. the survey suggests open problems in §6, identifying current challenges and gaps in knowledge distillation research that offer opportunities for future work. finally, the conclusion and discussion in §7 synthesize the insights gained, reflecting on the implica- tions for the broader ai and nlp research community and proposing directions for future research. figure 2 shows an overview of this survey. 2 o verview 2.1 comparing traditional recipe the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021). this technique is pivotal in mitigating the challenges posed by the computational demands and resource constraints of deploying large-scale models in practical applications. historically, knowledge distillation techniques, prior to the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh et al., 2019; kim and rush, 2016). this process was largely driven by the need to deploy machine learning models in resource-constrained environments, such as mobile devices or edge computing platforms, where the computational 4knowledge distillation of llmskd algorithmsknowledgelabelingannollm (he et al., 2023a), pandalm (wang et al., 2023b), cot-distill (hsieh et al., 2023) orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), baize (xu et al., 2023b), mammoth (yue et al., 2023a), mixed distill (chenglin et al., 2023) expansionself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), code alpaca (chaudhary, 2023) self-align (sun et al., 2024b), wizardlm (xu et al., 2023a), wizardcoder (luo et al., 2023a), wizardmath (luo et al., 2023b), auggpt (dai et al., 2023a), tdg (he et al., 2023b) curationultrachat (ding et al., 2023b), phi-1 (gunasekar et al., 2023), phi-1.5 (li et al., 2023a), phi-2 (mar, 2023), magicoder (wei et al., 2023), wavecoder (yu et al., 2024) zerogen (ye et al., 2022), sungen (gao et al., 2023a), inpars (bonifacio et al., 2022) featurebabyllama (timiryasov and tastet, 2023), minillm (gu et al., 2024), gkd (agarwal et al., 2024), quantgpt (tao et al., 2022a), llm-qat (liu et al., 2023a), feedbackcai (bai et al., 2022a), wizardmath (luo et al., 2023b), ultrafeedback (cui et al., 2023a), zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), rlaif (lee et al., 2023a), lion (jiang et al., 2023b), persd (chen et al., 2023a), gkd (agarwal et al., 2024) self-knowledgeself-instruct (wang et al., 2022a), self-align (sun et al., 2024b), rlcd (yang et al., 2024a), impdistill (jung et al., 2023), lmsi (huang et al., 2023a), rest (gulcehre et al., 2023), self-rewarding (yuan et al., 2024a), baize (xu et al., 2023b), star (zelikman et al., 2022) distillationsupervised fine-tuningalpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), self-instruct (wang et al., 2022a), baize (xu et al., 2023b), star (zelikman et al., 2022), divergence and similaritydistilgpt (sanh et al., 2019), f-distill (wen et al., 2023), minillm (gu et al., 2024) ted (liang et al., 2023a), gkd (agarwal et al., 2024),babyllama(timiryasov and tastet, 2023) reinforcement learningcai (bai et al., 2022a), ultrafeedback (cui et al., 2023a), wizardmath (luo et al., 2023b), minillm (gu et al., 2024), gkd (agarwal et al., 2024), gpt3 reward (kwon et al., 2023) rank optimization zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), skill distillationcontext followinginstruction followingself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), wizardmath (luo et al., 2023b), llama-gpt4 (peng et al., 2023a), multi-turn dialoguevicuna (chiang et al., 2023), baize (xu et al., 2023b), ultrallama (ding et al., 2023b), camel (li et al., 2023b), openchat (wang et al., 2023c), zephyr (tunstall et al., 2023), rag capbility kard (kang et al., 2023a), sail (luo et al., 2023c), self-rag (asai et al., 2023), alignmentthinking patternselfee (ye et al., 2023), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), aft (wang et al., 2023d), adaptllm (cheng et al., 2023), knowpat (zhang et al., 2023a), preferencecai (bai et al., 2022a), gpt-3 reward (kwon et al., 2023), ilf (scheurer et al., 2023), almost (kim et al., 2023a), rlef (roit et al., 2023), rlaif (lee et al., 2023a), zephy (tunstall et al., 2023), ultrafeedback (cui et al., 2023a), valuecai (bai et al., 2022a), align honesty (yang et al., 2023a), sandbox (liu et al., 2023b), self-align (sun et al., 2024b), ultrafeedback (cui et al., 2023a), rlcd (yang et al., 2024a) agenttool usingtoolformer (schick et al., 2023), graph-toolformer (zhang, 2023), gorilla (patil et al., 2023), toolalpaca (tang et al., 2023a), toolllm (qin et al., 2023a), craft (yuan et al., 2023a), confucius (gao et al., 2023b), mllm-tool (wang et al., 2024), α-umi (shen et al., 2024), planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task specializationnluauggpt (dai et al., 2023a), gpt annotation (gilardi et al., 2023), (ding et al., 2023a), tdg (he et al., 2023b), sungen (gao et al., 2023a), mix distill (chenglin et al., 2023), annollm (he et al., 2023a), udg (wang et al., 2021a), zerogen (ye et al., 2022), nlginheritsumm (xu et al., 2023c), recomp (xu et al., 2024b), mario (ramnath et al., 2023), id (jung et al., 2023), gpt-3 labeling (wang et al., 2021b), biogpt (guo et al., 2023a), chatgpt nmt (yang and nicolai, 2023), information retrievalquill (srinivasan et al., 2022), promptgator (dai et al., 2023b), inpars (bonifacio et al., 2022), augtriever (meng et al., 2023), (sun et al., 2023a), rankvicuna (pradeep et al., 2023a), rankzephyr (pradeep et al., 2023b), exaranker (ferraretto et al., 2023), recommendation ndr (mysore et al., 2023), instrcutrec (zhang et al., 2023b), once (liu et al., 2023c), text generation evaluationpandalm (wang et al., 2023b), prometheus (kim et al., 2024), instructscore (xu et al., 2023d), tigerscore (jiang et al., 2023c), auto-j (li et al., 2024a), codecodealpaca (chaudhary, 2023), codellama (rozi `ere et al., 2023), magicoder (wei et al., 2023) phi-1 (gunasekar et al., 2023), persd (chen et al., 2023a), mftcoder (liu et al., 2023d), wavecoder (yu et al., 2024), code clean (jain et al., 2023), multi-modalityllava (liu et al., 2023e), svit (zhao et al., 2023b), lvis-instruct4v (wang et al., 2023e), shikra (chen et al., 2023c), lskd (park et al., 2023), detgpt (pi et al., 2023; zhao et al., 2023c), lrv (liu et al., 2023f), next-gpt (wu et al., 2023b), valley (luo et al., 2023d), iluvui (jiang et al., 2023d), stablellava (li et al., 2023c), pointllm (xu et al., 2023e), verticalization distillationlaw (huang et al., 2023b; cui et al., 2023b); medical & healthcare (zhang et al., 2023c; chen et al., 2023d); finance (zhang and yang, 2023); science (xie et al., 2023a; zhang et al., 2024) and misc. (dan et al., 2023; guo et al., 2023b) fig. 3: taxonomy of knowledge distillation of large language models. the detailed taxonomy of verticalization distillation is shown in figure 7. 5 power and memory are limited. the focus was predomi- nantly on ad-hoc neural architecture selection and training objectives tailored for single tasks. these earlier methods involved training a smaller student network to mimic the output of a larger teacher network, often through techniques like soft target training, where the student learns from the softened softmax output of the teacher. please refer to the survey (gou et al., 2021) for more details on general knowledge distillation techniques in ai and dl. in contrast, the advent of llms has revolutionized the knowledge distillation landscape. the current era of knowledge distillation in llms shifts the focus from mere architecture compression to the more nuanced process of knowledge elicitation and transfer (taori et al., 2023; chaud- hary, 2023; tunstall et al., 2023). this paradigm change is largely due to the expansive and deep-seated knowledge that llms like gpt-4 and gemini possess. and the inacces- sible parameters of llms make it hard to compress them by using pruning (han et al., 2016) or quantization (liu et al., 2023a) techniques. unlike the earlier era, where the goal was to replicate the output behavior of the teacher model or reduce the model size , the current focus in llm-based knowledge distillation is to extract and transfer the rich, nuanced understanding that these models have developed. the key to this modern approach lies in heuristic and carefully designed prompts, which are used to elicit specific knowledge (ding et al., 2023b) or capabilities (chaudhary, 2023) from the llms. these prompts are crafted to tap into the llm’s understanding and capabilities in various domains, ranging from natural language understanding (he et al., 2023a) to more complex cognitive tasks like reason- ing (hsieh et al., 2023) and problem-solving (qiao et al., 2024). the use of prompts as a means of knowledge elici- tation offers a more flexible and dynamic approach to dis- tillation. it allows for a more targeted extraction of knowl- edge, focusing on specific skills or domains of interest. this method is particularly effective in harnessing the emergent abilities of llms, where the models exhibit capabilities beyond their explicit training objectives. furthermore, this era of knowledge distillation also em- phasizes the transfer of more abstract qualities such as reasoning patterns (mitra et al., 2023), preference align- ment (cui et al., 2023a), and value alignment (sun et al., 2024b). this is in stark contrast to the earlier focus on output replication (taori et al., 2023), indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. the current techniques involve not just the replication of outputs, but also the emulation of the thought processes (mitra et al., 2023) and decision-making (asai et al., 2023) patterns of the teacher model. this involves complex strategies like chain-of-thought prompting, where the student model is trained to learn the reasoning process of the teacher, thereby enhancing its problem-solving and decision-making capabilities. 2.2 relation to data augmentation (da) in the era of llms, data augmentation (da) (wang et al., 2022a; ye et al., 2022) emerges as a critical paradigm integral to the process of knowledge distillation. unlike traditional da techniques such as paraphrasing (gangal et al., 2022) orback-translation (longpre et al., 2019), which primarily aim at expanding the training dataset in a somewhat mechanical manner. da within the context of llms focuses on the generation of novel, context-rich training data tailored to specific domains and skills. this innovation is driven by the unique capabilities of llms to generate coherent, diverse, and intricate data samples that closely mimic the nuanced understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed knowledge, kd employs da to prompt llms to produce explicit data that encapsulates specific skills or domain expertise (chaudhary, 2023; west et al., 2022). this method stands out as a potent mechanism for bridging the knowl- edge and capability gap between proprietary and open- source models. through da, llms are prompted to create targeted, high-quality datasets that are not merely larger in volume but are also rich in diversity and specificity. this approach enables the distillation process to be more effec- tive, ensuring that the distilled models not only replicate the teacher model’s output behavior but also embody its deep-seated understanding and cognitive strategies. the significance and necessity of da for achieving kd in the llm era cannot be overstated. da acts as a force multiplier, enabling the distilled models to acquire and re- fine capabilities that would otherwise require exponentially larger datasets and computational resources. it facilitates a more nuanced and effective transfer of knowledge, fo- cusing on the qualitative aspects of learning rather than quantitative expansion. this strategic use of da within kd processes underscores a pivotal shift towards a more efficient, sustainable, and accessible approach to harnessing the power of llms. it empowers open-source models with the ability to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts, thereby democratizing access to advanced ai capabilities and fostering innovation across a broader spectrum of applications and users. 2.3 survey scope building on the discussions introduced earlier, this survey aims to comprehensively explore the landscape of knowl- edge distillation within the context of llms, following a meticulously structured taxonomy as in figure 3. the survey’s scope is delineated through three primary facets: kd algorithms, skill distillation, and verticalization dis- tillation. each facet encapsulates a range of subtopics and methodologies. it’s important to note that kd algorithms provide the technical foundations for skill distillation and verticalization distillation. kd algorithms. this segment focuses on the technical foundations and methodologies of knowledge distillation. it includes an in-depth exploration of the processes involved in constructing knowledge from teacher models (e.g., pro- prietary llms) and integrating this knowledge into student models (e.g., open-source llms). under the umbrella of ‘knowledge ’, we delve into strategies such as labeling (hsieh et al., 2023), expansion (taori et al., 2023), curation (gu- nasekar et al., 2023), feature understanding (agarwal et al., 6 2024), feedback mechanisms (tunstall et al., 2023), and self- knowledge generation (wang et al., 2022a). this exploration seeks to uncover the various ways in which knowledge can be identified, expanded, and curated for effective dis- tillation. the ‘ distillation ’ subsection examines learning ap- proaches like supervised fine-tuning (sft) (wang et al., 2022a), divergence minimization (agarwal et al., 2024), rein- forcement learning techniques (cui et al., 2023a), and rank optimization strategies (tunstall et al., 2023). this analysis aims to illuminate how these algorithms facilitate the trans- fer of knowledge, ensuring that open-source models can replicate and, in some cases, surpass the capabilities of their proprietary counterparts. skill distillation. this facet examines the specific compe- tencies and capabilities enhanced through kd. it encom- passes detailed discussions on context following (taori et al., 2023; luo et al., 2023c), with subtopics like instruction following and retrieval-augmented generation (rag) capa- bility. in the realm of alignment (mitra et al., 2023; tun- stall et al., 2023), the survey investigates thinking patterns, persona/preference modeling, and value alignment. the ‘agent’ category delves into skills such as tool using and planning. nlp task specialization (dai et al., 2023a; jung et al., 2023; chaudhary, 2023) is scrutinized through lenses like natural language understanding (nlu), natural lan- guage generation (nlg), information retrieval, recommen- dation systems, text generation evaluation, and code gen- eration. finally, the survey addresses multi-modality (liu et al., 2023e; zhao et al., 2023b), exploring how kd enhances llms’ ability to interpret and integrate multiple forms of input, enriching their utility and applicability across various contexts. verticalization distillation. this section assesses the ap- plication of kd across diverse vertical domains, offering insights into how distilled llms can be tailored for spe- cialized fields such as law (law, 2023), medical & health- care (wang et al., 2023a), finance (zhang and yang, 2023), science (zhang et al., 2024), among others. this exploration not only showcases the practical implications of kd tech- niques but also highlights their transformative impact on domain-specific ai solutions. through detailed analysis and examples, this part aims to demonstrate the versatility and efficacy of kd in adapting llms to meet the nuanced demands of different industries, thus contributing to the broader ai and ml ecosystem. by navigating through these facets, this survey en- deavors to provide an extensive and nuanced analysis of knowledge distillation in the era of llms. it serves as a guide for researchers, practitioners, and enthusiasts in the field, shedding light on current methodologies, challenges, and opportunities for innovation in this rapidly evolving domain. declaration. this survey represents our earnest effort to provide a comprehensive and insightful overview of knowl- edge distillation techniques applied to llms, focusing on algorithms, skill enhancement, and domain-specific appli- cations. given the vast and rapidly evolving nature of this field, especially with the prevalent practice of elic- iting knowledge from training data across academia, weacknowledge that this manuscript may not encompass every pertinent study or development. nonetheless, it endeavors to introduce the foundational paradigms of knowledge dis- tillation, highlighting key methodologies and their impacts across a range of applications. 2.4 distillation pipeline in llm era seedknowledgeskill/domain teacherllmknowledgeelicitationstudentmodeldistillationalgorithmsteer drivegeneratedknowledgelearningobjectivetrain fig. 4: an illustration of a general pipeline to distill knowl- edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge from a sophisticated teacher model to a less complex student model. this pipeline is integral for leveraging the advanced capabilities of models like gpt-4 or gemini in more acces- sible and efficient open-source counterparts. the outline of this pipeline can be broadly categorized into four distinct stages, each playing a crucial role in the successful distilla- tion of knowledge. an illustration is shown in figure 4. the detailed pipeline could also be seen in figure 2. i. target skill or domain steering teacher llm. the first stage involves directing the teacher llm towards a specific target skill or domain. this is achieved through care- fully crafted instructions or templates that guide the llm’s focus. these instructions are designed to elicit responses that demonstrate the llm’s proficiency in a particular area, be it a specialized domain like healthcare or law, or a skill such as reasoning or language understanding. the objective here is to utilize the teacher llm’s extensive training and nuanced capabilities to generate outputs that are rich in the specific knowledge or skills desired for the student model. ii. seed knowledge as input. once the target area is defined, the next step is to feed the teacher llm with seed knowledge. this seed knowledge typically comprises a small dataset or specific data clues relevant to the elicit skill or domain knowledge from the teacher llm. it acts as a catalyst, prompting the teacher llm to generate more elaborate and detailed outputs based on this initial infor- mation. the seed knowledge is crucial as it provides a foundation upon which the teacher model can build and expand, thereby creating more comprehensive and in-depth knowledge examples. iii. generation of distillation knowledge. in response to the seed knowledge and steering instructions, the teacher llm generates knowledge examples. these examples are predominantly in the form of question-and-answer (qa) dialogues or narrative explanations, aligning with the nat- ural language processing/understanding capabilities of the 7 llm. in certain specialized cases, the outputs may also in- clude logits or hidden features, although this is less common due to the complexity and specific requirements of such data forms. the generated knowledge examples constitute the core of the distillation knowledge, encapsulating the advanced understanding and skills of the teacher llm. iv . training the student model with a specific learn- ing objective. the final stage involves the utilization of the generated knowledge examples to train the student model. this training is guided by a loss function that aligns with the learning objectives. the loss function quantifies the student model’s performance in replicating or adapting the knowledge from the teacher model. by minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. the process involves iteratively adjusting the student model’s parameters to reduce the discrepancy be- tween its outputs and those of the teacher model, ensuring the effective transfer of knowledge. in essential, the above four stages can be abstracted as two formulations. the first formulation represents the process of eliciting knowledge: d(kd) i={parse( o, s)|o∼pt(o|i⊕s),∀s∼ s} , (1) where ⊕denotes fusing two pieces of text, idenotes an instruction or a template for a task, skill, or domain to steer the llm and elicit knowledge, s∼ s denotes an example of the seed knowledge, upon which the llm can explore to generate novel knowledge, parse( o, s)stands for to parse the distillation example ( e.g., (x, y)) from the teacher llm’s output o(plus the input sin some cases), andptrepresents the teacher llm with parameters θt. given the datasets d(kd) ibuilt for distillation, we then define a learning objective as l=x ili(d(kd) i;θs), (2) wherep idenotes there could be multiple tasks or skills being distilled into one student model, li(·;·)stands for a specific learning objective, and θsparameterizes the student model. following our exploration of the distillation pipeline and the foundational concepts underlying knowledge distilla- tion in the llm era, we now turn our focus to the specific algorithms that have gained prominence in this era. 3 k nowledge distillation algorithms this section navigates through the process of knowledge distillation. according to section 2.4, it is categorized into two principal steps: ‘knowledge,’ focusing on eliciting knowledge from teacher llms (eq.1), and ‘distillation,’ centered on injecting this knowledge into student models (eq.2). we will elaborate on these two processes in the subsequent sections. 3.1 knowledge this section focuses on the approaches to elicit knowledge from teacher llms. according to the manners to acquire knowledge, we divided them into labeling ,expansion ,datacuration ,feature ,feedback , and self-knowledge . figure 5 shows an illustration of these knowledge elicitation meth- ods. 3.1.1 labeling labeling knowledge refers to using a teacher llm to label the output yfor a given input xas the seed knowledge, according to the instruction ior demonstrations c, where c= (x1, y1), . . . , (xn, yn). this method of eliciting knowl- edge from teacher llms is straightforward yet effective and has been widely applied across various tasks and appli- cations. it requires only the collection of an input dataset and feeding it into llms to obtain the desired generations. moreover, the generation of yis controllable through the predefined iandc. this process can be formulated as follows: d(lab)={x, y|x∼ x, y∼pt(y|i⊕c⊕x)}. (3) input xcould be sourced from existing nlp task datasets, which serve as typical reservoirs for distillation efforts. numerous works have sought to harness the capa- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- egorize text (gilardi et al., 2023; ding et al., 2023a; he et al., 2023a), while in natural language generation, llms assist in generating sequences for outputs (hsieh et al., 2023; jung et al., 2023; wang et al., 2021b). text generation evaluation tasks leverage llms to label evaluated results (li et al., 2024b; wang et al., 2023b), and reasoning tasks utilize llms for labeling chains of thought (cot) explanations (hsieh et al., 2023; li et al., 2022; ho et al., 2023; magister et al., 2023; fu et al., 2023; ramnath et al., 2023; li et al., 2023d; liu et al., 2023g), among others. rather than concentrating on specific tasks, many current works focus on labeling outputs based on instructions, thereby teaching student models to solve tasks in a more flexible way by following in- structions. collections of various nlp tasks, complemented by instructional templates, serve as valuable input sources forx. for instance, flan-v2 collections (longpre et al., 2023) offers extensive publicly available sets of tasks with instructions, which are labeled with responses generated by teacher llms in orca (mukherjee et al., 2023; mitra et al., 2023). the instructions from these nlp tasks are built from predefined templates, which lack diversity and may have gaps between human’s natural query. the real conversations between humans and chat models provide large-scale data with real queries and generations labeled by powerful llms, like sharegpt. additionally, xu et al. (2023b) and anand et al. (2023) label the real questions sampled from forums like quora and stack overflow. moreover, the process of labeling could be guided by instructions ior demonstrations c. a commonly used in- struction type for guiding labeling is chain-of-thought (cot) prompt (hsieh et al., 2023; fu et al., 2023; magister et al., 2023). mukherjee et al. (2023) add multiple system messages (e.g. “you must generate a detailed and long answer.” or “explain like i’m five, think step-by-step”) to elicit rich signals. yue et al. (2023a) and chenglin et al. (2023) la- bel a hybrid of knowledge of chain-of-thought (cot) and 8 𝑐𝐼labelingexpansion𝑥𝐼𝑦𝑥𝑦expandcompleteupdatedata curation 𝑚meta sources 𝐼𝑥𝑦 input set completecreatesamplegenerate 𝑚meta-information𝑐demonstrations𝑥𝐼 𝑦 filterfeedback extractfeature𝑥𝑦 distributionintermediatefeature 𝑥input𝑦output𝐼instruction𝑦! 𝑦\" 𝑦# 𝑥guidefeedback𝑦#∗ 𝑦# feedback self-knowledge studentteacher generate≻≻𝑦\" 𝑦! 𝑦# 𝑥 𝑥& correctexpand𝑐 fig. 5: an illustration of different knowledge elicitation methods from teacher llms. labeling : the teacher generates the output from the input; expansion : the teacher generates samples similar to the given demonstrations through in- context learning; data curation : the teacher synthesizes data according to meta-information, such as a topic or an entity; feature : feed the data into the teacher and extract its internal knowledge, such as logits and features; feedback : the teacher provides feedback on the student’s generations, such as preferences, corrections, expansions of challenging samples, etc; self-knowledge : the student first generates outputs, which is then filtered for high quality or evaluated by the student itself. program-of-thought (pot) rationales. xu et al. (2023b) pro- pose a self-chat technique that two teacher llms simulate the real conversational to generate multi-turn dialogues for a question from quora and stack overflow. 3.1.2 expansion while the labeling approach is simple and effective, it faces certain limitations. primarily, it is constrained by the scale and variety of the input data. in real-world applications, especially those involving user conversations, there are also concerns regarding the privacy of the data involved. to address these limitations, various expansion methods have been proposed (wang et al., 2022a; taori et al., 2023; chaud- hary, 2023; si et al., 2023; ji et al., 2023a; luo et al., 2023b,a; wu et al., 2023c; sun et al., 2024b; xu et al., 2023a; guo et al., 2023c; rozi `ere et al., 2023; west et al., 2022). these methods take the demonstrations as seed knowledge and aim to expand a large scale and various data by in-context learning. a key characteristic of these expansion methods is the utilization of the in-context learning ability of llms to gen- erate data similar to the provided demonstrations c. unlike in the labeling approach, where the input xis sampled from the existing dataset, in the expansion approach, both x andyare generated by teacher llms. this process can be formulated as follows: d(exp)={(x, y)|x∼pt(x|i⊕c), y∼pt(y|i⊕x)}.(4) in this formulation, xand yrepresent the new input- output pairs generated by the teacher llm. the input x is generated based on a set of input-output demonstrations c. the output yis then generated in response to the new input xunder the guidance of an instruction i. note thatthe demonstrations could be predefined or dynamically updated by adding the newly generated samples. expansion techniques have been widely utilized to extract extensive instruction-following knowledge from teacher llms. wang et al. (2022a) first introduces an iter- ative bootstrapping method, self-instruct, to utilize llms to generate a wide array of instructions based on sev- eral demonstrations sampled from 175 manually-written in- structions. the newly generated instructions are then added back to the initial pool, benefiting subsequent expansion iterations. subsequently, taori et al. (2023) applies this ex- pansion method to a more powerful teacher llm, text- davinci-003, to distill 52k high-quality data. to improve the diversity and coverage during expansion, wu et al. (2023c) and (sun et al., 2024b) prompt the teacher llm to generate instructions corresponding to some specific topics. xu et al. (2023a) propose an evol-instruct method to ex- pand the instructions from two dimensions: difficulty (e.g. rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to expand the distillation of coding (luo et al., 2023a) and math (luo et al., 2023b). additionally, expansion methods can significantly augment nlp task datasets with similar samples, thereby enhancing task performance. for instance, auggpt (dai et al., 2023a) leverages a teacher llm to rephrase each sentence in the training samples into multi- ple conceptually similar, but semantically varied, samples to improve classification performance. similarly, tdg (he et al., 2023b) proposes the targeted data generation (tdg) framework, which automatically identifies challenging sub- groups within data and generates new samples for these subgroups using llms through in-context learning. in summary, the expansion method leverages the in- 9 context learning strengths of llms to produce more var- ied and extensive datasets with both inputs and outputs. however, the quality and diversity of the generated data are heavily reliant on the teacher llms and the initial seed demonstrations. this dependence can lead to a dataset with inherent bias from llms (yu et al., 2023a; wei et al., 2023) and a homogeneity issue where the generations may be prone to similarity ultimately, limiting the diversity this method seeks to achieve (ding et al., 2023b). moreover, the expansion process may inadvertently amplify any biases present in the seed data. 3.1.3 data curation the pursuit of high-quality and scalable data generation in knowledge distillation from llms has led to the emergence of the data curation approach. this method arises in re- sponse to the limitations observed in both the labeling and expansion approaches. these methods often yield data of variable quality and face constraints in quantity. in labeling, the seed knowledge is sourced from task datasets, leading to potential noise and dirty data. meanwhile, in expansion, the input xis derived from seed demonstrations, which can result in homogeneous data when generated in large quantities. to overcome these challenges, the data curation method curates high-quality or large-scale data by extensive meta-information as seed knowledge (ding et al., 2023b; gunasekar et al., 2023; li et al., 2023a; mar, 2023; liu et al., 2023d; wei et al., 2023; yu et al., 2024; ye et al., 2022; gao et al., 2023a; yang and nicolai, 2023). a distinct feature of data curation is its approach to synthesize data from scratch. numerous diverse meta- information, such as topics or knowledge points, could be incorporated into this process to generate controllable x andy. thus, this process can be meticulously controlled to yield datasets that are not only large in scale but also of high quality. the formulation for data curation can be represented as: d(cur)={(x, y)|x∼pt(x|i⊕m), y∼pt(y|i⊕x)}.(5) in this formulation, mrepresents the diverse meta- information used to guide the synthesis of x, and iis the instruction guiding teacher llms to generate xory. different studies primarily vary in their source and method of leveraging meta-information. ultrachat (ding et al., 2023b) effectively demonstrates the process of curating both high-quality and diverse data by distilled knowledge. they collect extensive meta-information across three do- mains: questions about the world, creation and generation , and assistance on existing materials . for example, under questions about the world , they explore 30 meta-topics like ”technology” and ”food and drink.” the teacher llms then use this meta-information to distill a broad array of instructions and conversations, achieving a substantial scale of 1.5 million instances. ultrachat stands out with its lexical and topical diversity. the ultrallama model, fine- tuned on this data, consistently surpasses other open-source models. another notable series, phi(gunasekar et al., 2023; li et al., 2023a; mar, 2023), focuses on distilling smaller, high-quality datasets akin to ”textbooks.” phi-1 (gunasekar et al., 2023) experiments with synthesizing ”textbook qual- ity” data in the coding domain. their approach involvesdistilling clear, self-contained, instructive, and balanced con- tent from llms, guided by random topics or function names to enhance diversity. the distilled data is a synthesis of 1 billion tokens of python textbooks, complete with natural language explanations and code snippets, as well as 180 mil- lion tokens of python exercises with solutions. remarkably, thephi-1 model, despite its smaller size, outperforms nearly all open-source models on coding benchmarks like hu- maneval and mbpp while being 10 times smaller in model size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in contrast, magicoder (wei et al., 2023) and wavecoder (yu et al., 2024) get raw code collections from open-source code datasets, using this as meta-information for generating instructional data. in the context of nlu tasks, certain studies (ye et al., 2022; gao et al., 2023a; wang et al., 2021a) explore the use of labels as meta-information to synthesize corresponding samples for data augmentation. similarly, in information retrieval tasks, there are efforts to utilize docu- ments as meta-information for generating potential queries, thereby constructing large-scale retrieval pairs (bonifacio et al., 2022; meng et al., 2023). in conclusion, data curation through teacher llms has emerged as a promising technique for synthesizing datasets that are not only high-quality and diverse but also large in scale. the success of models like phi-1 in specialized domains underscores the efficacy of this method. the ability to create synthetic datasets will become a crucial technical skill and a key area of focus in ai (li et al., 2023a). 3.1.4 feature the previously discussed knowledge elicitation methods are typically applied to powerful black-box models, which are expensive and somewhat unreproducible due to calling api. in contrast, white-box distillation offers a more trans- parent and accessible approach for researchers. it involves leveraging the output distributions, intermediate features, or activations from teacher llms, which we collectively refer to as feature knowledge. white-box kd approaches have predominantly been studied for smaller encoder-based lms, typically those with fewer than 1 billion parameters (cf. gou et al. (2021) for detail). however, recent research has begun to explore white-box distillation in the context of generative llms (timiryasov and tastet, 2023; liang et al., 2023a; gu et al., 2024; agarwal et al., 2024; liu et al., 2023a; wen et al., 2023; wan et al., 2024a; zhao and zhu, 2023; qin et al., 2023b; boizard et al., 2024; zhong et al., 2024). the typical method for acquiring this feature knowledge involves teacher llms annotating the output sequence y with its internal representations. these annotations are then distilled into the student model using methods such as kullback-leibler divergence (kld). the process of eliciting feature knowledge can be formulated as follows: d(feat)={(x, y, ϕ feat(x, y;θt))|x∼ x, y∼ y} . (6) in this formulation, yis the output set, which can be generated by teacher llms, the student model, or directly sourced from the dataset. ϕfeat(·;θt)represents the opera- tion of extracting feature knowledge (such as output distri- bution) from the teacher llm. 10 the most straightforward method to elicit feature knowl- edge of teacher is to label a fixed dataset of sequences with token-level probability distributions (sanh et al., 2019; wen et al., 2023). to leverage the rich semantic and syntactic knowledge in intermediate layers of the teacher model, ted (liang et al., 2023a) designs task-aware layer-wise distillation. they align the student’s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task. gu et al. (2024) and agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, termed ‘self- generated sequences.’ the student then learns by using feedback (i.e. output distribution) from teacher on these sequences. this method is particularly beneficial when the student model lacks the capacity to mimic teacher’s distri- bution. moreover, various llm-quantization methods with distilling feature knowledge from teacher llms have been proposed (tao et al., 2022a; liu et al., 2023a; kim et al., 2023b). these methods aim to preserve the original output distribution when quantizing the llms, ensuring minimal loss of performance. additionally, feature knowledge could serve as a potent source for multi-teacher knowledge distil- lation. timiryasov and tastet (2023) leverages an ensemble of gpt-2 and llama as teacher models to extract output distributions. similarly, fusellm (wan et al., 2024a) inno- vatively combines the capabilities of various llms through a weighted fusion of their output distributions, integrating them into a singular llm. this approach has the potential to significantly enhance the student model’s capabilities, surpassing those of any individual teacher llm. in summary, feature knowledge offers a more transpar- ent alternative to black-box methods, allowing for deeper insight into and control over the distillation process. by utilizing feature knowledge from teacher llms, such as output distributions and intermediate layer features, white- box approaches enable a more nuanced transfer of informa- tion. while showing promise, especially in smaller models, its application is not suitable for black-box llms where internal parameters are inaccessible. furthermore, student models distilled from white-box llms may underperform compared to their black-box counterparts, as the black-box teacher llms (e.g. gpt-4) tend to be more powerful. 3.1.5 feedback most previous works predominantly focus on one-way knowledge transfer from the teacher to the student for imitation, without considering feedback from the teacher on the student’s generation. the feedback from the teacher typically offers guidance on student-generated outputs by providing preferences, assessments, or corrective informa- tion. for example, a common form of feedback involves teacher ranking the student’s generations and distilling this preference into the student model through reinforcement learning from ai feedback (rlaif) (bai et al., 2022a). here is a generalized formulation for eliciting feedback knowledge: d(fb)={(x, y, ϕ fb(x, y;θt))|x∼ x, y∼ps(y|x)}, (7) where ydenotes the output generated by the student model in response to x, and ϕfb(·;θt))represents providing feedback from teacher llms. this operation evaluates thestudent’s output ygiven the input x, by offering assess- ment, corrective information, or other forms of guidance. this feedback knowledge can not only be distilled into the student to also generate feedback (such as creating a student preference model) but, more importantly, enable the student to refine its responses based on the feedback. various methods have been explored to elicit this advanced knowledge (bai et al., 2022a; luo et al., 2023b; cui et al., 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). preference, as previously discussed, represents a notable form of feedback knowledge from teacher models. various knowledge of preferences could be distilled from teachers by prompting it with specific criteria. bai et al. (2022a) in- troduce rlaif for distilling harmlessness preferences from llms. this involves using an sft-trained llm to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. this dataset is distilled into a preference model (pm), which then guides the rl training of a more harmless llm policy. wizard- math (luo et al., 2023b) places emphasis on mathematical reasoning. they employ chatgpt as teacher to directly provide process supervision and evaluate the correctness of each step in the generated solutions. to scale up high- quality distilled preference data, cui et al. (2023a) develop a large-scale preference dataset for distilling better preference models, ultrafeedback. it compiles various instructions and models to produce comparative data. then, gpt-4 is used to score candidates from various aspects of preference, including instruction-following, truthfulness, honesty and helpfulness. beyond merely assessing student generations, teachers can also furnish extensive feedback on instances where students underperform. in lion (jiang et al., 2023b), teacher model pinpoints instructions that pose challenges to the student model, generating new, more difficult instructions aimed at bolstering the student’s abilities. persd (chen et al., 2023a) showcases a method where teacher offers tailored refinement feedback on incorrect code snippets gen- erated by students, guided by the specific execution errors encountered. similarly, selfee (ye et al., 2023) leverages chatgpt to generate feedback and revise the student’s answer based on the feedback. in contrast, figa (guo et al., 2024) revises the student’s response by comparing it to the ground-truth response. furthermore, teacher model’s distribution over the student’s generations can itself act as a form of feedback. minillm (gu et al., 2024) and gkd (agarwal et al., 2024) present an innovative strategy wherein the student model initially generates sequences, followed by teacher model producing an output distribution as feedback. this method leverages the teacher’s insight to directly inform and refine the student model’s learning process. 3.1.6 self-knowledge the knowledge could also be elicited from the student itself, which we refer to as self-knowledge . in this setting, the same model acts both as the teacher and the student, iteratively improving itself by distilling and refining its own previously 11 generated outputs. this knowledge uniquely circumvents the need for an external, potentially proprietary, powerful teacher model, such as gpt-series llms. furthermore, it allows the model to surpass the limitations or “ceiling” inherent in traditional teacher-student methods. eliciting self-knowledge could be formulated as: d(sk)={(x, y, ϕ sk(x, y))|x∼ s, y∼ps(y|i⊕x)},(8) where ϕsk(·)is a generalized function that represents an additional process to the self-generated outputs y, which could include but is not limited to filtering, rewarding, or any other mechanisms for enhancing or evaluating y. it could be governed by external tools or the student itself θs. recent research in this area has proposed various innovative methodologies to elicit self-knowledge, demonstrating its potential for creating more efficient and autonomous learn- ing systems. (allen-zhu and li, 2020; wang et al., 2022a; sun et al., 2024b; yang et al., 2024a; jung et al., 2023; huang et al., 2023a; gulcehre et al., 2023; yuan et al., 2024a; xu et al., 2023b; zelikman et al., 2022; chen et al., 2024a; zheng et al., 2024; li et al., 2024c; zhao et al., 2024; singh et al., 2023; chen et al., 2024c; hosseini et al., 2024) a notable example of this methodology is self- instruct (wang et al., 2022a), which utilizes gpt-3 for data augmentation through the expansion approach, gen- erating additional data samples to enhance the dataset. this enriched dataset subsequently fine-tunes the original model. other methods aim to elicit targeted knowledge from student models by modifying prompts, and leveraging these data for further refinement. in self-align (sun et al., 2024b), they find that models fine-tuned by self-instruct data tend to generate short or indirect responses. they prompt this model with verbose instruction to produce in- depth and detailed responses. then, they employ context- distillation (askell et al., 2021) to distill these responses paired with non-verbose instructions back to the model. similarly, rlcd (yang et al., 2024a) introduces the use of contrasting prompts to generate preference pairs from an unaligned llm, encompassing both superior and inferior examples. a preference model trained on these pairs then guides the enhancement of the unaligned model through reinforcement learning. several other approaches employ filtering methods to refine self-generated data. for exam- ple, impossible distillation (jung et al., 2023) targets sen- tence summarization tasks, implementing filters based on entailment, length, and diversity to screen self-generated summaries. lmsi (huang et al., 2023a) generates multiple cot reasoning paths and answers for each question, and then retains only those paths that lead to the most consistent answer. note that refined self-knowledge can be iteratively ac- quired as the student model continuously improves, further enhancing the student’s capabilities. this is gulcehre et al. (2023) introduces a reinforced self-training (rest) frame- work that cyclically alternates between grow andimprove stages to progressively obtain better self-knowledge and refine the student model. during the grow stage, the student model generates multiple output predictions. then, in the improve stage, these self-generated outputs are ranked and filtered using a scoring function. subsequently, the lan- guage model undergoes fine-tuning on this curated dataset,employing an offline rl objective. self-play (chen et al., 2024a) introduces a framework resembling iterative dpo, where the language model is fine-tuned to differentiate the self-generated responses from the human-annotated data. these self-generated responses could be seen as “negative knowledge” to promote the student to better align with the target distribution. self-rewarding (yuan et al., 2024a) explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for the self-generated responses. the entire process can then be iterated, improving instruction following and reward modeling capabilities. 3.2 distillation this section focuses on the methodologies for effectively transferring the elicited knowledge from teacher llms into student models. we explore a range of distillation tech- niques, from the strategies that enhance imitation by su- pervised fine-tuning ,divergence and similarity , to advanced methods like reinforcement learning and rank optimization , as shown in figure 3. 3.2.1 supervised fine-tuning supervised fine-tuning (sft), or called sequence-level kd (seqkd) (kim and rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-box llms. sft finetunes student model by maximizing the like- lihood of sequences generated by the teacher llms, aligning the student’s predictions with those of the teacher. this process can be mathematically formulated as minimizing the objective function: lsft=ex∼x,y∼pt(y|x)[−logps(y|x)], (9) where yis the output sequence produced by the teacher model. this simple yet highly effective technique forms the basis of numerous studies in the field. numerous re- searchers have successfully employed sft to train student models using sequences generated by teacher llms (taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; luo et al., 2023b). additionally, sft has been ex- plored in many self-distillation works (wang et al., 2022a; huang et al., 2023c; xu et al., 2023b; zelikman et al., 2022). due to the large number of kd works applying sft, we only list representative ones here. more detailed works can be found in §4. 3.2.2 divergence and similarity this section mainly concentrates on algorithms designed for distilling feature knowledge from white-box teacher llms, including distributions and hidden state features. these algorithms can be broadly categorized into two groups: those minimizing divergence in probability distributions and those aimed at enhancing the similarity of hidden states. divergence. divergence-based methods minimize diver- gence between the probability distributions of the teacher 12 divergence type d(p, q)function forward kldpp(t) logp(t) q(t) reverse kldpq(t) logq(t) p(t) js divergence1 2\\x10pp(t) log2p(t) p(t)+q(t)+pq(t) log2q(t) p(t)+q(t)\\x11 table 1: functional forms of dfor various divergence types. p: reference similarity function lf expression l2-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥2 l1-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥1 cross-entropy loss −pφt(ft(x, y)) log(φ s(fs(x, y))) maximum mean discrepancy mmd (φt(ft(x, y)),φs(fs(x, y))) table 2: summary of similarity functions in knowledge distillation. and student models, represented by a general divergence function d: ldiv= e x∼x,y∼y[d(pt(y|x), ps(y|x))], (10) the specific form of dvaries depending on the type of divergence employed. table 1 outlines the functional forms ofdfor different divergence measures. the commonly-used standard kd objectives essentially minimize the approxi- mated forward kullback-leibler divergence (kld) between the teacher and the student distribution (sanh et al., 2019; wen et al., 2023; timiryasov and tastet, 2023; liang et al., 2023a; chen et al., 2024d) , which forces psto cover all the modes of pt. however, when a student model is unable to learn all modes of a highly complex teacher, the re- sultant “mode-covering” behavior might cause the student to assign probability mass to tokens with low probability under the teacher’s distribution (cf. figure 6 blue curve). this mode-covering phenomenon can potentially lead to hallucinations and low-quality generations. alternatively, mode-seeking divergences like reverse kl prioritize tokens where the teacher assigns high probabilities (cf. figure 6 green curve). this approach can mitigate the risk of low- quality outputs, fostering more accurate generations. how- ever, it often does so at the cost of reduced diversity. gu et al. (2024) adopt reverse kl divergence to prevent students from overestimating low-probability regions of the teacher’s distribution, employing policy gradient methods for opti- mization. both agarwal et al. (2024) and sason and verd ´u (2016) assess the efficacy of different divergence functions in llm distillation, finding the optimal divergence to be task-dependent. for instance, forward kl divergence is more suitable for tasks like machine translation, where the output has fewer modes or variations, while reverse kl divergence is preferable for tasks like dialogue generation and instruction tuning, which involve multiple modes and a wider range of potential responses. thus, the nature of the task significantly influences the selection of the divergence function for optimal performance. similarity. similarity-based methods in knowledge distilla- tion aim to align the hidden states or features of the student pargminqkl(p||q)argminqkl(q||p)fig. 6: comparison of forward and reverse kl diver- gences in approximating a target distribution . forward kl divergence approach tends to cover all modes of the target distribution but is less precise, i.e. “mode-covering” behavior. reverse kl divergence method focuses predom- inantly on the most prominent mode, thereby exhibiting a “mode-seeking” behavior. model with those of the teacher. these methods use various similarity metrics to measure and optimize the congruence of internal representations between the two models. the objective is to ensure that the student model not only produces similar outputs to the teacher but also processes information in a comparable manner. the formulation for a similarity-based objective might look like this: lsim= e x∼x,y∼y[lf(φt(ft(x, y)),φs(fs(x, y)))],(11) where ft(x, y)andfs(x, y)are the feature maps of the teacher and student models, respectively. the transforma- tion functions φtandφsare applied to these feature maps to ensure they are in the same shape, facilitating direct comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based methods in the kd of llms. among them, liang et al. (2023a) propose task-aware layer-wise distillation (ted), a method that utilizes task-aware filters. these filters are designed to selectively capture the most pertinent informa- tion for a specific task from the teacher model. the key objective is to minimize the discrepancy between the filtered representations in both teacher and student models. while similarity-based approaches are common in encoder-based lms (sun et al., 2019, 2020; jiao et al., 2020; hou et al., 2020; zuo et al., 2022; liang et al., 2021), their application in llm knowledge distillation is not as widespread. however, considering their effectiveness, we anticipate an increase in research exploring these methods for llm distillation in the near future. 3.2.3 reinforcement learning this section explores advanced methods of distilling knowl- edge into student models using reinforcement learning (rl). this approach is especially relevant for leveraging the feed- back from teacher to train student models (bai et al., 2022a; cui et al., 2023a; luo et al., 2023b; agarwal et al., 2024; chen et al., 2024b; ma et al., 2023a; pang et al., 2023; du et al., 2023a). the rl-based distillation process typically involves two main stages: 13 distilled reward model training. the first stage involves training a reward model rϕusing the feedback data d(fd) generated by teacher llms. preference data, as one of the typical feedback, is employed to train the student reward model (bai et al., 2022a; cui et al., 2023a; lee et al., 2023a; kim et al., 2023a). they usually consist of input-output pairs (x, yw, yl). here, ywandylrepresent “winning” and “losing” outputs relative to the teacher’s preferences. the loss function for the reward model is defined as: lrm(rϕ,d(fd)) =− e (x,yw,yl)∼d(fd)[logσ(rϕ(x, yw)−rϕ(x, yl))] (12) this formulation guides the reward model to correctly distinguish between more and less preferable outputs based on the teacher’s criteria. instead of learning the instance- level rewards, rlmec (chen et al., 2024b) adopts a dif- ferent approach by training a generative reward model. it is trained on an erroneous solution rewriting data distilled from a teacher llm. this distilled reward model can pro- duce token-level rewards for rl training. reinforcement learning optimization. in the second stage, the student model, represented by a policy πθ, is optimized to maximize the expected reward as per the trained reward model. simultaneously, it minimizes the divergence from a reference policy πref, typically the initial policy of the student model trained by sft, controlled by a factor β. the rl objective is given by: max πθe x∼x,y∼πθ(y|x)[rϕ(x, y)]−βdkl[πθ(y|x)∥πref(y|x)] (13) this rl framework not only ensures that the student model learns the explicit content from the teacher but also effec- tively adopts the teacher’s preference patterns. the use of rl, particularly with the ppo (schulman et al., 2017) algo- rithm, offers a robust mechanism for aligning the student model’s outputs with the teacher. alternatively, the teacher llm can also serve as the reward model to directly assign rewards during rl, circumventing the need for training a reward model (lee et al., 2023a; kwon et al., 2023). while this approach may exhibit superior performance, it comes at a higher computational cost compared to employing a smaller distilled reward model. 3.2.4 ranking optimization ranking optimization presents a stable and computationally efficient alternative to rl for injecting preference feedback into language models (rafailov et al., 2023; song et al., 2023a; yuan et al., 2023b). this method, diverging from traditional rl approaches, directly incorporates ranking information into language models from a fixed preference dataset during fine-tuning. intuitively, it directly updates policy to increase the relative likelihood of preferred over less favored responses. this direct optimization of prefer- ences, without the need for sampling outputs, makes the process more stable and efficient. recently, some works have been proposed to explore using ranking optimization todistill teacher’s preferences into student models (tunstall et al., 2023; hong et al., 2023; yuan et al., 2024a). zephyr (tunstall et al., 2023) utilizes direct preference optimization (dpo) (rafailov et al., 2023) to distill the preference alignment in teacher llms. dpo streamlines the objective of reinforcement learning (as in eq. 13), which involves reward maximization with a kl-divergence constraint, into a single-stage policy training. specifically, dpo’s training goal is to maximize the following expecta- tion: e (x,yw,yl)∼d(fd)\\x14 logσ\\x12 βlogπθ(yw|x) πref(yw|x)−βlogπθ(yl|x) πref(yl|x)\\x13\\x15 , (14) where ywis preferred over ylaccording to the teacher llm. hong et al. (2023) (hong et al., 2023) adopt two ranking-based optimization objectives, rank responses to align human feedback (rrhf) (yuan et al., 2023b) and preference ranking optimization (pro) (song et al., 2023a), for preference distillation. rrhf (yuan et al., 2023b) focuses on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj are their corresponding conditional log probabilities under the policy πθ. this approach emphasizes direct comparison and ranking of responses based on the teacher’s preferences. pro (song et al., 2023a) expands the concept of pairwise comparison to handle preference rankings of any length. for a given instruction xand a sequence of responses ordered by teacher preference as y1≻y2≻...≻yn, the rpo training objective is: lpro=−n−1x k=1logexp (pk)pn i=kexp (pi), (16) where pkrepresents the conditional log probabilities for ykunder the student policy πθ. by iteratively contrasting the likelihood of generating responses, pro optimizes the student lm to prioritize the most preferred response while progressively ranking the rest in the order of diminishing preference. 4 s kill distillation building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms. our exploration will encompass a diverse range of skills exhibited by llms, including context following ,alignment ,agent ,nlp task specializa- tion and multi-modality .context following focuses on the student’s ability to comprehend and respond effectively to input information. alignment delves into the student’s capability to align its output with the teacher’s responses. moving forward, agent underscores the autonomous nature of language models. nlp task specialization highlights the llm’s versatility in specializing across various natural language processing tasks, demonstrating its adaptability. 14 methods skill seed knowledge teacher llm student model knowledge elicitation objective context following self-instruct (wang et al., 2022a) if 175 human-curated tasks gpt3 gpt3 expansion + self-knowledge sft alpaca (taori et al., 2023) if 175 human-curated tasks gpt3 llama expansion + self-knowledge sft lamini-lm (wu et al., 2023c) if3.5k wikipedia categories + mixed datasetchatgpt various models expansion sft wizardlm (xu et al., 2023a) if alpaca data chatgpt llama expansion sft lion (jiang et al., 2023b) if alpaca cata chatgpt llama labeling + expansion + feedback - babyllama (timiryasov and tastet, 2023) if 10m-word babylm dataset gpt-2 + small llama 58m-parameter llama feature d&s minillm (gu et al., 2024) if dolly dataset gpt2 + opt + llama gpt2 + opt + llama feature d&s self-align (sun et al., 2024b) if human-written principles llama llama expansion + self-knowledge sft self-rewarding (yuan et al., 2024a) if human-written samples llama llama self-knowledge sft + rl star (zelikman et al., 2022) if arithmetic + commonsenseqa + gsm8k gpt-j gpt-j self-knowledge sft llama-gpt4 (peng et al., 2023a) if alpaca dataset gpt4 llama labeling sft reflection-tuning (li et al., 2023e) if alpaca/wizardlm dataset chatgpt llama labeling sft selective reflection-tuning (li et al., 2024d) if alpaca/wizardlm dataset chatgpt llama labeling sft vicuna (chiang et al., 2023) if/md human conversation chatgpt + gpt4 llama labeling sft koala (geng et al., 2023) if/md human conversation chatgpt llama labeling sft baize (xu et al., 2023b) if/md quora + stack overflow chatgpt llama expansion + self-knowledge sft ultrachat (ding et al., 2023b) if/md wikidata + text material + c4 chatgpt llama curation sft orca (mukherjee et al., 2023) if/tp flan-v2 chatgpt + gpt4 llama labeling sft orca2 (mitra et al., 2023) if/tp flan-v2 + few-shot/math/synthetic gpt4 llama labeling sft selfee (ye et al., 2023) if/tp human conv, flan/code/math collection chatgpt llama labeling sft cot-distill (hsieh et al., 2023) if/tp e-snli + anli + cqa + svamp palm t5 labeling sft knowpat (zhang et al., 2023a) if/tp cpkg + qa data chatgpt + chatglm + vicuna-7b llama labeling sft debatune (li et al., 2024e) if/tp controversial topics chatgpt llama labeling sft phi-1 (gunasekar et al., 2023) if/code - gpt3.5 phi-1 curation sft phi-1.5 (li et al., 2023a) if/code 20k topics from web gpt3.5 phi-1 curation + labeling sft sail (luo et al., 2023c) if/rag alpaca data + web content gpt4 llama label sft kard (kang et al., 2023b) if/rag medqausmle chatgpt t5 + opt label sft + d&s self-rag (asai et al., 2023) if/rag open-instruct gpt4 llama labeling sft alignment openchat (wang et al., 2023c) if/preference human conversation chatgpt + gpt4 llama labeling sft + rl zephyr (tunstall et al., 2023) if/preference mixed datasets gpt4 mistral labeling + feedback sft + ro almost (kim et al., 2023a) if/preference human-written prompts llama llama expansion + labeling sft + rl rlcd (yang et al., 2024a) if/preference human-written prompts llama llama labeling sft + rl rlaif (lee et al., 2023a) if/preference human-written prompts palm 2 palm 2 labeling + feedback rl gpt3 reward (kwon et al., 2023) preference human-written prompts gpt3 gpt3 labeling rl ilf (scheurer et al., 2023) preference task-specific datasets gpt3 + feedme gpt3 labeling rl ultrafeedback (cui et al., 2023a) preference mixed datasets gpt4 llama labeling rl constitutional ai (bai et al., 2022a) preference/value human-written prompts self-defined student model self-defined model labeling + expansion + feedback sft + rl sandbox (liu et al., 2023b) value simulationtext-davinci-002/-003 + gpt4 + chatgptllama data curation sft + rl agent toolformer (schick et al., 2023) tool ccnet gpt-j gpt-j labeling sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft gpt4tools (yang et al., 2023b) tool image content chatgpt llama curation + expansion sft toolalpaca (tang et al., 2023a) tool public-apis repository chatgpt llama curation sft toolllm (qin et al., 2023a) tool real-world apis chatgpt llama curation sft mllm-tool (wang et al., 2024) tool huggingface model cards gpt4 llama curation sft fireact (chen et al., 2023b) planning mixed qa dataset gpt4 llama labeling sft agenttuning (zeng et al., 2023a) planning 6 agent tasks gpt4 + chatgpt llama labeling + expansion sft lumos (yin et al., 2023a) planning mixed interactive tasks gpt4 llama labeling sft autoact (qiao et al., 2024) planning mixed qa tasks llama llama labeling sft nlp task specialization auggpt (dai et al., 2023a) nlu amazon/symptoms/pubmed20k dataset chatgpt bert label sft tdg (he et al., 2023b) nlu sst + qqp + mnli gpt3 bert expansion sft sungen (gao et al., 2023a) nlu text classification tasks gpt2 distilbert curation sft udg (wang et al., 2021a) nlu nlu tasks gpt3 bert expansion sft inheritsumm (xu et al., 2023c) nlg pile + arxiv + cnn/dm + wikihow gpt3.5 zcode++ label sft dimsum+ (jung et al., 2023) nlg none gpt2 + ctrl + biogpt t5 curation + self-knowledge sft genie (yehudai et al., 2024) nlg eli5 + asqa + nq + cnn/dm falcon + llama flan + llama label sft gkd (agarwal et al., 2024) nlg/nlu/if xsum+wmt14 en-de+gsm8k+flan2021 t5-xl t5 feature + feedback d&s + rl quill (srinivasan et al., 2022) ir ir datasets t5 4-layer transformer internal knowledge d&s rankvicuna (pradeep et al., 2023a) ir ir datasets chatgpt llama labeling sft rankzephyr (pradeep et al., 2023b) ir ir datasets chatgpt + gpt4 mistral labeling sft ndr (mysore et al., 2023) recommendation recommendation datasets gpt3 mpnet-110m labeling sft instrcutrec (zhang et al., 2023b) recommendation 39 instruction templates chatgpt flan-t5 expansion + self-knowledge sft once (liu et al., 2023c) recommendation recommendation dataset chatgpt llama labeling sft pandalm (wang et al., 2023b) evaluation alpaca data chatgpt llama labeling sft prometheus (kim et al., 2024) evaluation 50 seed rubrics gpt4 llama labeling sft instructscore (xu et al., 2023d) evaluation mixed dataset gpt4 llama labeling sft wizardmath (luo et al., 2023b) math gsm8k + math chatgpt llama expansion + feedback sft + rl mammoth (yue et al., 2023a) math/tp mixed math dataset gpt4 llama labeling sft mixed distill (chenglin et al., 2023) math/tp svamp + gsm8k + asdiv + strategyqa chatgpt llama labeling sft wizardcoder (luo et al., 2023a) code code alpaca data chatgpt starcoder expansion sft magicoder (wei et al., 2023) code existing source codes chatgpt llama curation sft wavecoder (yu et al., 2024) code existing source codes gpt4 llama curation sft code alpaca (chaudhary, 2023) code code instructions chatgpt llama expansion + self-knowledge sft code llama (rozi `ere et al., 2023) code human-written instructions llama llama expansion + self-knowledge sft code clean (jain et al., 2023) code code datasets chatgpt llama labeling sft multi-modality llava (liu et al., 2023e) vision-language coco gpt4 llama labeling sft svit (zhao et al., 2023b) vision-language visual genome + coco gpt4 llama labeling sft lvis-instruct4v (wang et al., 2023e) vision-language lvis gpt4v llama labeling sft llavar (zhang et al., 2023d) vision-language laion gpt4 llama labeling sft macaw-llm (lyu et al., 2023) multiple modalities image/video with caption chatgpt llama labeling sft mimic-it (li et al., 2023f) multiple modalities image/video dataset chatgpt llama labeling sft chatbridge (zhao et al., 2023d) multiple modalities task-specific/multimodal-chat data gpt4 + chatgpt llama labeling sft table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, rag: retrieval-augmented generation, nlu: natural language understanding, nlg: natural language generation, ir: information retrieval, sft: supervised fine-tuning, d&s: divergence and similarity, rl: reinforcement learning, ro: ranking optimization. finally, multi-modality encompasses the knowledge trans- fer from teacher llms to multi-modal models. table 3 summarizes the representative works, encompassing details such as the skills involved, seed knowledge, teacher llm, student model, knowledge elicitation method, and training objectives.4.1 context following this part concentrates on the distillation of context follow- ing skills from llms. this process involves transferring the ability of llms to handle a variety of complex contexts — such as few-shot demonstrations, intricate instructions, dia- logue history, and retrieval-augmented information — into smaller models. many research efforts in this domain aim to imbue smaller models with these sophisticated, context- 15 following capabilities. our discussion here will dissect this facet of skill distillation, categorizing it based on different types of context and elaborating on how each is distilled and incorporated into smaller, efficient models. 4.1.1 instruction following instruction-following capacity enables llms to understand and follow user-given instructions. this ability significantly enhances human-ai interaction, allowing for seamless un- derstanding and execution of tasks as directed by users. a primary method for acquiring this skill involves construct- ing instruction-like prompt-response pairs and employing supervised fine tuning (sft) for model training. data for this purpose can be manually curated by human experts or transformed from existing nlp tasks into instructional formats with templates, such as prefacing machine transla- tion data with ”translate this sentence to spanish:” . however, these approaches have limitations. manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input. llms like gpt-4 offer an efficient alternative for creating diverse and controlled sft data by their capabil- ities of in-context learning and instruction following. most relevant works use openai’s gpt series models to generate prompt-response data pairs and then train the student llms by supervised fine-tuning (wang et al., 2022a; taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; mukherjee et al., 2023; mitra et al., 2023; luo et al., 2023b; peng et al., 2023a). basic instructions. self-instruct (wang et al., 2022a) lever- ages the in-context learning capability of gpt-3 to expand a seed pool of 175 tasks to 52k task-agnostic instructions, ensuring a broad spectrum of general instructions. addi- tionally, a filtering and post-processing stage is introduced to eliminate redundant or similar instructions. notably, through training with this enriched dataset, gpt-3 acquires the ability to follow instructions, enabling it to perform comparably to instructgpt in zero-shot instruction tasks and when provided with expert-written instructions for novel tasks. based on the self-instruct method, taori et al. (2023) train an alpaca model using the llama 7b model on 52k instruction-following demonstrations, generated in a similar style as self-instruct but utilizing the more robust text-davinci-003 model. to enhance the diversity of instruc- tional data, wu et al. (2023c) introduce a technique known astopic-guided instruction generation . this method involves gathering 3.5k common topics from wikipedia to serve as guidance during the generation process. complex instructions. some works promote students to solve more complex instructions (xu et al., 2023a; luo et al., 2023b,a; guo et al., 2023c). according to xu et al. (2023a), in- struction datasets derived from human-written seeds often exhibit low to moderate complexity. to enhance the com- plex instruction-following capabilities of smaller models, wizardlm (xu et al., 2023a) introduces evol-instruct . this method gradually transforms instructions into more com- plex forms through a multi-step evolution process, focusing on both increasing difficulty levels and expanding the di- versity of topics. they conducted four rounds of evolution using the openai chatgpt api, resulting in a dataset of250k complex instructions. subsequently, they trained the llama 7b model, referred to as wizardlm, on this dataset. in the high-difficulty section of test instructions, wizardlm even outperformed chatgpt, achieving a win rate 7.9% higher than chatgpt. zhao et al. (2023e) further conduct preliminary studies revealing the effectiveness of increasing instruction complexity. instruction fusion (guo et al., 2023c) further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to distill specific skills such as coding (luo et al., 2023a) and mathematics (luo et al., 2023b). human instructions. in contrast to works that rely on gener- ating instructions from chatgpt, which may lack diversity and have gaps with real human instructions, vicuna (chiang et al., 2023) and koala (geng et al., 2023) showcase impres- sive performance by using human conversations and natu- ral instructions from community-contributed conversations. these conversations, found in platforms like sharegpt, pro- vide a forum for users to share their interactions with chat- gpt. it’s important to note, however, that models trained on such natural conversations might mimic the style but may not fully capture the reasoning process of the original teacher (gudibande et al., 2023; mukherjee et al., 2023). system instructions. to encourage student models to learn the reasoning process, orca and orca 2 (mukherjee et al., 2023; mitra et al., 2023) enhance the prompt, response data pairs by introducing a system message (e.g., ”explain like i’m five, think step-by-step”) to encourage student mod- els to grasp the reasoning process. this system message prompts gpt-4 to provide explanation traces that eluci- date the teacher’s reasoning process. orca 2 (mitra et al., 2023) further trains the student model to identify the most effective solution strategy for each task, guided by orca’s performance. this approach significantly improves the abil- ity of smaller models to follow instructions that involve reasoning. high-quality instructions. as demonstrated in zhou et al. (2023a) and (li et al., 2024f), the data quality is crucial for instruction following training. ultrachat (ding et al., 2023b) distills large-scale data with high-quality and di- verse instructions from teacher llms by various meta- information. the ultrallama model, fine-tuned on this data, consistently surpasses other open-source models. the phi series models (gunasekar et al., 2023; li et al., 2023a; mar, 2023) prioritize data quality and employ synthetic methods to generate data of “textbook quality” to enhance the learning experience for smaller models. notably, phi exhibits the ability to follow instructions effectively even without specific instruction fine-tuning. what’s particularly remarkable is that phi-2, with just 2.7 billion parameters, outperforms mistral and llama-2 models with 7b and 13b parameters across various benchmark evaluations. improved instructions. another line of work focuses on improving the quality of existing instruction data, including both the improvement of instruction and corresponding response. selfee (ye et al., 2023) utilizes the chatgpt to iter- atively improve the quality of responses. expertllama (xu et al., 2023f) improves the quality of responses by augment- 16 ing vanilla instructions with specialized expert identity descriptions. reflection-tuning (li et al., 2023e) improves both the instruction and response sequentially by reflecting on specific criteria. deita (liu et al., 2023h) proposes to enhance and score instructions in three directions includ- ing complexity, quality, and diversity to get high-quality distillation data. muffin (lou et al., 2023) proposes to scale the instruction according to the input by diversifying these tasks with various input facets. selective reflection- tuning (li et al., 2024d) first involves the student model in the data improvement pipeline with a novel student- selection module, in which the student model is able to decide the data learn from. in summary, distilling instruction data from teachers presents a promising avenue for training cheap and re- producible instruction-following language models. cur- rent small models have made strides in enhancing var- ious aspects of instruction-following ability, like diver- sity, complexity and explanation. however, student mod- els trained on instruction data expanded by chatgpt of- ten mimic chatgpt’s style without replicating its factual accuracy (gudibande et al., 2023). achieving a more ca- pable instruction-following capability requires a stronger teacher llm (gudibande et al., 2023) and access to di- verse, high-quality instruction data, such as the one used in orca (mukherjee et al., 2023; mitra et al., 2023), which incorporates extensive task instructions from the flan 2022 collection (longpre et al., 2023). 4.1.2 multi-turn dialogue while instruction following focuses on single-instance com- mand execution, multi-turn dialogue extends this to com- prehend and maintain context through ongoing interactions. this skill is vital for models to engage meaningfully in human-like conversations and respond coherently over suc- cessive dialogue turns. some works have been dedicated to train to small chat models by distilling multi-turn knowl- edge from teacher llms (chiang et al., 2023; xu et al., 2023b; ding et al., 2023b; li et al., 2023b; wang et al., 2023c; tunstall et al., 2023). sharegpt serves as a platform for users to share their conversations with chatgpt, offering a vast repository of multi-turn conversations readily available. some small chat models are trained using this data to acquire the capability for engaging in multi-turn dialogues (chiang et al., 2023; ye et al., 2023; wang et al., 2023c). for example, vicuna (chiang et al., 2023) is a chat model exclusively trained on sharegpt data. despite its sole training source being sharegpt, vi- cuna achieves a high mt-bench (zheng et al., 2023a) score assigned by gpt-43. in the study conducted by wang et al. (2023c), gpt-3.5 and gpt-4 are employed to generate mixed responses using sharegpt data. they assign higher rewards to responses generated by gpt-4, aiming to incentivize student models to produce high-quality responses. addi- tionally, ye et al. (2023) enhance the quality of multi-turn data from sharegpt by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. 3. mt-bench: a multi-turn question set, where the generations of models are evaluated by llm, like gpt-4.to enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversa- tional datasets through self-chat and using them to train smaller models (xu et al., 2023b; ding et al., 2023b; tunstall et al., 2023). for instance, xu et al. (2023b) initiate their work by using questions sourced from quora and stack overflow as seeds, resulting in the collection of 111.5k dialogues through self-chat. subsequently, they employ parameter- efficient tuning to train a chat model named baize. ding et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions and dialogues from chatgpt. notably, ultrachat encom- passes a wide range of topics and instructions. building upon the ultrachat dataset, they fine-tune a llama model, resulting in the creation of a powerful chat model known as ultrallama. ultrallama consistently outperforms other open-source chat models, including vicuna and baize. fur- thermore, ultrachat is employed in conjunction with an ai preference-aligned chat model named zephyr (tunstall et al., 2023). zephyr enhances intent alignment through the application of distilled direct preference optimization (ddpo). 4.1.3 rag capbility llms are known to lack the ability to utilize up-to-date knowledge, and often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge. retrieval-augmented generation (rag) is a promising technique to decrease this issue. handling the augmented context of retrieved information is also a non- trivial skill of llms. several approaches to distill rag capabilities have been proposed (kang et al., 2023a; luo et al., 2023c; asai et al., 2023). sail (luo et al., 2023c) starts by retrieving search results for each training case using search apis, creating search- augmented instructions that include both the instruction and grounding information. to encourage the language model to prioritize informative retrieval results, they input each retrieved passage along with the ground truth response into the entailment model to label each retrieval result for relevance. subsequently, the search-augmented instructions and relevance labels are fed into teacher llms (like gpt- 4) for generating responses. following fine-tuning on this training set, the student model becomes proficient at de- noising search results and generating accurate responses. kard (kang et al., 2023b) distills rationales rfrom the teacher llm in response to questions x. these rationales are then utilized to train two models: a student lm and a reranker. for training the student lm, the rationales serve as a means to retrieve relevant knowledge d, and the student lm is subsequently fine-tuned using the rationales along- side questions and knowledge. however, during inference, only questions are available. to address this, the reranker is trained to mimic how the retriever scores passages with the rationale by minimizing the kl divergence between retriever (d|r)andreranker (d|x). however, the integra- tion of a fixed number of passages in language models, without considering their necessity or relevance, can reduce versatility and lead to the generation of unhelpful responses. to equip student lms with adaptive rag capabilities, self- 17 rag (asai et al., 2023) distills this adaptive ability from teacher llms into a small critic model. this critic model determines whether retrieval is necessary and evaluates the quality of the retrieved results by generating ‘reflection to- kens.’ for instance, self-rag initiates the retrieval operation when generating the reflection token retrieve . to distill this critic data, gpt-4 is prompted to assess the need for retrieval using few-shot demonstrations i, the task input x, and output yto predict a reflection token ras follows: p(r|i, x, y ). 4.2 alignment 4.2.1 thinking pattern most existing methods mainly focus on directly aligning the direct responses of the student models to the responses of teacher models (taori et al., 2023). though effective, these models might suffer the problems that they tend to learn to imitate the response style of the teacher models, but not the reasoning process (mukherjee et al., 2023). thus in order to better distill from the teacher models, methods are proposed that not only imitate the pure responses but some novel thinking patterns (ye et al., 2023; mukherjee et al., 2023; mitra et al., 2023; wang et al., 2023d; cheng et al., 2023; zhang et al., 2023a). motivated by the effectiveness of llms in generat- ing their own feedback without relying on external mod- els (schick et al., 2022; madaan et al., 2023; saunders et al., 2022), selfee (ye et al., 2023) proposes to train a model that has been fine-tuned to continuously revise its own answer until it provides a high-quality response in a single inference. during training, it utilizes both the final response and feedback chain as the fitting target. this pat- tern, response with the revision process, shows a promising performance gain. following selfee, reflection-tuning (li et al., 2023e, 2024d) also utilizes the reflection process as the learning pattern. noticing the lack of reasoning imitation of the previous methods, orca (mukherjee et al., 2023) first proposes explanation tuning, which aims to learn the reasoning steps, including explanation traces, step-by-step thought processes, and other complex instructions, from the teacher model, rather than just the vanilla styles. extensive experiments verify the effectiveness of distilling with this thinking pattern. the following orca2 (mitra et al., 2023) further presents to equip the student models with the ability to utilize different solution strategies for different tasks, mo- tivated by the capability discrepancies between the smaller and larger models. by employing this training pattern, the student models are able to gain a better reasoning ability. be- sides learning with the corresponding revision or reflection process, another thinking pattern that recently appeared is generating both responses and preferences. zhang et al. (2023a) propose to learn both the knowledge and corre- sponding preference for domain-specific qa with llms. recently, debatune (li et al., 2024e) proposes to improve the controllability of llms in generating statements on controversial topics. by engaging two agents in a structured multi-round debate on controversial topics, salient and in- depth statements can be obtained and further distilled into the student models.4.2.2 preference the previously mentioned methods primarily focus on the basic capability of student models to produce outcomes that are strictly accurate but may not align with human preferences, reaching alignment at this level enables these models to aid in various tasks without meeting higher-level demands. early methods mainly utilize human feedback for the alignment of human preferences (ziegler et al., 2019; stiennon et al., 2020; wu et al., 2021; ouyang et al., 2022; bai et al., 2022b; k ¨opf et al., 2023; yuan et al., 2023b). however, obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon et al., 2023; scheurer et al., 2023; kim et al., 2023a; roit et al., 2023; yang et al., 2024a; lee et al., 2023a; tunstall et al., 2023; cui et al., 2023a; wang et al., 2023f). the concept of rlaif, introduced by bai et al. (2022a), involves the integration of preferences labeled by llms with those labeled by humans. this approach is designed to simultaneously optimize two key objectives: ensuring the helpfulness of the output and minimizing any potential harm, making the responses of llms more aligned with human preferences. kwon et al. (2023) develop a proxy reward function using llms like gpt-3, which is created by first providing the llm with a description of the behaviors desired by the user, along with a small number of examples. the llm then produces rewards by evaluating how closely the outputs of a model align with the provided descrip- tions, essentially measuring their relevance to the estab- lished ground truth. scheurer et al. (2023) propose imitation learning from language feedback, in which a language model is utilized to improve various outputs generated by a model. this refinement is based on a reference provided by a human. following this process, the most effectively refined output is chosen to be used in further supervised fine-tuning. as outlined by kim et al. (2023a), almost in- volves condensing human preferences into a set of heuristic guidelines. an example of such a rule is the idea that larger llms that utilize more comprehensive and higher-quality prompts are likely to yield superior responses. based on these established guidelines, comparison data is generated using responses from llms of different sizes and with varying prompts. this data is then used to train a reward model. yang et al. (2024a) propose reinforcement learning from contrast distillation, which aims to align language models without relying on human feedback. this approach involves training a preference model using simulated pairs of preferences, including both high-quality and low-quality examples which are generated through contrasting prompts, positive and negative. lee et al. (2023a) further highlight the effectiveness of rlaif. this work proposes that rlaif not only matches but in some cases surpasses rlhf, and interestingly, rlaif can also enhance the performance of supervised fine-tuning. another notable discovery is that directly prompting the llm for reward scores during reinforcement learning can be more effective than the conventional approach of training a reward model based on llm preferences. wang et al. (2023f) propose conditioned-rlft, which treats different data sources as coarse-grained reward labels and develops 18 a class-conditioned policy to effectively utilize the varying qualities of data, which is a reinforcement learning-free supervised learning approach. cui et al. (2023a) propose a large-scale, high-quality, and diversified preference dataset labeled by gpt4 for comprehensive feedback. tunstall et al. (2023), by proposing distilled direct preference optimiza- tion (rafailov et al., 2023) on ultrafeedback, obtaining a small by powerful llm. 4.2.3 value attaining alignment with human preferences allows large models to optimize human satisfaction by operating in a manner that aligns with human preferences. however, to establish trustworthy llms, the notion of ’aligning llms with human values’ is proposed and the key principles of alignment are often summarized as the “hhh” criteria: helpful, harmless, honest (weidinger et al., 2021; askell et al., 2021). numerous methods have been undertaken for building trustworthy llms. however, due to the intrinsic difficulty of this aim, which is still an unsolved problem for proprietary models (sun et al., 2024a), most existing methods rely on constructing high-quality human prefer- ence datasets (ji et al., 2023b; solaiman and dennison, 2021; bai et al., 2022b; qiu et al., 2022; kiesel et al., 2022; liu et al., 2022a), utilizing human-written rules as constrains (glaese et al., 2022; sun et al., 2023b, 2024b), etc. for detailed progress on trustworthy llms, please further refer to yao et al. (2023a); liu et al. (2023i); sun et al. (2024a). though slightly under-explored, aligning llms with human values by distilling is still possible (bai et al., 2022a; cui et al., 2023a; yang et al., 2024a; sun et al., 2024b). for instance, bai et al. (2022a) propose rlaif, utilizing ai- generated labels to interactively improve both helpfulness and harmlessness. sun et al. (2024b) prompt the student model with 16 principles as guidelines for generating help- ful, ethical, and reliable responses. similarly, both harmless and harmful generations could be elicited by modifying the prompts, and then are used to train the preference model (yang et al., 2024a). cui et al. (2023a) utilize gpt- 4 to rank generations regarding helpfulness, truthfulness, and honesty. liu et al. (2023b) advance the alignment of llms with societal values by incorporating simulated social interactions into the training process. this approach encom- passes a range of elements, including demonstrations that are both in alignment and in conflict with social norms, as well as collective ratings, in-depth feedback, and responses that are revised iteratively. 4.3 agent 4.3.1 tool using while recent llms have shown proficiency in solving var- ious tasks, they still tend to make mistakes when handling large numerical values or executing intricate mathematical calculations (qian et al., 2022; she et al., 2023; manikandan et al., 2023; liang et al., 2023b; mialon et al., 2023). thus equipping llm agents with the capability to utilize tools has been increasingly focused on. commonly used methods mainly relied on human-curated data for training (parisi et al., 2022; nakano et al., 2022; qin et al., 2023c; song et al., 2023b) or prompt designing(cai et al., 2023; shenet al., 2023a; hao et al., 2024). recently, distillation-based methods are also proposed (schick et al., 2023; zhang, 2023; patil et al., 2023; tang et al., 2023a; qin et al., 2023a; yuan et al., 2023a; gao et al., 2023b; wang et al., 2024; shen et al., 2024; yuan et al., 2024b). toolformer (schick et al., 2023) utilizes a self-supervised manner, avoiding large human annotations, to obtain the most required apis to use and further distill this capability to the model itself. the performance of the gpt-j-based toolformer surpasses opt (66b) (zhang et al., 2022) and gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed to enhance llms with graph reasoning skills using exter- nal graph reasoning api tools by adopting chatgpt to annotate and augment a larger graph reasoning statement dataset for training. gorilla (patil et al., 2023) addresses the limitations of current llms in generating accurate input arguments and reduces the problem of ”hallucination” or generating incorrect api usage and it collects thousands of models from platforms like huggingface and torch hub as the api calls and utilizes gpt4 to generate synthetic instruction data for training. gpt4tools (yang et al., 2023b) introduces to enable open-source llms like llama and opt to use multimodal tools, a capability previously limited to advanced proprietary models like chatgpt and gpt-4. the approach involves generating an instruction-following dataset by prompting an advanced teacher model with mul- timodal contexts, using the low-rank adaptation optimiza- tion. toolalpaca (tang et al., 2023a) proposes a framework aimed at enhancing the tool-use capabilities of compact language models for embodied intelligence. it creates a dataset with 3938 instances from over 400 real-world tool apis across 50 categories and utilizes chatgpt to generate documentation for each prompt for later training. toolllm (qin et al., 2023a) proposes a comprehensive framework for enhancing llms with tool-use proficiency, focusing on data creation, model training, and evaluation by distilling from chatgpt. their toolllama shows impressive performance in executing complex instructions and handling new apis, rivaling chatgpt. craft (yuan et al., 2023a) builds a general tool creation and retrieval framework, which uti- lizes gpt4 to generate code snippets as the created tools. during the inference, other small llms could select and retrieve from the generated code snippets to execute or generate other methods conditioned on the given snippets. confucius (gao et al., 2023b) introduces a tiered training strategy for llms to master tool usage through a graduated curriculum and an innovative method called iterative self- instruction from introspective feedback (isif) for dynamic dataset enhancement to handle complex tools. mllm-tool (wang et al., 2024) is a multi-modal tool agent capable of interpreting instructions embedded in visual or audio content through the integration of multi-modal encoders with open-source large language models. as a trainable method, the initial instruction-answer pairs are generated by utilizing gpt4. shen et al. (2024) demonstrate that small llms are weak tool learners and proposes a multi-llm framework that decomposes the tool-use ability of a single model into a planner, caller, and summarizer for the tool using, leading to a supreme performance. the two-stage 19 training strategy introduced by this work is powered by chatgpt and gpt4 for collecting execution trajectories for the training set. yuan et al. (2024b) notice the potential issue of the current lengthy tool documentation, which hinders llms from understanding how to utilize a tool, thus proposing easytool to purify the important infor- mation from extensive documentation. the ground truth summarization of the training documents is obtained by using chatgpt. 4.3.2 planning another important aspect for llm agents is the ability to decompose high-level tasks to a chosen set of actionable steps (huang et al., 2022b), which is especially useful when acting in interactive environments. huang et al. (2022b) first demonstrate that llms can generate plausible goal-driven action plans without training, introduces non-invasive tools to enhance model executability, and assesses these methods through human evaluation to balance executability and semantic accuracy. most existing methods utilize prompting strategies for task planning (singh et al., 2022; zhou et al., 2023b; song et al., 2023c; wang et al., 2023g; yao et al., 2023b; liu et al., 2023j; hao et al., 2023; hu et al., 2023a), or building human-curated data for training (lin et al., 2023a; valmeekam et al., 2023). recently, there have also been some distilling methods emerging (chen et al., 2023b; zeng et al., 2023a; yin et al., 2023a; qiao et al., 2024; kong et al., 2023). fireact (chen et al., 2023b) introduces an innovative ap- proach for refining llms. this method involves fine-tuning smaller-scale llms using agent trajectories that are derived from a variety of tasks and prompting techniques. applying this method with trajectories generated by gpt4 has been shown to consistently enhance performance. agenttuning (zeng et al., 2023a) aims to enhance the performance of llms in executing agent tasks without sacrificing their wide-ranging capabilities. by utilizing a new dataset called agentinstruct, which includes high-quality interaction tra- jectories, it applies a hybrid instruction-tuning approach that merges these trajectories with general domain instruc- tions. lumos (yin et al., 2023a) pertains to a novel frame- work designed to train agents using a unified data format and modular architecture based on open-source llms. this system comprises three key modules: planning, grounding, and execution, enabling the decomposition of tasks into subgoals and actionable steps. tptu-v2 (kong et al., 2023) focuses on improving the task planning and tool usage abili- ties of llms in real-world scenarios, by utilizing data gener- ated by human experts or llms. it introduces a framework comprising three components: an api retriever, an llm finetuner, and a demo selector. autoact (qiao et al., 2024) proposes an agent learning framework that does not require large-scale annotated data or synthetic trajectories from high-resource models like gpt-4. instead, it uses a self- instruct method to generate its own planning trajectories with limited initial data. it then applies a division-of-labor strategy, creating sub-agents specialized in different aspects of the task completion process. distillation also works out for the training of embodied multi-modal agents (sumers et al., 2023; yang et al., 2023c; ma et al., 2023a; du et al., 2023a; sumers et al., 2023). for instance, sumers et al. (2023) aim to enhance the ability ofai agents to follow instructions by using pretrained vision- language models to provide supervision for understanding and acting upon language within their operational environ- ment, leveraging model distillation and hindsight experi- ence replay to teach them contextually relevant interactions in a simulated 3d setting. emma (yang et al., 2023c) evalu- ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using imitation learning, guided by an expert language model (like chatgpt), which operates in a corresponding text- based simulation, focusing on the same tasks. 4.4 nlp task specialization nlp tasks often grapple with challenges like data scarcity, interpretability issues, privacy concerns, and noisy data. the “knowledge” section of our survey illustrates various methods for distilling knowledge from llms, effectively setting the stage for student models to adapt to a range of nlp tasks. this knowledge provides supervision for the training of student models through information aug- mentation (e.g., cot and explanation), data augmentation, and semantic representation. by transferring the distilled knowledge from llms, student models can better handle diverse nlp challenges, improving task performance and addressing data limitations more robustly. 4.4.1 natural language understanding natural language understanding (nlu) is a fundamen- tal nlp task that involves comprehending and interpret- ing human language. the knowledge distilled from llms, such as through data labeling or augmentation, is typi- cally transferred into encoder-based language models like bert (vaswani et al., 2017) and roberta (liu et al., 2019). regarding the task of classification, certain studies have been noteworthy (dai et al., 2023a; gilardi et al., 2023; he et al., 2023b; gao et al., 2023a; chenglin et al., 2023; li et al., 2023g). auggpt (dai et al., 2023a) focuses on both general and clinical domain text classification. to address the limitations of small-scale clinical datasets, which often lack expert annotation and are subject to stringent privacy regulations, auggpt utilizes knowledge from teacher llms to rephrase each sentence in the training samples. this process creates multiple conceptually similar but seman- tically distinct samples, enhancing the dataset’s richness and diversity. another approach is demonstrated by gilardi et al. (2023), who employ chatgpt as an annotator to cate- gorize inputs. this method has been shown to outperform crowd-workers in several tasks, including relevance, stance, topics, and frame detection. furthermore, he et al. (2023b) propose targeted data generation (tdg), a novel approach for identifying challenging subgroups within a dataset. tdg leverages llms, along with human-in-the-loop, to generate new data specifically tailored for these subgroups, thereby enriching the dataset and improving model performance in sentiment analysis and natural language inference tasks. to facilitate the clinical information extraction task, tang et al. (2023b) elicit diverse samples from llms by providing examples and different seeds of clinical entities, i.e. the curation manner. 20 several studies have also focused on multiple nlu tasks (ding et al., 2023a; he et al., 2023a; wang et al., 2021a; he et al., 2022; ye et al., 2022; meng et al., 2022). for example, he et al. (2023a) utilize the knowledge in gpt-3.5 to annotate inputs with labels and explanations for various nlu tasks, including user input and keyword relevance assessment, boolq, and wic. wang et al. (2021a) employ few-shot prompts to expand high-quality training data using gpt-3, i.e. the expansion manner. beyond merely employing a single approach to elicit nlp task knowledge, ding et al. (2023a) explore a combination of labeling ,ex- pansion , and curation methods to extract knowledge from gpt-3 for distilling data for both sequence- and token-level nlp tasks. 4.4.2 natural language generation natural language generation (nlg) is a key aspect of eval- uating the capabilities of llms, encompassing tasks such as summarization, machine translation, and other open-ended text generation tasks. known for their potent generative abilities and creativity, llms excel in these areas, making them prime sources for distilling knowledge into student models tailored for nlg tasks (xu et al., 2023c, 2024b; ramnath et al., 2023; agarwal et al., 2024). additionally, the knowledge distilled from llms can be effectively used for nlg task-specific data augmentation (jung et al., 2023; wang et al., 2021b; guo et al., 2023a; yang and nicolai, 2023; wang et al., 2023h; yang et al., 2023d). while the previous sections have focused on the works about open- ended generation and multi-turn dialogue, this part will specifically highlight the distillation techniques relevant to other nlg tasks. although automatic metrics often favor smaller, fine- tuned models in summarization tasks, human evaluators tend to prefer the summaries generated by llms. address- ing this discrepancy, xu et al. (2023c) develop a student sum- marization model by distilling a gptsumm dataset, which comprises over 4 million paragraph-summary pairs gener- ated by querying gpt-3.5. in a different approach, jung et al. (2023) introduce ‘impossible distillation,’ a method that creates high-quality summarization-specific dataset from weak teacher llms. this method involves training a stu- dent model on the generated dataset and enhancing its capabilities through self-knowledge. turning to the task of machine translation, where creating parallel corpora is tra- ditionally expensive and time-consuming, yang and nicolai (2023) propose a three-step distillation process. this process involves generating seeds of verbs and nouns, forming sen- tences, and then translating these sentences. their findings suggest that while the distilled dataset may lack diversity, it effectively improves the translation signal for training student translation models. to distill high-quality content- grounded data automatically, genie (yehudai et al., 2024) proposes a general methodology containing three key steps: (a) preparation of the content, (b) distillation of responses from a teacher llm corresponding to the content, and (c) filtering mechanism to ensure the quality and faithfulness of the generated data. genie demonstrates that student models trained through this distilled data can match or even surpass models trained on human-generated data.4.4.3 information retrieval information retrieval (ir) represents a crucial branch of computer science, focused on efficiently retrieving infor- mation relevant to user queries from extensive reposito- ries (cai et al., 2022; liu et al., 2022b; feng et al., 2023; shen et al., 2023b). a typical ir system encompasses three main components: the query rewriter, the retriever, and the reranker. recent studies have highlighted the effective- ness of employing llms in ir systems, e.g. in enhancing the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in ir systems faces challenges, primarily due to their slower generation speed, which conflicts with the low-latency re- quirements of ir tasks (sun et al., 2023a). as a result, the kd of llms emerges as a more promising approach for ir, offering a way to infuse the distilled knowledge from llms into various stages of the ir pipeline without compromising on speed. there has been a significant body of work demonstrating how knowledge distilled from llms can benefit each component of the ir system, including the query rewriter (srinivasan et al., 2022; ma et al., 2023c), the retriever (dai et al., 2023b; sachan et al., 2022, 2023; schick and sch ¨utze, 2021; meng et al., 2023; peng et al., 2023b), and thereranker (bonifacio et al., 2022; sun et al., 2023a; pradeep et al., 2023a,b; saad-falcon et al., 2023; ferraretto et al., 2023; jeronymo et al., 2023; sun et al., 2023c). query rewriter. the query rewriter (qr) is a pivotal com- ponent in ir systems, tasked with enhancing the precision and expressiveness of user queries by refining or modifying the initial query to more accurately align with the user’s information needs. one notable approach is quill (srini- vasan et al., 2022), which introduces a two-stage distillation method for query intent understanding. initially, a retrieval- augmented llm, serving as the ‘professor,’ is distilled into a non-retrieval augmented teacher llm, aiming to bolster its understanding capabilities. subsequently, this enhanced teacher llm is distilled into a final student model using a large dataset, further refining the process. incorporating the qr into ir systems, ma et al. (2023c) develop a ’rewrite- retrieve-read’ framework. this process begins with an llm rewriting the queries via prompting, followed by a retrieval-augmented reading stage. to integrate the rewrit- ten queries effectively into the ir system, the knowledge gleaned from the llm is distilled into a compact student rewriter. this rewriter is then fine-tuned using feedback from the llm reader through reinforcement learning. retriever and reranker. in ir systems, the retriever is designed to efficiently locate the top-k relevant texts from a large corpus. it encodes both queries and documents into vector representations and performs retrieval by computing the dot product between these vectors. the reranker further refines the order of the retrieved documents to improve the overall quality of the output. this is achieved in two primary ways, including pointwise reranker and listwise reranker . pointwise reranker takes both the query and a single candidate document as input to directly generate a relevance score. listwise reranker directly reorders a list of input documents in terms of their relevance. 21 retriever and pointwise reranker. for the retriever and pointwise reranker, a common application of kd from llms is the generation of pseudo-queries for given documents. this approach aims to expand the pairwise data, enhancing the training of dense retrievers or rerankers. for example, inpars (bonifacio et al., 2022) utilizes gpt-3 to generate multiple pseudo-queries for an unlabeled document. to ensure the relevance of these queries, the system filters them based on the highest log probabilities of generating a query conditioned on the documents. subsequently, inpars fine-tunes a reranker based on monot5 (raffel et al., 2020). another similar approach, promptagator (dai et al., 2023b), introduces a few-shot dense retrieval method that leverages a small number of demonstrations from the target domain for pseudo-query generation. diverging from the reliance on unlabeled documents, sachan et al. (2022) distill knowl- edge from gpt-4 to curate diverse synthetic data for text embedding tasks across nearly 100 languages. they fine- tune powerful decoder-only llms, such as mistral-7b (jiang et al., 2023a), on this synthetic data using standard con- trastive loss. remarkably, this method demonstrates strong performance on text embedding and multilingual retrieval benchmarks without any labeled data. beyond generating pseudo-queries, teacher llms can also be employed to gen- erate relevance scores as soft labels. these scores are used to train the retriever by minimizing the kl-divergence loss between the teacher and student distributions, as explored by sachan et al. (2023). listwise reranker. a distinct set of studies focuses on listwise reranking, where its advantage lies in compar- ing multiple documents simultaneously to determine the optimal reorder. rankgpt (sun et al., 2023a) leverages gpt-4 to generate permutations for a group of candidate passages. to distill this listwise ranking knowledge into a pointwise student reranker, various training loss functions are employed, such as listwise cross-entropy (bruch et al., 2019), ranknet (burges et al., 2005), and lambdaloss (wang et al., 2018). building upon rankgpt’s framework, rankvi- cuna (pradeep et al., 2023a) and rankzephyr (pradeep et al., 2023b) further refine this approach by directly fine- tuning a listwise reranker using teacher-generated textual permutations. this enables the student reranker to produce sequences of ranked results directly, bypassing the interme- diate step of calculating individual relevance scores. 4.4.4 recommendation recommender systems are integral to enhancing user ex- perience in various online services, providing personalized content based on user preferences and behaviors. many works have demonstrated that llms could be directly used as recommenders without fine-tuning (wang et al., 2023i; dai et al., 2023c) or generate auxiliary textual features to benefit recommender systems (xi et al., 2023; ren et al., 2023; wei et al., 2024). (wang et al., 2023j; ren et al., 2023; wei et al., 2024). however, the real-time nature of online rec- ommender systems demands rapid response times, posing a challenge with the inherent inference latency associated with llms. to address this, several studies have explored ways to distill and integrate the knowledge from llms into recommender systems, thereby leveraging their advanced capabilities while mitigating latency issues for efficient real-time recommendations (mysore et al., 2023; zhang et al., 2023b; liu et al., 2023c). mysore et al. (2023) tackle data scarcity in narrative- driven recommendation (ndr), where users provide de- tailed descriptions of their preferences. they utilize gpt-3 to create synthetic narrative queries from user-item interac- tions via few-shot prompting, then distill this data into re- trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized content, aiding the training of content-based recommenda- tion models. to bridge the gap between language models and recommender systems, some research views behavior modeling as an extension of language modeling (cui et al., 2022; liu et al., 2023k). instructrec (zhang et al., 2023b), for instance, interprets recommendation as instruction fol- lowing. they use chatgpt to distill a wealth of user- personalized instruction data reflecting diverse preferences and intentions based on real historical interactions. this data is then used to fine-tune a 3b student language model specifically for recommendation purposes. 4.4.5 text generation evaluation text generation evaluation, i.e. nlg evaluation, focuses on assessing the quality of generated content. unlike tradi- tional nlg evaluation metrics like bleu (papineni et al., 2002) or rouge (lin, 2004), which primarily rely on surface-level text comparisons, llms, trained on extensive corpora and refined through techniques like rlhf, offer a more nuanced and human-aligned assessment. this so- phistication has led to the increasing use of llms in nlg evaluation (detailed further in (li et al., 2024b)). through kd of llms, student evaluators could enhance inference efficiency and achieve more flexible and highly customized evaluation (wang et al., 2023b; kim et al., 2024; xu et al., 2023d; jiang et al., 2023c; li et al., 2024a). pandalm (wang et al., 2023b) concentrates on a pairwise evaluator designed to compare two pieces of generated content. it utilizes a teacher llm (gpt-3.5) to judge which response is better for a given instruction and input, provid- ing reasons for its decision. addressing the need for cus- tomized and flexible criteria to meet realistic user demands, prometheus (kim et al., 2024) distills gpt-4 to construct a training dataset that includes reference answers and a vari- ety of customized scoring rubrics. this dataset is then used to tune llama for evaluating model-generated responses. instructscore (xu et al., 2023d) takes a more fine-grained ap- proach by using gpt-4 to create detailed analysis data. this data is employed to tune llama, enabling it to perform error analysis on generated texts compared to reference texts. the system further refines its evaluation capabilities through self-training with real model-generated response- reference pairs. for reference-free evaluation across diverse domains, tigerscore (jiang et al., 2023c) samples data from a variety of text generation datasets, such as summariza- tion, translation, and data-to-text. it distills error analysis knowledge from gpt-4 and uses this to fine-tune llama for more nuanced evaluation. lastly, to adapt evaluation to real-world scenarios beyond conventional nlp tasks, auto-j (li et al., 2024a) collects real-world user queries and their evaluations from a teacher llm. this massive dataset 22 of real-world scenarios is then used to distill evaluation knowledge into llama through fine-tuning, enhancing its practical applicability. 4.4.6 code llms, trained on extensive corpora containing code, are highlighted for their proficiency in code-related tasks. their capabilities extend beyond direct code generation to include the provision of external knowledge and data, which is crucial in distilling their expertise into smaller, more effi- cient models. several works have successfully distilled code knowledge from llms into those compact and specialized code models (chaudhary, 2023; rozi `ere et al., 2023; gu- nasekar et al., 2023; wei et al., 2023; chen et al., 2023a; liu et al., 2023d; yu et al., 2024; jain et al., 2023; su and mcmillan, 2023; guo et al., 2023d). a primary focus in these student code models is on code generation, a task of both common utility and practical significance. for instance, code alpaca (chaudhary, 2023) fine-tunes llama using self-instruct with chatgpt-distilled instructions specifically for code generation tasks. similarly, code llama-instruct (rozi `ere et al., 2023) is fine-tuned via self-instruct, prompting llama-2 (touvron et al., 2023) with coding problems, and further refined with unit tests. phi- 1 (gunasekar et al., 2023) aims to enhance the quality of dis- tilled code data by extracting “textbook quality” data from a teacher llm, incorporating python textbook and exercise data. magicoder (wei et al., 2023) addresses potential biases in teacher llms by referencing a wealth of open-source code, yielding more diverse and grounded data for code generation. to consider the capability of the student model and leverage the feedback of the teacher, persd (chen et al., 2023a) introduces a personalized distillation method where the teacher llm refines the student’s generated code based on the execution feedback of the executor. however, these models primarily target the code gener- ation task, lacking generalizability across a broader range of code-related tasks. to address this issue, mftcoder (liu et al., 2023d) utilizes self-instruct to distill diverse code data from teacher llms for various tasks, such as code comple- tion and text-to-code generation, training a student model via multi-task learning. wavecoder (yu et al., 2024), in contrast, creates a comprehensive instruction tuning dataset covering four universal code-related tasks distilled from gpt-3.5-turbo. wavecoder first selects a diverse coreset of raw data using the kcentergreedy (sener and savarese, 2018) clustering method, then employs the teacher llm for generating task definitions and outputs. the teacher model also plays a role in evaluating and filtering this data. notably, wavecoder demonstrates superior generalization across different code-related tasks compared to other open- source models. 4.5 multi-modality multimodal large language models (mllms) surpass tra- ditional language-only llms by understanding and pro- cessing information across multiple modalities, more closely mirroring human perception and enabling a broader range of real-world applications. there is a growing trend towards developing mllms that follow multimodal instructions,facilitating tasks with enhanced levels of interactivity. to ad- dress the scarcity of multimodal instruction-following data and to harness the commonsense and world knowledge embedded in teacher llms, numerous studies have focused on multimodal knowledge distillation from llms (liu et al., 2023e; zhao et al., 2023b; wang et al., 2023e; chen et al., 2023c; park et al., 2023; pi et al., 2023; zhao et al., 2023c; liu et al., 2023f; wu et al., 2023b; luo et al., 2023d; jiang et al., 2023d; li et al., 2023c; xu et al., 2023e). vision-language. in the vision-language domain, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, including captions and bounding boxes, and distills gpt-4 for generating new data in the context of seed examples. this approach creates a llava-instruct-150k dataset, which serves as the foundation for further developments like llava-1.5 (liu et al., 2023l) and gpt4roi (zhang et al., 2023e), enhancing the instruction- following capabilities of mllms. to expand the dataset’s scale, svit (zhao et al., 2023b) introduces a 4.2 million image dataset, distilled from gpt-4 by leveraging manual image annotations. it employs a novel data recipe to select an informative, diverse, and balanced subset of training data. lvis-instruct4v (wang et al., 2023e) leverages gpt- 4v (openai, 2023), a powerful large multimodal model, as a teacher to distill a more accurate and context-aware instruction-following dataset, focusing on fine-grained understanding. further advancements include integrating specific region referencing in image-based instruction following. for instance, shikra (chen et al., 2023c) uses gpt-4 to distill referential question-answer pairs from the flickr30k (plummer et al., 2015) dataset, enhancing the understanding of referential regions within images. lskd (park et al., 2023) introduces localized references to specific image regions, prompting the teacher llm to generate commonsense inferences about these areas. to enhance the visual instruction tuning pipeline with text-rich images, llavar (zhang et al., 2023d) employs the text-only gpt-4 as a teacher, using recognized texts and image captions to generate 16k conversation pairs for text-rich images. the resultant student mllm demonstrates enhanced interaction skills in content that combines both text and imagery. multiple modalities. to extend knowledge distillation of llms to encompass more modalities, such as audio and video, several innovative approaches have been in- troduced. these methods typically involve transforming these modalities into a textual format comprehensible to teacher llms, followed by the distillation of the teacher. macaw-llm (lyu et al., 2023) leverages gpt-4 to generate instruction-response pairs corresponding to the content of images or videos. mimic-it (li et al., 2023f) aims to broaden the scope to language, image, and video understanding, creating a substantial dataset with 2.8 million multimodal instruction-response pairs distilled from chatgpt. chat- bridge (zhao et al., 2023d), on the other hand, represents a novel approach in multimodal language modeling. it translates various non-textual modalities into text, combin- ing fine-grained and global descriptions. this information 23 verticalization distillationlaw lawyerllama (huang et al., 2023b), lawgpt (cui et al., 2023b), fuzi (wu et al., 2023d) medical and healthcarehuatuogpt (zhang et al., 2023c), huatuogpt-ii (chen et al., 2023d), doctorglm (xiong et al., 2023), alpacare (zhang et al., 2023f), huatuo (wang et al., 2023a), chatdoctor (li et al., 2023i), medalpaca (han et al., 2023), pmc-llama (wu et al., 2023e), disc-medllm (bao et al., 2023a) finance xuanyuan (zhang and yang, 2023) sciencedarwin (xie et al., 2023a), sciglm (zhang et al., 2024), wizardmath (luo et al., 2023b), mammoth (yue et al., 2023a), tora (gou et al., 2024), astrollama-chat (perkowski et al., 2024), g-llava (gao et al., 2023c), gimlet (zhao et al., 2023f), llm-prop (rubungo et al., 2023), instructmol (cao et al., 2023a), prot2text (abdine et al., 2023), biomedgpt (luo et al., 2023e), xtrimopglm (chen et al., 2024e), k2 (deng et al., 2023), oceangpt (bi et al., 2023), marinegpt (zheng et al., 2023b), geogalactica (lin et al., 2024), miscellaneous educhat (dan et al., 2023), owl (guo et al., 2023b) fig. 7: taxonomy of verticalization distillation. is then used to distill responses from chatgpt or gpt-4 through an in-context learning process, effectively bridging the gap between different modalities. others. beyond distilling instruction-following data, sev- eral methods have emerged that concentrate on harnessing different aspects of knowledge from llms. for instance, emma (yang et al., 2023c) trains an mllm to act as an embodied reflex agent within a visual environment. it achieves this by distilling gpt-4’s skills in a parallel textual world, generating actions and providing reflective feedback. silkie (li et al., 2023h) takes a unique approach by distilling preferences from gpt-4v , focusing on criteria like helpfulness and visual faithfulness. ha et al. (2023) represent another innovative direction, where it generates, labels, and distills diverse robot-centric exploration experiences by llms into a multi-task visuo-linguo-motor policy. 5 d omain -specified vertical distillation this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc. it delves into cus- tomizing distilled llms for these fields, showing its signifi- cant role in enhancing domain-specific ai applications. the taxonomy of these works is shown in figure 7. 5.1 law law holds a crucial position in molding societies, over- seeing human interactions, and ensuring justice prevails. informed decision-making, legal interpretation, and the pro- vision of legal advice by professionals hinge on precise and current information. legal intelligent applications in different scenarios usually require combinations of multiple fundamental capabilities of legal text retrieval, understand- ing, reasoning and generating (zhang et al., 2023g; sun, 2023; lai et al., 2023). addressing the intricacies of legal ter- minology, subtle interpretations, and the constant evolution of legislation presents distinctive challenges that demand customized resolutions. to handle the above challenges, several studies have investigated the customization of llms for intelligent legal services (cui et al., 2023b; yue et al., 2023b; huang et al., 2023b; wu et al., 2023d). this involves a continued pre-training process on extensive legal corpora, followed by fine-tuning with self-constructed instructions or augmented data using advanced llms.huang et al. (2023b) have unveiled a chinese legal large model named lawyerllama. the model undergoes an initial pre-training phase on an extensive legal corpus, systematically assimilating knowledge of the chinese legal system. subsequently, fine-tuning occurs through the analy- sis of objective questions from the chinese national judicial examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge to specific scenarios. cui et al. (2023b) present lawgpt, built upon the foundation of openllama. the model is trained using a construction process that incorporates real- world legal text, legal regulations, judicial interpretations, and actual legal consultation data. additionally, the authors utilize the chatgpt api for assisted construction, enabling the generation of supplementary data derived from the existing dataset. wu et al. (2023d) have developed a large- scale chinese legal model (named fuzi) with chatglm as its foundation. this model undergoes training on an extensive chinese legal corpus, which incorporates unsu- pervised judicial language data, including diverse judgment documents and legal regulations. additionally, it undergoes supervised judicial fine-tuning with data encompassing le- gal qa and case retrieval. fuzi’s training also involves both general instruction fine-tuning datasets, such as alpaca, and domain-specific instruction fine-tuning datasets from lawyerllama (huang et al., 2023b) and lawgpt (cui et al., 2023b). 5.2 medical and healthcare the integration of llms carries substantial promise in fun- damentally reshaping the landscape of medical data anal- ysis, comprehension, and smart medical services (singhal et al., 2023; yang et al., 2024b). significant research endeav- ors have been dedicated to adapting general-purpose llms to the medical domain, given the ever-expanding wealth of information encompassing electronic health records, med- ical literature, and clinical data. especially in healthcare, llms are revolutionizing patient care, research, and admin- istrative efficiency. they enhance diagnostic accuracy by an- alyzing patient data and medical literature, offering person- alized recommendations, and identifying potential drug in- teractions. llms also streamline administrative tasks by au- tomating patient documentation and processing insurance claims, reducing the burden on healthcare providers and 24 improving patient experiences. furthermore, they facilitate medical research by synthesizing vast amounts of data to uncover new insights into diseases and treatments (will be discussed later). this integration of llms into healthcare is paving the way for more informed clinical decision-making, improved patient outcomes, and more efficient healthcare systems. these adaptations extend across a spectrum, ranging from refining the precision of medical diagnoses (wang et al., 2023k) and providing personalized treatment rec- ommendations (zhu et al., 2023) to automating routine administrative processes within healthcare settings. while existing studies predominantly concentrate on training using dedicated medical dialogue datasets com- prising medical textbooks (wu et al., 2023e), biomedical papers (luo et al., 2023e) medical knowledge-graphs (bao et al., 2023b), or authentic doctor-patient interactions (bao et al., 2023b), an expanding body of research is delving into the augmentation of medical instruction-following data with advanced llms to enhance the alignment of the intricacies within practical user instructions. zhang et al. (2023c) introduce huatuogpt specifically tailored for med- ical consultations. the model leverages both distilled data from chatgpt and real-world data from doctors during the supervised fine-tuning stage. in a parallel effort, xiong et al. (2023) construct a dataset of medical dialogues in chinese, employing chatgpt’s assistance. their method- ology encompassed various techniques to train doctor- glm, an easily deployable llm designed for tasks such as diagnoses, drug recommendations, and other medical advice. zhang et al. (2023f) fine-tune llama-series models using 52k diverse, machine-generated, medical instruction- following data named medinstruct-52k. this effort resulted in the development of alpacare, a model demonstrating robust medical proficiency and generalizability across both general and medical-specific domain free-form instruction evaluations. in a different vein, wang et al. (2023a) propose huatuo, a llama-based model that undergoes supervised fine-tuning with generated qa instances. this refinement process enhances the model’s possession of more reliable medical knowledge. li et al. (2023i) introduce chatdoctor, which was first trained as a generic conversation model based on llama. it utilized 52k instruction-following data from stanford university’s alpaca project (taori et al., 2023). subsequently, the conversation model underwent fine-tuning on a dataset of 100k patient-physician conver- sations collected from an online medical consultation web- site. this two-step training process underscores the model’s adaptability to diverse conversational contexts, particularly those specific to patient-physician interactions. built upon existing datasets, medalpaca (han et al., 2023) proposes to reconstruct the data with gpt-3.5-turbo, which is then used to fine-tune llms for effective medical applications. furthermore, pmc-llama (wu et al., 2023f) proposes a training framework (i.e., continual pre-training and domain-specific multi-task supervised fine-tuning) to adapt a general llm to the medicine domain, where gpt- 4 is leveraged to write synonymous sentences for data augmentation in the sft. to adapt llms to real-world medical consultation, disc-medllm (bao et al., 2023a) leverages gpt-3.5 to 1) construct 50k qa pairs in a few-shot manner and 2) re-generate the 420k dialogues based on real cases, which are then used to train llms in a supervised fine-tuning manner. more recently, huatuogpt- ii (chen et al., 2023d) proposes a one-stage training with instruction-formatting unification of domain data collection for medical adaption upon llms, where gpt-4 is used to formulate medical questions to fine-tuning instructions. these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- ploration of various methodologies, these approaches pro- vide valuable insights into the challenges and potential breakthroughs at the intersection of cutting-edge language models and medical applications. 5.3 finance the application of llms to the finance domain (xue et al., 2023) significantly transforms how financial data is ana- lyzed, decisions are made, and customer interactions are managed. in finance, llms offer unprecedented capabil- ities in understanding complex financial documents, pre- dicting market trends, and automating risk assessment, thus enabling more informed and faster decision-making processes. by processing and analyzing vast amounts of unstructured financial data, such as news articles, reports, and real-time market feeds, llms can identify patterns and insights that were previously inaccessible, leading to more accurate forecasts and strategic financial planning. furthermore, llms enhance customer experiences through personalized financial advice, automated customer service, and sophisticated chatbots that can handle intricate queries. this level of automation and insight has the potential to increase efficiency, reduce operational costs, and improve compliance and risk management practices in financial insti- tutions, making llms a transformative force in the finance sector. knowledge distillation from a proprietary llm is still under-explored, and most existing works focus on adapting llms to finance applications by continual pre-training on finance-specific corpora (wu et al., 2023g; lu et al., 2023) or fine-tuning in a supervised manner on multi-task finance- specific instructions (yang et al., 2023e; xie et al., 2023b; wang et al., 2023l). specifically, xuanyuan (zhang and yang, 2023) lever- ages self-instruct over seed data and self-qa over struc- tured/unstructured data to generate instruction data in the finance domain, which is used to train a finance llm. 5.4 science the integration of llms into the science domain (taylor et al., 2022; yin et al., 2023b) represents a paradigm shift in research, knowledge discovery, and the dissemination of scientific information. in science, llms are leveraged to digest and synthesize vast amounts of literature, aiding in the identification of new research opportunities and the ac- celeration of scientific breakthroughs. they facilitate the un- derstanding of complex scientific concepts by summarizing research papers, generating hypotheses, and even drafting research proposals and manuscripts, thus significantly re- ducing the time researchers spend on literature review and 25 enabling them to focus more on experimental work. llms also democratize access to scientific knowledge by pro- viding layperson summaries of complex research findings, making science more accessible to non-experts and fostering a broader public understanding of scientific advancements. by enhancing the efficiency of research workflows and fostering interdisciplinary collaborations, llms are poised to accelerate the pace of scientific discovery and innovation across various fields. to distill knowledge from an llm, darwin series (xie et al., 2023a) utilizes a semi self- instruct for instruction generation for science papers, which is then used to fine-tune an llm. sciglm (zhang et al., 2024) proposes to train a scientific llm, which prompts a teacher llm to generate detailed answers for unlabelled scientific questions, as well as a self-reflective critic-and- revise to improve data quality. besides the above knowledge distillation methods to adapt llms to science, we will also delve into how the distillation happens in sub-domains, e.g., mathematics, as- tronautics, chemistry, etc. mathematics. the application of llms within the sub- domain of mathematics heralds a transformative era in mathematical research, education, and problem-solving (azerbayev et al., 2023; yu et al., 2023b). llms in mathemat- ics facilitate the exploration and understanding of complex mathematical theories and problems by providing intuitive explanations, proofs, and solutions that can bridge the gap between advanced mathematical concepts and learn- ers at various levels. these models have shown potential in conjecturing new mathematical theorems and patterns, thus opening new avenues for research and discovery that might not have been readily accessible to humans alone. in education, they serve as personalized tutors, offering students step-by-step guidance through mathematical prob- lems and adapting explanations to the learner’s level of un- derstanding. this democratizes access to high-quality math- ematical education and fosters a deeper appreciation and understanding of mathematics among a broader audience. by enhancing collaborative efforts through the generation of new ideas and the simplification of complex concepts, llms are poised to significantly advance the field of math- ematics, making it more accessible, efficient, and innova- tive. wizardmath (luo et al., 2023b) enhances the mathe- matical reasoning capabilities of llama-2 by applying the novel reinforcement learning from evol-instruct feedback (rleif) method, significantly outperforming other open- source llms on the gsm8k and math benchmarks, as well as surpassing several closed-source llms including chatgpt-3.5 and minerva. mammoth (yue et al., 2023a) is a series of open-source llms specifically developed for gen- eral math problem-solving, achieving superior performance on nine mathematical reasoning datasets. utilizing a novel instruction tuning dataset called mathinstruct, which com- bines chain-of-thought and program-of-thought rationales, mammoth models demonstrate substantial improvements over existing models. tora (gou et al., 2024), a series of tool-integrated reasoning agents, significantly advances mathematical problem-solving by combining natural lan- guage reasoning with the use of external computational tools. it markedly outperforms existing open-source modelson 10 mathematical reasoning datasets, showcasing notable improvements over both rationale-based and program- based approaches, and introduces innovative training tech- niques such as output space shaping to enhance model rea- soning capabilities. g-llava (gao et al., 2023c) introduces a significant advancement in geometric problem-solving for llms by leveraging a multimodal approach that combines text and image data. this model, utilizing the geo170k dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. astronautics. the application of llms in astronau- tics (nguyen et al., 2023) propels the field forward. astrollama-chat (perkowski et al., 2024) is an ad- vancement of the astrollama model, leveraging a 7b- parameter llama-2 model and targeted continual pre- training on a curated astronomy corpus to enhance per- formance in astronomy-focused question-answering. this model demonstrates significant improvements in special- ized topic comprehension and introduces a chat-enabled version for the astronomy community, highlighting the effectiveness of domain-specific knowledge distillation in achieving superior performance on specialized topics. chemistry and materials science. the integration of llms into chemistry and materials science has revolutionized the way researchers approach the discovery and develop- ment of new compounds and materials. by analyzing vast datasets and scientific literature, llms can predict the prop- erties and behaviors of substances, significantly accelerating the innovation cycle. gimlet (zhao et al., 2023f), graph instruction based molecule zero-shot learning, is a novel approach to molecule property prediction that integrates graph and text data within a single language model framework, aiming to improve instruction-based zero-shot learning for molec- ular tasks. by leveraging a transformer mechanism with generalized position embedding and decoupled attention, gimlet significantly outperforms traditional molecule-text baselines in zero-shot learning scenarios, demonstrating the model’s effectiveness in generalizing from instructions to a broad range of molecule-related tasks without prior explicit task-specific training. llm-prop (rubungo et al., 2023), leveraging the t5 model, showcases how llms can outperform sota graph neural networks in predicting the physical and electronic properties of crystalline solids from text descriptions. this approach underscores the potential of text-based methods in materials science, offering significant improvements in prediction accuracy while also contribut- ing a benchmark dataset, textedge, to foster further re- search in this emerging field. instructmol (cao et al., 2023a) integrates multi-modal data, aligning molecular structures with natural language instructions for drug discovery tasks. through a novel two-stage instruction-tuning approach, it significantly enhances performance in molecule-related tasks, establishing a reliable molecular assistant that outper- forms existing llms and reduces the performance gap with specialized models. this demonstrates the value of multi- modal integration in developing versatile tools for complex domains like drug discovery. 26 biology. in the field of biology, particularly in the study of pro- teins, dna, and rna, llms are revolutionizing our under- standing of the fundamental molecules of life. by analyzing vast datasets of biological sequences and structures, llms can predict the three-dimensional shapes of proteins, poten- tial functions, and interactions at a scale and speed beyond traditional computational methods. this capability is critical for unraveling the complexities of biological systems, ad- vancing drug discovery by identifying targets and designing molecules with high precision, and understanding genetic diseases through the interpretation of genomic variations. prot2text (abdine et al., 2023) introduces a novel multi- modal framework for generating protein function descrip- tions in free text by combining gnns and llms. this approach, which integrates structural and sequential protein information, highlights the transformative impact of knowl- edge distillation through the fusion of gnns and llms for accurate protein function prediction, potentially revolu- tionizing research in bioinformatics and biological sciences. biomedgpt (luo et al., 2023e) introduces a multimodal generative pre-trained transformer specifically designed for the biomedicine domain, emphasizing the significance of aligning molecular, protein, and natural language modal- ities to enhance biomedical question-answering, molecule, and protein qa tasks. this framework showcases the critical role of knowledge distillation in bridging the gap between complex biological data and human language, thereby fa- cilitating groundbreaking advancements in drug discovery and therapeutic target identification. xtrimopglm (chen et al., 2024e), a unified 100b-scale pre-trained transformer model, addresses both protein understanding and genera- tion tasks by integrating autoencoding and autoregressive pre-training objectives. its significant advancements over existing models in 18 protein understanding benchmarks and its capability in de novo protein sequence generation highlight the model’s importance in advancing the field of protein science through knowledge distillation. geography, geology, and environmental science. the inte- gration of llms into geography, geology, and environmen- tal science is revolutionizing these fields by enhancing data analysis, predictive modeling, and interdisciplinary research (roberts et al., 2023; lin et al., 2023b; wang et al., 2023m). k2 (deng et al., 2023), the first-ever llm specialized in the geoscience domain, demonstrates the significant impact of knowledge distillation in vertical domain specialization. by adapting the general-domain llama-7b model with a 5.5b token geoscience corpus and introducing the geosig- nal instruction tuning dataset, k2 showcases enhanced performance in geoscience knowledge understanding and utilization. the model’s development highlights a novel approach to efficiently gather domain-specific data and align model responses to specialized user queries, underpin- ning the importance of domain-specified vertical distillation in advancing research and applications within geoscience. oceangpt (bi et al., 2023), introduced as the first llm for ocean science tasks, underscores the vital role of knowl- edge distillation in the vertical domain of oceanography. it leverages doinstruct, a novel framework for generating domain-specific instruction data through multi-agent col-laboration, and establishes oceanbench, a benchmark for evaluating llms in the ocean domain. the model’s comprehensive experiments demonstrate its superior capa- bility in understanding and generating knowledge for ocean science, showcasing the significant potential of targeted knowledge distillation in enhancing domain-specific model performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language model tailored for marine science. utilizing the marine-5m dataset, which includes over 5 million marine image-text pairs, marinegpt excels in providing detailed, accurate, and domain-specific responses. this advancement underscores the model’s ability to significantly enhance marine knowl- edge comprehension and application, emphasizing the crit- ical role of domain-specific distillation in bridging the gap between general-purpose models and specialized domain requirements. geogalactica (lin et al., 2024) represents a pioneering step in specializing llms for geoscience, lever- aging a 30 billion parameter model pre-trained on a vast geoscience corpus. this model, notable for being the largest of its kind within the geoscience domain, showcases the significant potential of knowledge distillation in fostering scientific discoveries by bridging artificial intelligence with geoscience research and applications. 5.5 miscellaneous the expansion of llms into various verticals beyond the ones previously discussed showcases their versatility and transformative potential across numerous industries and societal sectors. llms are being tailored to meet the spe- cific needs and challenges of different domains, from legal and governmental to entertainment and beyond, providing sophisticated natural language understanding, generation, and decision-making capabilities. education. educhat (dan et al., 2023) is a large-scale lan- guage model-based chatbot system designed for the educa- tion domain. it aims to revolutionize intelligent education by providing personalized, fair, and compassionate support to teachers, students, and parents. knowledge distillation is emphasized through the pre-training on an educational corpus and fine-tuning on custom instructions to activate education-specific functions like open question answering, essay assessment, and emotional support. educhat demon- strates the importance of domain-specified knowledge dis- tillation in enhancing the performance of llms within specific verticals, offering a significant contribution to in- telligent education technology. it operations. owl (guo et al., 2023b) is a specialized llm tailored for it operations, focusing on enhancing efficiency and analysis within this domain. the model leverages a unique owl-instruct dataset covering a broad range of it-related information, employing a mixture-of- adapter strategy for efficient domain-specific tuning. this approach significantly improves it operation tasks’ perfor- mance, highlighting the critical role of knowledge distilla- tion in adapting general llms to specialized fields such as it operations, thereby pushing forward the frontier in specialized ai applications within this sector. 27 6 o penproblems further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions. in the field of instruction tuning, one of the most commonly used methods for distillation, zhou et al. (2023a) propose that only 1000 human-curated high-quality data is enough for the alignment of llms, hypothesizing that llms have learned the required knowl- edge from pretraining and only a small amount of data is required for the alignment. its finding further raises a new question, how to automatically select the data for better distillation? chen et al. (2023e) directly apply chatgpt to rate each data sample together with explanations, and then the data is selected based on the rating. cao et al. (2023b) split the existing instruction-tuning datasets and trains a linear function to select the most effective data based on their statistical properties. li et al. (2023j) propose a data selection pipeline similar to self-distillation, in which the llm firstly learns from a small subset of the data to get the basic ability, and then further uses this learned model to rate for the original dataset. du et al. (2023b) propose to consider three aspects including quality, coverage, and necessity for the filtering process. li et al. (2023k) select instruction data by evaluating their one-shot improvement on a hold-out set. li et al. (2024f) recently propose superfiltering, which is able to utilize small language models like gpt2 to filter out the high-quality subset from a given high-quality dataset. despite the emergence of these works working on data fil- tering, how to efficiently select the optimal distillation data for llms, and how much data is required for distillation are still unsolved. reduce the distillation cost (lightweight methods) de- spite the remarkable abilities of the latest llms, their sig- nificant resource requirements underscore the urgent need to find efficient solutions to overcome these challenges. common ways to further reduce the distillation cost include model compression and efficient fine-tuning. in the realm of model compression, quantization (frantar et al., 2023; dettmers et al., 2022; kim et al., 2023c; tao et al., 2022b; yao et al., 2022; xiao et al., 2023), parameter pruning (ma et al., 2023d; zhang et al., 2023h; frantar and alistarh, 2023), and low-rank approximation (xu et al., 2023g; li et al., 2023l) are commonly utilized. in the realm of efficient fine-tuning, parameter efficient fine-tuning (hu et al., 2023b; liu et al., 2022c; wang et al., 2022b; hu et al., 2021; li and liang, 2021; liu et al., 2022d), and memory efficient fine-tuning (dettmers et al., 2023; kim et al., 2023d; malladi et al., 2024) are utilized. a detailed survey on efficient large language models can be found here in wan et al. (2024b). the problem that remains is how can we further compress the model and build effective distillation algorithms. multi-teacher distillation most of the existing distilled models are distilled from a single teacher model, how- ever, it is widely accepted that models trained with dif- ferent sources of data have various capabilities. thus a question arises: is it possible to distill knowledge from different teacher models into one student model? babyl- lama (timiryasov and tastet, 2023) proposes to distill the knowledge from both the gpt2 and llama into the small-size student models. ensemble-instruct (lee et al., 2023b) tries to generate both instructions and responses ensembled from several different llms with rougel as the indicator. fusellm (wan et al., 2024a) externalizes the collective knowledge and unique strengths by leveraging the genera- tive distributions of different llms aiming to train a student model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated in table 3, the majority of teacher llms are closed-source due to their advanced capabilities. consequently, current methodologies primarily focus on using the generations from these models as hard labels, training student models through simple supervised fine-tuning. however, beyond the straightforward imitation of output behaviors via hard labels, there is a growing interest in harnessing richer knowledge from teacher llms, including feedback and feature knowledge, as well as exploring diverse combina- tions of knowledge elicitation methods. as highlighted in thefeedback section, teachers can provide various types of feedback based on the student’s outputs (lee et al., 2023a; jiang et al., 2023b; chen et al., 2023a). similarly, the feature section discusses how knowledge based on features, such as logits serving as soft labels, can offer deeper, intrinsic insights into the teacher model (gu et al., 2024; agarwal et al., 2024). these explorations have demonstrated promis- ing outcomes, suggesting that access to a broader spectrum of knowledge can significantly enhance student model per- formance beyond what is achievable through simple sft distillation alone. this highlights the critical need for further research into varied knowledge extraction methods from teacher llms to augment the effectiveness of kd processes. overcoming catastrophic forgetting during distillation previous research has delved into the fine-tuning of llms to acquire the ability to follow instructions or transfer knowledge for forthcoming tasks, skills, or domains, lever- aging advancements in llm technology. nevertheless, in- vestigations have revealed that the continual fine-tuning of llms on particular datasets (skills, domains) can lead to a phenomenon known as catastrophic forgetting, wherein previously acquired knowledge and problem-solving abil- ities for earlier tasks are compromised (chen et al., 2023f; kotha et al., 2023; koloski et al., 2023; wu et al., 2024; luo et al., 2023f). earlier studies in machine learning and deep learning have investigated various techniques to help mitigate forgetting during the fine-tuning or continue learn- ing process, such as rehearsal, which entails periodically revisiting and training on past data (kirkpatrick et al., 2017; rostami et al., 2019; rolnick et al., 2019), as well as reg- ularization methods like elastic weight consolidation (lee et al., 2017), or dynamic architecture methods (mallya et al., 2018; wang et al., 2022c; hu et al., 2023c; chen et al., 2023f). to address the challenges of catastrophic forgetting and to enhance the diversity of generated instructions in knowl- edge distillation for llms, jiang et al. (2023b) randomly sample an instruction from the easy instructions and also prompt the generator to generate a new instruction that belongs to the same domain as the sampled one. in a similar vein, li et al. (2023m) study the problem of instruction- 28 tuning in multi-modal llms knowledge distillation and introduce a competitive distillation framework. the model tries to produce new instructions that differ in content but are similar in difficulty to the original pictures in the multi- modal augmentation phase, so as to alleviate catastrophic forgetting of the model and enhance the diversity of the instruction tuning pool. chen et al. (2023f) propose the lifelong-moe (mixture-of experts) architecture based on general language models, which dynamically adds model capacity via adding experts with regularized pretraining. additionally, the model also introduces implicit regulariza- tion via distillation of the knowledge from old experts and gatings to effectively preserve old knowledge. zeng et al. (2023b) propose a new generative-based rehearsal method as dirichlet continual learning (dcl). this method com- bines task distribution modeling and knowledge distillation to mitigate catastrophic forgetting without requiring access to the old data. to evaluate the effectiveness of instruction tuning in the context of continuous learning tasks, zhang et al. (2023i) introduce a more challenging yet practical problem called continual instruction tuning (cit) and also establish a benchmark suite consisting of learning and eval- uation protocols. although current research has explored some simple methods to alleviate knowledge forgetting dur- ing model fine-tuning or knowledge distillation processes, effectively avoiding catastrophic forgetting across domains and skills remains a challenging issue. how to retain the original model’s capabilities effectively during knowledge distillation or transfer processes is still a challenging prob- lem. trustworthy knowledge distillation trustworthiness in llms is paramount, encompassing attributes such as truth- fulness, safety, fairness, robustness, privacy, and adherence to machine ethics (sun et al., 2024a). the rapid advancement of llms brings to the forefront concerns regarding their trustworthiness, stemming from their complex outputs, the biases present in vast training datasets, and the potential inclusion of private information. current efforts in kd of llms primarily focus on distilling various skills from llms, with relatively little attention paid to trustworthiness aspects. existing studies tend to concentrate on a subset of trustworthiness aspects, such as helpfulness, honesty, and harmlessness (bai et al., 2022a; yang et al., 2024a; cui et al., 2023a). consequently, in the distillation process, student models may inherit issues related to trustworthiness from their teacher llms. as assessed in sun et al. (2024a), smaller open-source llms generally fall short of their proprietary counterparts in trustworthiness metrics. therefore, consid- ering trustworthiness alongside the distillation of capabil- ities into student models is crucial. it is imperative that future research on kd not only enhances the capabilities of student models but also ensures that broader aspects of trustworthiness are meticulously addressed. weak-to-strong distillation. the concept of “weak-to- strong generalization” in llms (burns et al., 2023) empha- sizes the potential to leverage weak supervision to elicit the advanced capabilities of more powerful models. this approach challenges the traditional distillation paradigm by suggesting that even with limited or imperfect supervision, it is possible to enhance the performance of llms sig-nificantly. this necessitates exploring innovative strategies that enable weaker models to guide the learning process of stronger ones effectively, highlighting the importance of developing methods that can bridge the gap between these models. such research could unlock new avenues for improving llms’ efficiency and effectiveness, making the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision can unlock the full capabilities of much stronger models. through experiments with pre-trained language models in the gpt-4 family across nlp , chess, and reward modeling tasks, it finds that finetuning strong models on weak labels leads to better performance than their weak supervisors, demonstrating weak-to-strong generalization. then, li et al. (2024g) introduce superfiltering, a method that employs smaller, weaker models like gpt-2 to select high-quality data for fine-tuning larger, more capable models such as llama2. this approach is rooted in discovering a strong consistency in evaluating instruction tuning data difficulty across models of varying sizes. more recently, ji et al. (2024) introduce aligner, a novel approach for aligning llms with human values and intentions by utilizing weak supervisory signals from smaller models to improve the performance of larger models. however, burns et al. (2023) find that achieving the full capabilities of strong models requires more than naive finetuning, suggesting the need for further research in this area. therefore, open questions still remain about 1) what are the theoretical and practical limits of weak-to-strong distillation? can weak supervision reliably extract and enhance the full spectrum of capabilities in stronger models across all domains, or are there inherent limitations based on model architecture or task specificity? 2) how do we identify or design the optimal weak su- pervisors for distilling knowledge into stronger models? is there a framework or criteria to predict which weak models would be most effective in guiding the learning process of more complex models for specific tasks? 3) to what extent are weak-to-strong distillation techniques transferable and scalable across different sizes and types of models? how can these methods be adapted to ensure efficacy and ef- ficiency in distilling knowledge from very large models to significantly smaller ones, especially in resource-constrained environments? self-alignment. aligning llms traditionally relies heavily on human or teacher llms to supply extensive preference data. consequently, the alignment of the student model is limited by the quantity of distilled preference data and the teacher’s capabilities. self-alignment offers a promising alternative, aiming to enhance alignment beyond the con- straints of teacher-provided preferences. in self-alignment, the student model endeavors to autonomously improve and align its responses with desired behaviors, including generating model-written feedback, critiques, and explana- tions. several studies have explored utilizing the student model’s inherent capabilities to generate knowledge for alignment (bai et al., 2022a; sun et al., 2024b; li et al., 2024c; yuan et al., 2024a). beyond merely producing improved responses (bai et al., 2022a; sun et al., 2024b), implemen- tations of self-alignment include employing the student as 29 its reward model to offer feedback (yuan et al., 2024a), a strategy that merges self-knowledge with feedback methods of eliciting knowledge. we advocate for increasingly lever- aging the student model itself to provide feedback, thereby enhancing self-alignment capabilities. this approach not only facilitates moving beyond traditional human/teacher preference-based rewards but also opens avenues for con- tinual self-improvement and alignment. 7 c onclusion and discussion this survey has traversed the expansive domain of knowl- edge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field. we have underscored the pivotal role of kd in democratizing access to the advanced capabilities of proprietary llms, thereby fostering a more equitable ai landscape. through meticulous examination, we have highlighted how kd serves as a bridge, enabling resource- constrained entities to benefit from the profound advance- ments in llms without the prohibitive costs associated with training and deploying state-of-the-art models. our exploration delineates the multifaceted approaches to kd, ranging from algorithmic innovations and skill en- hancement to domain-specific distillations. each segment reveals the nuanced complexities and potentialities inherent in tailoring distilled models to emulate the sophisticated un- derstandings and functionalities of their more cumbersome counterparts. notably, the integration of data augmentation strategies within kd processes emerges as a critical lever for enhancing distillation in this llm era, underscoring the synergistic potential between generating context-rich training data and the distillation endeavor. as we project into the future, several avenues for re- search beckon. the evolving landscape of ai, marked by rapid advancements in model architectures and training methodologies, presents both challenges and opportunities for kd. the quest for more efficient, transparent, and ethical ai models necessitates continued innovation in kd tech- niques, especially those that can navigate the delicate bal- ance between model fidelity, computational efficiency, and ethical considerations. furthermore, the exploration of kd in nascent areas such as weak-to-strong generalization, self- alignment, multi-modal llms, real-time adaptation, and personalized ai services promises to expand the horizons of what distilled models can achieve. therefore, knowledge distillation of llms stands at a critical juncture, embodying the potential to significantly influence the trajectory of ai development and application. as this survey elucidates, the concerted efforts of the re- search community in pushing the boundaries of kd will be instrumental in realizing the vision of accessible, efficient, and responsible ai for all. legal considerations for using llm outputs: impor- tantly, it’s crucial to note the legal implications of utilizing llm outputs, such as those from chatgpt4, llama5, etc. we strongly advocate compliance with the terms of use specified by the model providers, such as the restrictions on developing competitive products, and so on. 4. https://openai.com/policies/business-terms 5. https://llama.meta.com/llama-downloads/references l. ouyang, j. wu, x. jiang, d. almeida, c. wainwright, p . mishkin, c. zhang, s. agarwal, k. slama, a. ray et al. , “training language models to follow instructions with human feedback,” advances in neural information processing systems , vol. 35, pp. 27 730–27 744, 2022. openai, :, j. achiam, s. adler, s. agarwal, l. ahmad, i. akkaya, f. l. aleman, d. almeida, j. altenschmidt, s. altman, s. anadkat, r. avila, i. babuschkin, s. balaji, v . balcom, p . baltescu, h. bao, m. bavarian, j. belgum, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, a. cann, b. carey, c. carlson, r. carmichael, b. chan, c. chang, f. chantzis, d. chen, s. chen, r. chen, j. chen, m. chen, b. chess, c. cho, c. chu, h. w. chung, d. cummings, j. currier, y. dai, c. decareaux, t. degry, n. deutsch, d. deville, a. dhar, d. dohan, s. dowling, s. dunning, a. ecoffet, a. eleti, t. eloundou, d. farhi, l. fedus, n. felix, s. p . fishman, j. forte, i. fulford, l. gao, e. georges, c. gibson, v . goel, t. gogineni, g. goh, r. gontijo-lopes, j. gordon, m. grafstein, s. gray, r. greene, j. gross, s. s. gu, y. guo, c. hallacy, j. han, j. harris, y. he, m. heaton, j. heidecke, c. hesse, a. hickey, w. hickey, p . hoeschele, b. houghton, k. hsu, s. hu, x. hu, j. huizinga, s. jain, s. jain, j. jang, a. jiang, r. jiang, h. jin, d. jin, s. jomoto, b. jonn, h. jun, t. kaf- tan, łukasz kaiser, a. kamali, i. kanitscheider, n. s. keskar, t. khan, l. kilpatrick, j. w. kim, c. kim, y. kim, h. kirchner, j. kiros, m. knight, d. kokotajlo, łukasz kondraciuk, a. kondrich, a. konstantinidis, k. kosic, g. krueger, v . kuo, m. lampe, i. lan, t. lee, j. leike, j. leung, d. levy, c. m. li, r. lim, m. lin, s. lin, m. litwin, t. lopez, r. lowe, p . lue, a. makanju, k. mal- facini, s. manning, t. markov, y. markovski, b. mar- tin, k. mayer, a. mayne, b. mcgrew, s. m. mckin- ney, c. mcleavey, p . mcmillan, j. mcneil, d. medina, a. mehta, j. menick, l. metz, a. mishchenko, p . mishkin, v . monaco, e. morikawa, d. mossing, t. mu, m. murati, o. murk, d. m ´ely, a. nair, r. nakano, r. nayak, a. nee- lakantan, r. ngo, h. noh, l. ouyang, c. o’keefe, j. pa- chocki, a. paino, j. palermo, a. pantuliano, g. parascan- dolo, j. parish, e. parparita, a. passos, m. pavlov, a. peng, a. perelman, f. de avila belbute peres, m. petrov, h. p . de oliveira pinto, michael, pokorny, m. pokrass, v . pong, t. powell, a. power, b. power, e. proehl, r. puri, a. rad- ford, j. rae, a. ramesh, c. raymond, f. real, k. rimbach, c. ross, b. rotsted, h. roussez, n. ryder, m. saltarelli, t. sanders, s. santurkar, g. sastry, h. schmidt, d. schnurr, j. schulman, d. selsam, k. sheppard, t. sherbakov, j. shieh, s. shoker, p . shyam, s. sidor, e. sigler, m. simens, j. sitkin, k. slama, i. sohl, b. sokolowsky, y. song, n. staudacher, f. p . such, n. summers, i. sutskever, j. tang, n. tezak, m. thompson, p . tillet, a. tootoonchian, e. tseng, p . tuggle, n. turley, j. tworek, j. f. c. uribe, a. vallone, a. vijayvergiya, c. voss, c. wainwright, j. j. wang, a. wang, b. wang, j. ward, j. wei, c. weinmann, a. welihinda, p . welinder, j. weng, l. weng, m. wiethoff, d. willner, c. winter, s. wolrich, h. wong, l. workman, s. wu, j. wu, m. wu, k. xiao, t. xu, s. yoo, k. yu, 30 q. yuan, w. zaremba, r. zellers, c. zhang, m. zhang, s. zhao, t. zheng, j. zhuang, w. zhuk, and b. zoph, “gpt- 4 technical report,” 2023. g. team, r. anil, s. borgeaud, y. wu, j.-b. alayrac, j. yu, r. soricut, j. schalkwyk, a. m. dai, a. hauth et al. , “gemini: a family of highly capable multimodal models,” arxiv preprint arxiv:2312.11805 , 2023. j. wei, y. tay, r. bommasani, c. raffel, b. zoph, s. borgeaud, d. yogatama, m. bosma, d. zhou, d. metzler, e. h. chi, t. hashimoto, o. vinyals, p . liang, j. dean, and w. fedus, “emergent abilities of large language models,” trans. mach. learn. res. , vol. 2022, 2022. [online]. available: https://openreview.net/forum?id=yzksu5zdwd j. wei, x. wang, d. schuurmans, m. bosma, f. xia, e. chi, q. v . le, d. zhou et al. , “chain-of-thought prompting elicits reasoning in large language models,” advances in neural information processing systems , vol. 35, pp. 24 824– 24 837, 2022. x. xu, c. tao, t. shen, c. xu, h. xu, g. long, and j. guang lou, “re-reading improves reasoning in large language models,” 2024. p . liang, r. bommasani, t. lee, d. tsipras, d. soylu, m. yasunaga, y. zhang, d. narayanan, y. wu, a. kumar, b. newman, b. yuan, b. yan, c. zhang, c. cosgrove, c. d. manning, c. r ´e, d. acosta-navas, d. a. hudson, e. zelikman, e. durmus, f. ladhak, f. rong, h. ren, h. yao, j. wang, k. santhanam, l. j. orr, l. zheng, m. y ¨uksekg ¨on¨ul, m. suzgun, n. kim, n. guha, n. s. chatterji, o. khattab, p . henderson, q. huang, r. chi, s. m. xie, s. santurkar, s. ganguli, t. hashimoto, t. icard, t. zhang, v . chaudhary, w. wang, x. li, y. mai, y. zhang, and y. koreeda, “holistic evaluation of language models,” corr , vol. abs/2211.09110, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09110 x. wu, r. duan, and j. ni, “unveiling security, privacy, and ethical concerns of chatgpt,” journal of information and intelligence , 2023. h. touvron, l. martin, k. stone, p . albert, a. almahairi, y. babaei, n. bashlykov, s. batra, p . bhargava, s. bhosale, d. bikel, l. blecher, c. c. ferrer, m. chen, g. cucurull, d. esiobu, j. fernandes, j. fu, w. fu, b. fuller, c. gao, v . goswami, n. goyal, a. hartshorn, s. hosseini, r. hou, h. inan, m. kardas, v . kerkez, m. khabsa, i. kloumann, a. korenev, p . s. koura, m.-a. lachaux, t. lavril, j. lee, d. liskovich, y. lu, y. mao, x. martinet, t. mihaylov, p . mishra, i. molybog, y. nie, a. poulton, j. reizen- stein, r. rungta, k. saladi, a. schelten, r. silva, e. m. smith, r. subramanian, x. e. tan, b. tang, r. taylor, a. williams, j. x. kuan, p . xu, z. yan, i. zarov, y. zhang, a. fan, m. kambadur, s. narang, a. rodriguez, r. stojnic, s. edunov, and t. scialom, “llama 2: open foundation and fine-tuned chat models,” 2023. a. q. jiang, a. sablayrolles, a. mensch, c. bamford, d. s. chaplot, d. de las casas, f. bressand, g. lengyel, g. lam- ple, l. saulnier, l. r. lavaud, m.-a. lachaux, p . stock, t. l. scao, t. lavril, t. wang, t. lacroix, and w. e. sayed, “mistral 7b,” 2023. l. zheng, w. chiang, y. sheng, s. zhuang, z. wu, y. zhuang, z. lin, z. li, d. li, e. p . xing, h. zhang, j. e. gonzalez, and i. stoica, “judging llm-as-a-judge with mt-bench and chatbot arena,” corr , vol. abs/2306.05685, 2023. [online].available: https://doi.org/10.48550/arxiv.2306.05685 l. sun, y. huang, h. wang, s. wu, q. zhang, c. gao, y. huang, w. lyu, y. zhang, x. li, z. liu, y. liu, y. wang, z. zhang, b. kailkhura, c. xiong, c. xiao, c. li, e. xing, f. huang, h. liu, h. ji, h. wang, h. zhang, h. yao, m. kellis, m. zitnik, m. jiang, m. bansal, j. zou, j. pei, j. liu, j. gao, j. han, j. zhao, j. tang, j. wang, j. mitchell, k. shu, k. xu, k.-w. chang, l. he, l. huang, m. backes, n. z. gong, p . s. yu, p .-y. chen, q. gu, r. xu, r. ying, s. ji, s. jana, t. chen, t. liu, t. zhou, w. wang, x. li, x. zhang, x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. j. gou, b. yu, s. j. maybank, and d. tao, “knowledge distillation: a survey,” international journal of computer vision , vol. 129, pp. 1789–1819, 2021. m. gupta and p . agrawal, “compression of deep learning models for text: a survey,” acm transactions on knowledge discovery from data (tkdd) , vol. 16, no. 4, pp. 1–55, 2022. s. y. feng, v . gangal, j. wei, s. chandar, s. vosoughi, t. mi- tamura, and e. hovy, “a survey of data augmentation approaches for nlp,” arxiv preprint arxiv:2105.03075 , 2021. r. taori, i. gulrajani, t. zhang, y. dubois, x. li, c. guestrin, p . liang, and t. b. hashimoto, “stanford alpaca: an instruction-following llama model,” https://github.com/ tatsu-lab/stanford alpaca, 2023. y. gu, l. dong, f. wei, and m. huang, “minillm: knowledge distillation of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=5h0qf7ibzz r. agarwal, n. vieillard, y. zhou, p . stanczyk, s. r. garea, m. geist, and o. bachem, “on-policy distillation of language models: learning from self-generated mistakes,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=3zktaqxlhw w. yuan, r. y. pang, k. cho, s. sukhbaatar, j. xu, and j. weston, “self-rewarding language models,” 2024. z. chen, y. deng, h. yuan, k. ji, and q. gu, “self-play fine-tuning converts weak language models to strong language models,” 2024. y. huang, y. chen, z. yu, and k. mckeown, “in-context learning distillation: transferring few-shot learning abil- ity of pre-trained language models,” 2022. g. cui, l. yuan, n. ding, g. yao, w. zhu, y. ni, g. xie, z. liu, and m. sun, “ultrafeedback: boosting lan- guage models with high-quality feedback,” arxiv preprint arxiv:2310.01377 , 2023. s. mukherjee, a. mitra, g. jawahar, s. agarwal, h. palangi, and a. awadallah, “orca: progressive learning from complex explanation traces of gpt-4,” arxiv preprint arxiv:2306.02707 , 2023. b. ding, c. qin, l. liu, y. k. chia, b. li, s. joty, and l. bing, “is gpt-3 a good data annotator?” in acl (1) . asso- ciation for computational linguistics, 2023, pp. 11 173– 11 195. s. chaudhary, “code alpaca: an instruction-following llama model for code generation,” https://github.com/ sahil280114/codealpaca, 2023. h. wang, c. liu, n. xi, z. qiang, s. zhao, b. qin, and 31 t. liu, “huatuo: tuning llama model with chinese medi- cal knowledge,” arxiv preprint arxiv:2304.06975 , 2023. lawgpt . github, 2023. d. zhang, z. hu, s. zhoubian, z. du, k. yang, z. wang, y. yue, y. dong, and j. tang, “sciglm: training scientific language models with self-reflective instruction annotation and tuning,” corr , vol. abs/2401.07950, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401. 07950 w.-l. chiang, z. li, z. lin, y. sheng, z. wu, h. zhang, l. zheng, s. zhuang, y. zhuang, j. e. gonzalez, i. stoica, and e. p . xing, “vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” march 2023. [online]. available: https://lmsys.org/blog/2023-03-30- vicuna/ c. xu, q. sun, k. zheng, x. geng, p . zhao, j. feng, c. tao, and d. jiang, “wizardlm: empowering large language models to follow complex instructions,” arxiv preprint arxiv:2304.12244 , 2023. w. x. zhao, k. zhou, j. li, t. tang, x. wang, y. hou, y. min, b. zhang, j. zhang, z. dong, y. du, c. yang, y. chen, z. chen, j. jiang, r. ren, y. li, x. tang, z. liu, p . liu, j.-y. nie, and j.-r. wen, “a survey of large language models,” 2023. x. he, z. lin, y. gong, a. jin, h. zhang, c. lin, j. jiao, s. m. yiu, n. duan, w. chen et al. , “annollm: making large language models to be better crowdsourced annotators,” arxiv preprint arxiv:2303.16854 , 2023. y. wang, z. yu, z. zeng, l. yang, c. wang, h. chen, c. jiang, r. xie, j. wang, x. xie, w. ye, s. zhang, and y. zhang, “pandalm: an automatic evaluation benchmark for llm instruction tuning optimization,” 2023. c. hsieh, c. li, c. yeh, h. nakhost, y. fujii, a. ratner, r. krishna, c. lee, and t. pfister, “distilling step-by-step! outperforming larger language models with less training data and smaller model sizes,” in acl (findings) . associ- ation for computational linguistics, 2023, pp. 8003–8017. a. mitra, l. d. corro, s. mahajan, a. codas, c. simoes, s. agarwal, x. chen, a. razdaibiedina, e. jones, k. aggar- wal, h. palangi, g. zheng, c. rosset, h. khanpour, and a. awadallah, “orca 2: teaching small language models how to reason,” 2023. c. xu, d. guo, n. duan, and j. j. mcauley, “baize: an open- source chat model with parameter-efficient tuning on self- chat data,” in emnlp . association for computational linguistics, 2023, pp. 6268–6278. x. yue, x. qu, g. zhang, y. fu, w. huang, h. sun, y. su, and w. chen, “mammoth: building math generalist mod- els through hybrid instruction tuning,” arxiv preprint arxiv:2309.05653 , 2023. l. chenglin, c. qianglong, w. caiyu, and z. yin, “mixed distillation helps smaller language model better reason- ing,” 2023. y. wang, y. kordi, s. mishra, a. liu, n. a. smith, d. khashabi, and h. hajishirzi, “self-instruct: aligning language model with self generated instructions,” arxiv preprint arxiv:2212.10560 , 2022. z. sun, y. shen, q. zhou, h. zhang, z. chen, d. cox, y. yang, and c. gan, “principle-driven self-alignment of language models from scratch with minimal human supervision,” advances in neural information processingsystems , vol. 36, 2024. z. luo, c. xu, p . zhao, q. sun, x. geng, w. hu, c. tao, j. ma, q. lin, and d. jiang, “wizardcoder: empowering code large language models with evol-instruct,” arxiv preprint arxiv:2306.08568 , 2023. h. luo, q. sun, c. xu, p . zhao, j. lou, c. tao, x. geng, q. lin, s. chen, and d. zhang, “wizardmath: empower- ing mathematical reasoning for large language models via reinforced evol-instruct,” arxiv preprint arxiv:2308.09583 , 2023. h. dai, z. liu, w. liao, x. huang, y. cao, z. wu, l. zhao, s. xu, w. liu, n. liu, s. li, d. zhu, h. cai, l. sun, q. li, d. shen, t. liu, and x. li, “auggpt: leveraging chatgpt for text data augmentation,” 2023. z. he, m. t. ribeiro, and f. khani, “targeted data generation: finding and fixing model weaknesses,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8506–8520. [online]. available: https://aclanthology.org/2023.acl-long.474 n. ding, y. chen, b. xu, y. qin, s. hu, z. liu, m. sun, and b. zhou, “enhancing chat language models by scaling high-quality instructional conversations,” in emnlp . as- sociation for computational linguistics, 2023, pp. 3029– 3051. s. gunasekar, y. zhang, j. aneja, c. c. t. mendes, a. d. giorno, s. gopi, m. javaheripi, p . kauffmann, g. de rosa, o. saarikivi, a. salim, s. shah, h. s. behl, x. wang, s. bubeck, r. eldan, a. t. kalai, y. t. lee, and y. li, “textbooks are all you need,” 2023. y. li, s. bubeck, r. eldan, a. del giorno, s. gunasekar, and y. t. lee, “textbooks are all you need ii: phi-1.5 technical report,” arxiv preprint arxiv:2309.05463 , 2023. phi-2: the surprising power of small lan- guage models , december 2023. [online]. avail- able: https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/ y. wei, z. wang, j. liu, y. ding, and l. zhang, “magicoder: source code is all you need,” 2023. z. yu, x. zhang, n. shang, y. huang, c. xu, y. zhao, w. hu, and q. yin, “wavecoder: widespread and versatile en- hanced instruction tuning with refined data generation,” 2024. j. ye, j. gao, q. li, h. xu, j. feng, z. wu, t. yu, and l. kong, “zerogen: efficient zero-shot learning via dataset generation,” in emnlp . association for computational linguistics, 2022, pp. 11 653–11 669. j. gao, r. pi, y. lin, h. xu, j. ye, z. wu, w. zhang, x. liang, z. li, and l. kong, “self-guided noise-free data generation for efficient zero-shot learning,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=h5opjgd lo6 l. h. bonifacio, h. q. abonizio, m. fadaee, and r. f. nogueira, “inpars: data augmentation for information retrieval using large language models,” corr , vol. abs/2202.05144, 2022. [online]. available: https://arxiv. org/abs/2202.05144 i. timiryasov and j.-l. tastet, “baby llama: knowledge 32 distillation from an ensemble of teachers trained on a small dataset with no performance penalty,” in proceedings of the babylm challenge at the 27th conference on computational natural language learning , a. warstadt, a. mueller, l. choshen, e. wilcox, c. zhuang, j. ciro, r. mosquera, b. paranjabe, a. williams, t. linzen, and r. cotterell, eds. singapore: association for computational linguistics, dec. 2023, pp. 279–289. [online]. available: https://aclanthology.org/2023.conll- babylm.24 c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre- trained language models via quantization,” arxiv preprint arxiv:2203.10705 , 2022. z. liu, b. oguz, c. zhao, e. chang, p . stock, y. mehdad, y. shi, r. krishnamoorthi, and v . chandra, “llm-qat: data-free quantization aware training for large language models,” arxiv preprint arxiv:2305.17888 , 2023. y. bai, s. kadavath, s. kundu, a. askell, j. kernion, a. jones, a. chen, a. goldie, a. mirhoseini, c. mckinnon, c. chen, c. olsson, c. olah, d. hernandez, d. drain, d. gan- guli, d. li, e. tran-johnson, e. perez, j. kerr, j. mueller, j. ladish, j. landau, k. ndousse, k. lukosuite, l. lovitt, m. sellitto, n. elhage, n. schiefer, n. mercado, n. das- sarma, r. lasenby, r. larson, s. ringer, s. johnston, s. kravec, s. e. showk, s. fort, t. lanham, t. telleen- lawton, t. conerly, t. henighan, t. hume, s. r. bow- man, z. hatfield-dodds, b. mann, d. amodei, n. joseph, s. mccandlish, t. brown, and j. kaplan, “constitutional ai: harmlessness from ai feedback,” 2022. l. tunstall, e. beeching, n. lambert, n. rajani, k. rasul, y. belkada, s. huang, l. von werra, c. fourrier, n. habib et al. , “zephyr: direct distillation of lm alignment,” arxiv preprint arxiv:2310.16944 , 2023. j. hong, q. tu, c. chen, x. gao, j. zhang, and r. yan, “cyclealign: iterative distillation from black-box llm to white-box models for better human alignment,” arxiv preprint arxiv:2310.16271 , 2023. h. lee, s. phatale, h. mansoor, k. lu, t. mesnard, c. bishop, v . carbune, and a. rastogi, “rlaif: scaling reinforcement learning from human feedback with ai feedback,” arxiv preprint arxiv:2309.00267 , 2023. y. jiang, c. chan, m. chen, and w. wang, “lion: adversarial distillation of closed-source large language model,” arxiv preprint arxiv:2305.12870 , 2023. h. chen, a. saha, s. hoi, and s. joty, “personalized distillation: empowering open-sourced llms with adaptive learning for code generation,” in the 2023 conference on empirical methods in natural language processing , 2023. [online]. available: https://openreview. net/forum?id=alxwmbcnvn k. yang, d. klein, a. celikyilmaz, n. peng, and y. tian, “rlcd: reinforcement learning from contrastive distilla- tion for lm alignment,” in the twelfth international confer- ence on learning representations , 2024. [online]. available: https://openreview.net/forum?id=v3xxtxwki6 j. jung, p . west, l. jiang, f. brahman, x. lu, j. fisher, t. sorensen, and y. choi, “impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing,” 2023. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, andj. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 c. gulcehre, t. l. paine, s. srinivasan, k. konyushkova, l. weerts, a. sharma, a. siddhant, a. ahern, m. wang, c. gu, w. macherey, a. doucet, o. firat, and n. de freitas, “reinforced self-training (rest) for language modeling,” 2023. e. zelikman, y. wu, j. mu, and n. d. goodman, “star: boot- strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,” arxiv preprint arxiv:1910.01108 , 2019. y. wen, z. li, w. du, and l. mou, “f-divergence minimization for sequence-level knowledge distillation,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 10 817–10 834. [online]. available: https: //aclanthology.org/2023.acl-long.605 c. liang, s. zuo, q. zhang, p . he, w. chen, and t. zhao, “less is more: task-aware layer-wise distillation for lan- guage model compression,” in international conference on machine learning . pmlr, 2023, pp. 20 852–20 867. m. kwon, s. m. xie, k. bullard, and d. sadigh, “reward de- sign with language models,” in iclr . openreview.net, 2023. b. peng, c. li, p . he, m. galley, and j. gao, “instruction tuning with gpt-4,” 2023. g. li, h. a. a. k. hammoud, h. itani, d. khizbullin, and b. ghanem, “camel: communicative agents for” mind” exploration of large scale language model society,” arxiv preprint arxiv:2303.17760 , 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, “openchat: advancing open-source language models with mixed-quality data,” sep. 2023, arxiv:2309.11235 [cs]. [online]. available: http://arxiv.org/abs/2309.11235 m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” arxiv preprint arxiv:2305.18395 , 2023. h. luo, y.-s. chuang, y. gong, t. zhang, y. kim, x. wu, d. fox, h. meng, and j. glass, “sail: search-augmented in- struction learning,” arxiv preprint arxiv:2305.15225 , 2023. a. asai, z. wu, y. wang, a. sil, and h. hajishirzi, “self- rag: learning to retrieve, generate, and critique through self-reflection,” arxiv preprint arxiv:2310.11511 , 2023. s. ye, y. jo, d. kim, s. kim, h. hwang, and m. seo, “selfee: iterative self-revising llm empowered by self-feedback generation,” blog post, may 2023. [online]. available: https://kaistai.github.io/selfee/ p . wang, l. li, l. chen, f. song, b. lin, y. cao, t. liu, and z. sui, “making large language models better reasoners with alignment,” 2023. d. cheng, s. huang, and f. wei, “adapting large language models via reading comprehension,” 2023. y. zhang, z. chen, y. fang, l. cheng, y. lu, f. li, w. zhang, 33 and h. chen, “knowledgeable preference alignment for llms in domain-specific question answering,” 2023. j. scheurer, j. a. campos, t. korbak, j. s. chan, a. chen, k. cho, and e. perez, “training language models with language feedback at scale,” 2023. s. kim, s. bae, j. shin, s. kang, d. kwak, k. yoo, and m. seo, “aligning large language models through synthetic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 677–13 700. [online]. available: https://aclanthology. org/2023.emnlp-main.844 p . roit, j. ferret, l. shani, r. aharoni, g. cideron, r. dadashi, m. geist, s. girgin, l. hussenot, o. keller, n. momchev, s. ramos garea, p . stanczyk, n. vieillard, o. bachem, g. elidan, a. hassidim, o. pietquin, and i. szpektor, “factually consistent summarization via reinforcement learning with textual entailment feedback,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 6252–6272. [online]. available: https://aclanthology.org/2023.acl-long.344 y. yang, e. chern, x. qiu, g. neubig, and p . liu, “alignment for honesty,” arxiv preprint arxiv:2312.07000 , 2023. r. liu, r. yang, c. jia, g. zhang, d. zhou, a. m. dai, d. yang, and s. vosoughi, “training socially aligned lan- guage models on simulated social interactions,” 2023. t. schick, j. dwivedi-yu, r. dess `ı, r. raileanu, m. lomeli, l. zettlemoyer, n. cancedda, and t. scialom, “tool- former: language models can teach themselves to use tools,” 2023. j. zhang, “graph-toolformer: to empower llms with graph reasoning ability via prompt augmented by chatgpt,” arxiv preprint arxiv:2304.11116 , 2023. s. g. patil, t. zhang, x. wang, and j. e. gonzalez, “gorilla: large language model connected with massive apis,” 2023. q. tang, z. deng, h. lin, x. han, q. liang, b. cao, and l. sun, “toolalpaca: generalized tool learning for lan- guage models with 3000 simulated cases,” 2023. y. qin, s. liang, y. ye, k. zhu, l. yan, y. lu, y. lin, x. cong, x. tang, b. qian, s. zhao, l. hong, r. tian, r. xie, j. zhou, m. gerstein, d. li, z. liu, and m. sun, “toolllm: facilitating large language models to master 16000+ real- world apis,” 2023. l. yuan, y. chen, x. wang, y. r. fung, h. peng, and h. ji, “craft: customizing llms by creating and retrieving from specialized toolsets,” 2023. s. gao, z. shi, m. zhu, b. fang, x. xin, p . ren, z. chen, j. ma, and z. ren, “confucius: iterative tool learning from introspection feedback by easy-to-difficult curriculum,” 2023. c. wang, w. luo, q. chen, h. mai, j. guo, s. dong, xiaohua, xuan, z. li, l. ma, and s. gao, “mllm-tool: a multimodal large language model for tool agent learning,” 2024. w. shen, c. li, h. chen, m. yan, x. quan, h. chen, j. zhang, and f. huang, “small llms are weak tool learners: a multi- llm agent,” 2024.b. chen, c. shu, e. shareghi, n. collier, k. narasimhan, and s. yao, “fireact: toward language agent fine-tuning,” 2023. a. zeng, m. liu, r. lu, b. wang, x. liu, y. dong, and j. tang, “agenttuning: enabling generalized agent abilities for llms,” 2023. d. yin, f. brahman, a. ravichander, k. chandu, k.-w. chang, y. choi, and b. y. lin, “lumos: learning agents with unified data, modular design, and open-source llms,” 2023. s. qiao, n. zhang, r. fang, y. luo, w. zhou, y. e. jiang, c. lv, and h. chen, “autoact: automatic agent learning from scratch via self-planning,” 2024. y. kong, j. ruan, y. chen, b. zhang, t. bao, s. shi, g. du, x. hu, h. mao, z. li, x. zeng, and r. zhao, “tptu-v2: boosting task planning and tool usage of large language model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” proceedings of the national academy of sciences , vol. 120, no. 30, jul. 2023. [online]. available: http: //dx.doi.org/10.1073/pnas.2305016120 z. wang, a. w. yu, o. firat, and y. cao, “towards zero-label language learning,” 2021. y. xu, r. xu, d. iter, y. liu, s. wang, c. zhu, and m. zeng, “inheritsumm: a general, versatile and compact summarizer by distilling from gpt,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 879–13 892. [online]. available: https: //aclanthology.org/2023.findings-emnlp.927 f. xu, w. shi, and e. choi, “recomp: improving retrieval- augmented lms with context compression and selective augmentation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=mljlvignhp s. ramnath, b. joshi, s. hallinan, x. lu, l. h. li, a. chan, j. hessel, y. choi, and x. ren, “tailoring self-rationalizers with multi-reward distillation,” 2023. s. wang, y. liu, y. xu, c. zhu, and m. zeng, “want to reduce labeling cost? gpt-3 can help,” in findings of the association for computational linguistics: emnlp 2021 , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 4195–4205. [online]. available: https: //aclanthology.org/2021.findings-emnlp.354 z. guo, p . wang, y. wang, and s. yu, “improving small language models on pubmedqa via generative data aug- mentation,” 2023. w. yang and g. nicolai, “neural machine translation data generation and augmentation using chatgpt,” 2023. k. srinivasan, k. raman, a. samanta, l. liao, l. bertelli, and m. bendersky, “quill: query intent with large language models using retrieval augmentation and multi-stage distillation,” in proceedings of the 2022 conference on empirical methods in natural language processing: industry track , y. li and a. lazaridou, eds. abu dhabi, uae: association for computational linguistics, dec. 2022, pp. 492–501. [online]. available: 34 https://aclanthology.org/2022.emnlp-industry.50 z. dai, v . y. zhao, j. ma, y. luan, j. ni, j. lu, a. bakalov, k. guu, k. b. hall, and m. chang, “promptagator: few-shot dense retrieval from 8 examples,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=gml46ympu2j r. meng, y. liu, s. yavuz, d. agarwal, l. tu, n. yu, j. zhang, m. bhat, and y. zhou, “augtriever: unsupervised dense retrieval by scalable data augamentation,” 2023. w. sun, l. yan, x. ma, s. wang, p . ren, z. chen, d. yin, and z. ren, “is chatgpt good at search? investigating large language models as re-ranking agents,” 2023. r. pradeep, s. sharifymoghaddam, and j. lin, “rankvicuna: zero-shot listwise document reranking with open-source large language models,” 2023. ——, “rankzephyr: effective and robust zero-shot listwise reranking is a breeze!” 2023. f. ferraretto, t. laitz, r. lotufo, and r. nogueira, “exaranker: synthetic explanations improve neural rankers,” in proceedings of the 46th international acm sigir conference on research and development in information retrieval , ser. sigir ’23. new york, ny, usa: association for computing machinery, 2023, p. 2409–2414. [online]. available: https://doi.org/10.1145/3539618.3592067 s. mysore, a. mccallum, and h. zamani, “large language model augmented narrative driven recommendations,” inproceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 777–783. [online]. available: https://doi.org/10.1145/3604915.3608829 j. zhang, r. xie, y. hou, w. x. zhao, l. lin, and j.-r. wen, “recommendation as instruction following: a large language model empowered recommendation approach,” 2023. q. liu, n. chen, t. sakai, and x.-m. wu, “once: boost- ing content-based recommendation with both open- and closed-source large language models,” 2023. s. kim, j. shin, y. cho, j. jang, s. longpre, h. lee, s. yun, s. shin, s. kim, j. thorne, and m. seo, “prometheus: inducing evaluation capability in language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=8eujatvekw w. xu, d. wang, l. pan, z. song, m. freitag, w. wang, and l. li, “instructscore: towards explainable text generation evaluation with automatic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5967–5994. [online]. available: https://aclanthology.org/2023.emnlp-main.365 d. jiang, y. li, g. zhang, w. huang, b. y. lin, and w. chen, “tigerscore: towards building explainable metric for all text generation tasks,” 2023. j. li, s. sun, w. yuan, r.-z. fan, hai zhao, and p . liu, “generative judge for evaluating alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=gtkfw6szgsb. rozi `ere, j. gehring, f. gloeckle, s. sootla, i. gat, x. e. tan, y. adi, j. liu, t. remez, j. rapin, a. kozhevnikov, i. evtimov, j. bitton, m. bhatt, c. c. ferrer, a. grattafiori, w. xiong, a. d ´efossez, j. copet, f. azhar, h. touvron, l. martin, n. usunier, t. scialom, and g. synnaeve, “code llama: open foundation models for code,” 2023. b. liu, c. chen, c. liao, z. gong, h. wang, z. lei, m. liang, d. chen, m. shen, h. zhou, h. yu, and j. li, “mftcoder: boosting code llms with multitask fine-tuning,” 2023. n. jain, t. zhang, w. chiang, j. e. gonzalez, k. sen, and i. stoica, “llm-assisted code cleaning for training accurate code generators,” corr , vol. abs/2311.14904, 2023. [online]. available: https://doi.org/10.48550/ arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up visual instruction tuning,” 2023. j. wang, l. meng, z. weng, b. he, z. wu, and y.-g. jiang, “to see is to believe: prompting gpt-4v for better visual instruction tuning,” 2023. k. chen, z. zhang, w. zeng, r. zhang, f. zhu, and r. zhao, “shikra: unleashing multimodal llm’s referential dialogue magic,” 2023. j. s. park, j. hessel, k. r. chandu, p . p . liang, x. lu, p . west, y. yu, q. huang, j. gao, a. farhadi, and y. choi, “localized symbolic knowledge distillation for visual commonsense models,” 2023. r. pi, j. gao, s. diao, r. pan, h. dong, j. zhang, l. yao, j. han, h. xu, l. kong, and t. zhang, “detgpt: detect what you need via reasoning,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 14 172–14 189. [online]. available: https: //aclanthology.org/2023.emnlp-main.876 l. zhao, e. yu, z. ge, j. yang, h. wei, h. zhou, j. sun, y. peng, r. dong, c. han, and x. zhang, “chatspot: bootstrapping multimodal llms via precise referring in- struction tuning,” 2023. f. liu, k. lin, l. li, j. wang, y. yacoob, and l. wang, “mitigating hallucination in large multi-modal models via robust instruction tuning,” 2023. s. wu, h. fei, l. qu, w. ji, and t.-s. chua, “next-gpt: any- to-any multimodal llm,” 2023. r. luo, z. zhao, m. yang, j. dong, d. li, p . lu, t. wang, l. hu, m. qiu, and z. wei, “valley: video assistant with large language model enhanced ability,” 2023. y. jiang, e. schoop, a. swearngin, and j. nichols, “iluvui: instruction-tuned language-vision modeling of uis from machine conversations,” 2023. y. li, c. zhang, g. yu, z. wang, b. fu, g. lin, c. shen, l. chen, and y. wei, “stablellava: enhanced visual in- struction tuning with synthesized image-dialogue data,” 2023. r. xu, x. wang, t. wang, y. chen, j. pang, and d. lin, “pointllm: empowering large language models to under- stand point clouds,” 2023. q. huang, m. tao, z. an, c. zhang, c. jiang, z. chen, z. wu, and y. feng, “lawyer llama technical report,” arxiv preprint arxiv:2305.15062 , 2023. 35 j. cui, z. li, y. yan, b. chen, and l. yuan, “chatlaw: open- source legal large language model with integrated ex- ternal knowledge bases,” arxiv preprint arxiv:2306.16092 , 2023. h. zhang, j. chen, f. jiang, f. yu, z. chen, g. chen, j. li, x. wu, z. zhiyi, q. xiao, x. wan, b. wang, and h. li, “huatuogpt, towards taming language model to be a doctor,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 10 859– 10 885. [online]. available: https://aclanthology.org/ 2023.findings-emnlp.725 j. chen, x. wang, a. gao, f. jiang, s. chen, h. zhang, d. song, w. xie, c. kong, j. li, x. wan, h. li, and b. wang, “huatuogpt-ii, one-stage training for medical adaption of llms,” corr , vol. abs/2311.09774, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09774 x. zhang and q. yang, “xuanyuan 2.0: a large chinese financial chat model with hundreds of billions parameters,” in proceedings of the 32nd acm international conference on information and knowledge management, cikm 2023, birmingham, united kingdom, october 21- 25, 2023 , i. frommholz, f. hopfgartner, m. lee, m. oakes, m. lalmas, m. zhang, and r. l. t. santos, eds. acm, 2023, pp. 4435–4439. [online]. available: https://doi.org/10.1145/3583780.3615285 t. xie, y. wan, w. huang, z. yin, y. liu, s. wang, q. linghu, c. kit, c. grazian, w. zhang, i. razzak, and b. hoex, “darwin series: domain specific large language models for natural science,” corr , vol. abs/2308.13565, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2308.13565 y. dan, z. lei, y. gu, y. li, j. yin, j. lin, l. ye, z. tie, y. zhou, y. wang, a. zhou, z. zhou, q. chen, j. zhou, l. he, and x. qiu, “educhat: a large-scale language model-based chatbot system for intelligent education,” corr , vol. abs/2308.02773, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.02773 h. guo, j. yang, j. liu, l. yang, l. chai, j. bai, j. peng, x. hu, c. chen, d. zhang, x. shi, t. zheng, l. zheng, b. zhang, k. xu, and z. li, “owl: a large language model for it operations,” corr , vol. abs/2309.09298, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.09298 y. kim and a. m. rush, “sequence-level knowledge distil- lation,” arxiv preprint arxiv:1606.07947 , 2016. s. han, h. mao, and w. j. dally, “deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding,” international confer- ence on learning representations (iclr) , 2016. v . gangal, s. y. feng, m. alikhani, t. mitamura, and e. hovy, “nareor: the narrative reordering problem,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 10 645–10 653. s. longpre, y. lu, z. tu, and c. dubois, “an exploration of data augmentation and sampling techniques for domain- agnostic question answering,” in proceedings of the 2nd workshop on machine reading for question answering , a. fisch, a. talmor, r. jia, m. seo, e. choi, and d. chen, eds. hong kong, china: association for computational linguistics, nov. 2019, pp. 220–227. [online]. available:https://aclanthology.org/d19-5829 p . west, c. bhagavatula, j. hessel, j. hwang, l. jiang, r. le bras, x. lu, s. welleck, and y. choi, “symbolic knowledge distillation: from general language models to commonsense models,” in proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 4602–4625. [online]. available: https://aclanthology.org/2022.naacl-main.341 z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, j. qian, b. peng, y. mao, w. chen, and x. yan, “explana- tions from large language models make small reasoners better,” 2022. n. ho, l. schmid, and s. yun, “large language models are reasoning teachers,” in acl (1) . association for computational linguistics, 2023, pp. 14 852–14 882. l. c. magister, j. mallinson, j. adamek, e. malmi, and a. severyn, “teaching small language models to reason,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: short papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 1773–1781. [online]. available: https://aclanthology.org/2023.acl-short.151 y. fu, h. peng, l. ou, a. sabharwal, and t. khot, “specializ- ing smaller language models towards multi-step reason- ing,” 2023. l. h. li, j. hessel, y. yu, x. ren, k.-w. chang, and y. choi, “symbolic chain-of-thought distillation: small models can also “think” step-by-step,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2665–2679. [online]. available: https://aclanthology.org/2023.acl- long.150 w. liu, g. li, k. zhang, b. du, q. chen, x. hu, h. xu, j. chen, and j. wu, “mind’s mirror: distilling self- evaluation capability and comprehensive thinking from large language models,” 2023. s. longpre, l. hou, t. vu, a. webson, h. w. chung, y. tay, d. zhou, q. v . le, b. zoph, j. wei et al. , “the flan collec- tion: designing data and methods for effective instruction tuning,” arxiv preprint arxiv:2301.13688 , 2023. y. anand, z. nussbaum, b. duderstadt, b. schmidt, and a. mulyar, “gpt4all: training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo,” github , 2023. q. si, t. wang, z. lin, x. zhang, y. cao, and w. wang, “an empirical study of instruction-tuning large language models in chinese,” in emnlp (findings) . association for computational linguistics, 2023, pp. 4086–4107. y. ji, y. deng, y. gong, y. peng, q. niu, l. zhang, b. ma, and x. li, “exploring the impact of instruction data scaling on large language models: an empirical study on real-world use cases,” 2023. m. wu, a. waheed, c. zhang, m. abdul-mageed, and a. f. 36 aji, “lamini-lm: a diverse herd of distilled models from large-scale instructions,” 2023. w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” 2023. y. yu, y. zhuang, j. zhang, y. meng, a. ratner, r. krishna, j. shen, and c. zhang, “large language model as at- tributed training data generator: a tale of diversity and bias,” 2023. f. wan, x. huang, d. cai, x. quan, w. bi, and s. shi, “knowledge fusion of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=jidsk12qcz q. zhao and b. zhu, “towards the fundamental limits of knowledge transfer over finite domains,” inneurips 2023 workshop on mathematics of modern machine learning , 2023. [online]. available: https: //openreview.net/forum?id=9qxoxqxa0n c. qin, w. xia, f. jiao, and s. joty, “improving in-context learning via bidirectional alignment,” 2023. n. boizard, k. el-haddad, c. hudelot, and p . colombo, “towards cross-tokenizer distillation: the universal logit distillation loss for llms,” arxiv preprint arxiv:2402.12030 , 2024. q. zhong, l. ding, l. shen, j. liu, b. du, and d. tao, “revis- iting knowledge distillation for autoregressive language models,” 2024. m. kim, s. lee, j. lee, s. hong, d.-s. chang, w. sung, and j. choi, “token-scaled logit distillation for ternary weight generative language models,” arxiv preprint arxiv:2308.06744 , 2023. z. chen, k. zhou, w. x. zhao, j. wan, f. zhang, d. zhang, and j.-r. wen, “improving large language models via fine- grained reinforcement learning with minimum editing constraint,” 2024. g. guo, r. zhao, t. tang, x. zhao, and j.-r. wen, “beyond imitation: leveraging fine-grained quality signals for alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=lnlju5c5dk z. allen-zhu and y. li, “towards understanding ensemble, knowledge distillation and self-distillation in deep learn- ing,” arxiv preprint arxiv:2012.09816 , 2020. t. zheng, s. guo, x. qu, j. guo, w. zhang, x. du, c. lin, w. huang, w. chen, j. fu et al. , “kun: answer polish- ment for chinese self-alignment with instruction back- translation,” arxiv preprint arxiv:2401.06477 , 2024. x. li, p . yu, c. zhou, t. schick, o. levy, l. zettlemoyer, j. e. weston, and m. lewis, “self-alignment with instruction backtranslation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=1oijhjbrst b. zhao, h. hajishirzi, and q. cao, “apt: adaptive pruning and tuning pretrained language models for efficient train- ing and inference,” arxiv preprint arxiv:2401.12200 , 2024. a. singh, j. d. co-reyes, r. agarwal, a. anand, p . patil, p . j. liu, j. harrison, j. lee, k. xu, a. parisi et al. , “beyond hu- man data: scaling self-training for problem-solving with language models,” arxiv preprint arxiv:2312.06585 , 2023. w. chen, d. song, and b. li, “grath: gradual self-truthifyingfor large language models,” 2024. a. hosseini, x. yuan, n. malkin, a. courville, a. sordoni, and r. agarwal, “v-star: training verifiers for self-taught reasoners,” 2024. a. askell, y. bai, a. chen, d. drain, d. ganguli, t. henighan, a. jones, n. joseph, b. mann, n. dassarma, n. elhage, z. hatfield-dodds, d. hernandez, j. kernion, k. ndousse, c. olsson, d. amodei, t. brown, j. clark, s. mccandlish, c. olah, and j. kaplan, “a general lan- guage assistant as a laboratory for alignment,” 2021. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, and j. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 h. chen, x. quan, h. chen, m. yan, and j. zhang, “knowl- edge distillation for closed-source language models,” arxiv preprint arxiv:2401.07013 , 2024. i. sason and s. verd ´u, “f-divergence inequalities,” ieee transactions on information theory , vol. 62, no. 11, pp. 5973– 6006, 2016. s. sun, y. cheng, z. gan, and j. liu, “patient knowledge distillation for bert model compression,” 2019. z. sun, h. yu, x. song, r. liu, y. yang, and d. zhou, “mobilebert: a compact task-agnostic bert for resource-limited devices,” in proceedings of the 58th annual meeting of the association for computational linguistics , d. jurafsky, j. chai, n. schluter, and j. tetreault, eds. online: association for computational linguistics, jul. 2020, pp. 2158–2170. [online]. available: https://aclanthology.org/2020.acl-main.195 x. jiao, y. yin, l. shang, x. jiang, x. chen, l. li, f. wang, and q. liu, “tinybert: distilling bert for natural language understanding,” in findings of the association for computational linguistics: emnlp 2020 , t. cohn, y. he, and y. liu, eds. online: association for computational linguistics, nov. 2020, pp. 4163–4174. [online]. available: https://aclanthology.org/2020.findings-emnlp.372 l. hou, z. huang, l. shang, x. jiang, x. chen, and q. liu, “dynabert: dynamic bert with adaptive width and depth,” advances in neural information processing systems , vol. 33, pp. 9782–9793, 2020. s. zuo, q. zhang, c. liang, p . he, t. zhao, and w. chen, “moebert: from bert to mixture-of-experts via importance- guided adaptation,” arxiv preprint arxiv:2204.07675 , 2022. k. j. liang, w. hao, d. shen, y. zhou, w. chen, c. chen, and l. carin, “mixkd: towards efficient distillation of large- scale language models,” in 9th international conference on learning representations, iclr 2021, virtual event, austria, may 3-7, 2021 . openreview.net, 2021. [online]. available: https://openreview.net/forum?id=ufgeeljklu5 y. j. ma, w. liang, g. wang, d.-a. huang, o. bastani, d. ja- yaraman, y. zhu, l. fan, and a. anandkumar, “eureka: human-level reward design via coding large language models,” 2023. j.-c. pang, p . wang, k. li, x.-h. chen, j. xu, z. zhang, and y. yu, “language model self-improvement by reinforce- ment learning contemplation,” 2023. y. du, o. watkins, z. wang, c. colas, t. darrell, p . abbeel, 37 a. gupta, and j. andreas, “guiding pretraining in reinforcement learning with large language models,” inproceedings of the 40th international conference on machine learning , ser. proceedings of machine learning research, a. krause, e. brunskill, k. cho, b. engelhardt, s. sabato, and j. scarlett, eds., vol. 202. pmlr, 23–29 jul 2023, pp. 8657–8677. [online]. available: https://proceedings.mlr.press/v202/du23f.html j. schulman, f. wolski, p . dhariwal, a. radford, and o. klimov, “proximal policy optimization algorithms,” 2017. r. rafailov, a. sharma, e. mitchell, s. ermon, c. d. man- ning, and c. finn, “direct preference optimization: your language model is secretly a reward model,” 2023. f. song, b. yu, m. li, h. yu, f. huang, y. li, and h. wang, “preference ranking optimization for human alignment,” arxiv preprint arxiv:2306.17492 , 2023. z. yuan, h. yuan, c. tan, w. wang, s. huang, and f. huang, “rrhf: rank responses to align language mod- els with human feedback without tears,” arxiv preprint arxiv:2304.05302 , 2023. m. li, l. chen, j. chen, s. he, and t. zhou, “reflection-tuning: recycling data for better instruction- tuning,” in neurips 2023 workshop on instruction tuning and instruction following , 2023. [online]. available: https://openreview.net/forum?id=xaqozzqkpu m. li, l. chen, j. chen, s. he, j. gu, and t. zhou, “selective reflection-tuning: student-selected data recycling for llm instruction-tuning,” 2024. [online]. available: https: //api.semanticscholar.org/corpusid:267682220 x. geng, a. gudibande, h. liu, e. wallace, p . abbeel, s. levine, and d. song, “koala: a dialogue model for academic research,” blog post, april 2023. [online]. available: https://bair.berkeley.edu/blog/2023/04/03/ koala/ m. li, j. chen, l. chen, and t. zhou, “can llms speak for diverse people? tuning llms via debate to generate controllable controversial statements,” 2024. m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” 2023. r. yang, l. song, y. li, s. zhao, y. ge, x. li, and y. shan, “gpt4tools: teaching large language model to use tools via self-instruction,” 2023. a. yehudai, b. carmeli, y. mass, o. arviv, n. mills, a. toledo, e. shnarch, and l. choshen, “genie: achieving human parity in content-grounded datasets generation,” 2024. y. zhang, r. zhang, j. gu, y. zhou, n. lipka, d. yang, and t. sun, “llavar: enhanced visual instruction tuning for text-rich image understanding,” 2023. c. lyu, m. wu, l. wang, x. huang, b. liu, z. du, s. shi, and z. tu, “macaw-llm: multi-modal language modeling with image, audio, video, and text integration,” arxiv preprint arxiv:2306.09093 , 2023. b. li, y. zhang, l. chen, j. wang, f. pu, j. yang, c. li, and z. liu, “mimic-it: multi-modal in-context instruction tuning,” 2023. z. zhao, l. guo, t. yue, s. chen, s. shao, x. zhu, z. yuan, and j. liu, “chatbridge: bridging modalities with large language model as a language catalyst,” 2023.y. zhao, b. yu, b. hui, h. yu, f. huang, y. li, and n. l. zhang, “a preliminary study of the intrinsic relationship between complexity and alignment,” 2023. a. gudibande, e. wallace, c. snell, x. geng, h. liu, p . abbeel, s. levine, and d. song, “the false promise of imitating proprietary llms,” arxiv preprint arxiv:2305.15717 , 2023. c. zhou, p . liu, p . xu, s. iyer, j. sun, y. mao, x. ma, a. efrat, p . yu, l. yu, s. zhang, g. ghosh, m. lewis, l. zettlemoyer, and o. levy, “lima: less is more for alignment,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview.net/forum?id=kbmokmx2he m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to-strong data filtering for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and z. mao, “expertprompting: instructing large language models to be distinguished experts,” 2023. w. liu, w. zeng, k. he, y. jiang, and j. he, “what makes good data for alignment? a comprehensive study of auto- matic data selection in instruction tuning,” 2023. r. lou, k. zhang, j. xie, y. sun, j. ahn, h. xu, y. su, and w. yin, “muffin: curating multi-faceted instructions for improving instruction-following,” 2023. t. schick, j. dwivedi-yu, z. jiang, f. petroni, p . lewis, g. izacard, q. you, c. nalmpantis, e. grave, and s. riedel, “peer: a collaborative language model,” 2022. a. madaan, n. tandon, p . gupta, s. hallinan, l. gao, s. wiegreffe, u. alon, n. dziri, s. prabhumoye, y. yang, s. gupta, b. p . majumder, k. hermann, s. welleck, a. yaz- danbakhsh, and p . clark, “self-refine: iterative refinement with self-feedback,” 2023. w. saunders, c. yeh, j. wu, s. bills, l. ouyang, j. ward, and j. leike, “self-critiquing models for assisting human evaluators,” 2022. d. m. ziegler, n. stiennon, j. wu, t. b. brown, a. radford, d. amodei, p . christiano, and g. irving, “fine-tuning language models from human preferences,” arxiv preprint arxiv:1909.08593 , 2019. n. stiennon, l. ouyang, j. wu, d. ziegler, r. lowe, c. voss, a. radford, d. amodei, and p . f. christiano, “learning to summarize with human feedback,” advances in neu- ral information processing systems , vol. 33, pp. 3008–3021, 2020. j. wu, l. ouyang, d. m. ziegler, n. stiennon, r. lowe, j. leike, and p . christiano, “recursively summarizing books with human feedback,” 2021. y. bai, a. jones, k. ndousse, a. askell, a. chen, n. das- sarma, d. drain, s. fort, d. ganguli, t. henighan et al. , “training a helpful and harmless assistant with rein- forcement learning from human feedback,” arxiv preprint arxiv:2204.05862 , 2022. a. k ¨opf, y. kilcher, d. von r ¨utte, s. anagnostidis, z.-r. tam, k. stevens, a. barhoum, n. m. duc, o. stanley, r. nagyfi, s. es, s. suri, d. glushkov, a. dantuluri, a. maguire, c. schuhmann, h. nguyen, and a. mattick, “openassis- tant conversations – democratizing large language model alignment,” 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, 38 “openchat: advancing open-source language models with mixed-quality data,” 2023. l. weidinger, j. mellor, m. rauh, c. griffin, j. uesato, p .- s. huang, m. cheng, m. glaese, b. balle, a. kasirzadeh, z. kenton, s. brown, w. hawkins, t. stepleton, c. biles, a. birhane, j. haas, l. rimell, l. a. hendricks, w. isaac, s. legassick, g. irving, and i. gabriel, “ethical and social risks of harm from language models,” 2021. j. ji, m. liu, j. dai, x. pan, c. zhang, c. bian, c. zhang, r. sun, y. wang, and y. yang, “beavertails: towards improved safety alignment of llm via a human-preference dataset,” 2023. i. solaiman and c. dennison, “process for adapting lan- guage models to society (palms) with values-targeted datasets,” advances in neural information processing sys- tems , vol. 34, pp. 5861–5873, 2021. l. qiu, y. zhao, j. li, p . lu, b. peng, j. gao, and s.-c. zhu, “valuenet: a new dataset for human value driven dialogue system,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 11 183– 11 191. j. kiesel, m. alshomary, n. handke, x. cai, h. wachsmuth, and b. stein, “identifying the human values behind arguments,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4459–4471. [online]. available: https://aclanthology.org/2022.acl-long.306 r. liu, g. zhang, x. feng, and s. vosoughi, “aligning generative language models with human values,” in findings of the association for computational linguistics: naacl 2022 , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 241– 252. [online]. available: https://aclanthology.org/2022. findings-naacl.18 a. glaese, n. mcaleese, m. trebacz, j. aslanides, v . firoiu, t. ewalds, m. rauh, l. weidinger, m. chadwick, p . thacker et al. , “improving alignment of dialogue agents via targeted human judgements,” arxiv preprint arxiv:2209.14375 , 2022. h. sun, z. zhang, f. mi, y. wang, w. liu, j. cui, b. wang, q. liu, and m. huang, “moraldial: a framework to train and evaluate moral dialogue systems via moral discussions,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2213–2230. [online]. available: https://aclanthology.org/2023.acl-long.123 j. yao, x. yi, x. wang, j. wang, and x. xie, “from instructions to intrinsic human values – a survey of alignment goals for big models,” 2023. y. liu, y. yao, j.-f. ton, x. zhang, r. g. h. cheng, y. klochkov, m. f. taufiq, and h. li, “trustworthy llms: a survey and guideline for evaluating large language models’ alignment,” arxiv preprint arxiv:2308.05374 , 2023. j. qian, h. wang, z. li, s. li, and x. yan, “limitations of language models in arithmetic and symbolic induction,” 2022.x. she, y. liu, y. zhao, y. he, l. li, c. tantithamthavorn, z. qin, and h. wang, “pitfalls in language models for code intelligence: a taxonomy and survey,” 2023. h. manikandan, y. jiang, and j. z. kolter, “language models are weak learners,” 2023. y. liang, c. wu, t. song, w. wu, y. xia, y. liu, y. ou, s. lu, l. ji, s. mao, y. wang, l. shou, m. gong, and n. duan, “taskmatrix.ai: completing tasks by connecting foundation models with millions of apis,” 2023. g. mialon, r. dess `ı, m. lomeli, c. nalmpantis, r. pa- sunuru, r. raileanu, b. rozi `ere, t. schick, j. dwivedi- yu, a. celikyilmaz, e. grave, y. lecun, and t. scialom, “augmented language models: a survey,” 2023. a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, k. cobbe, t. eloundou, g. krueger, k. button, m. knight, b. chess, and j. schulman, “webgpt: browser-assisted question-answering with human feedback,” 2022. y. qin, z. cai, d. jin, l. yan, s. liang, k. zhu, y. lin, x. han, n. ding, h. wang, r. xie, f. qi, z. liu, m. sun, and j. zhou, “webcpm: interactive web search for chinese long-form question answering,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8968–8988. [online]. available: https://aclanthology.org/2023.acl-long.499 y. song, w. xiong, d. zhu, w. wu, h. qian, m. song, h. huang, c. li, k. wang, r. yao, y. tian, and s. li, “restgpt: connecting large language models with real- world restful apis,” 2023. t. cai, x. wang, t. ma, x. chen, and d. zhou, “large language models as tool makers,” 2023. y. shen, k. song, x. tan, d. li, w. lu, and y. zhuang, “hugginggpt: solving ai tasks with chatgpt and its friends in hugging face,” 2023. s. hao, t. liu, z. wang, and z. hu, “toolkengpt: augment- ing frozen language models with massive tools via tool embeddings,” 2024. s. yuan, k. song, j. chen, x. tan, y. shen, r. kan, d. li, and d. yang, “easytool: enhancing llm-based agents with concise tool instruction,” 2024. s. zhang, s. roller, n. goyal, m. artetxe, m. chen, s. chen, c. dewan, m. diab, x. li, x. v . lin, t. mihaylov, m. ott, s. shleifer, k. shuster, d. simig, p . s. koura, a. sridhar, t. wang, and l. zettlemoyer, “opt: open pre-trained transformer language models,” 2022. t. brown, b. mann, n. ryder, m. subbiah, j. d. ka- plan, p . dhariwal, a. neelakantan, p . shyam, g. sastry, a. askell et al. , “language models are few-shot learners,” advances in neural information processing systems , vol. 33, pp. 1877–1901, 2020. w. huang, p . abbeel, d. pathak, and i. mordatch, “lan- guage models as zero-shot planners: extracting actionable knowledge for embodied agents,” in international confer- ence on machine learning . pmlr, 2022, pp. 9118–9147. i. singh, v . blukis, a. mousavian, a. goyal, d. xu, j. trem- blay, d. fox, j. thomason, and a. garg, “progprompt: 39 generating situated robot task plans using large language models,” 2022. d. zhou, n. sch ¨arli, l. hou, j. wei, n. scales, x. wang, d. schuurmans, c. cui, o. bousquet, q. le, and e. chi, “least-to-most prompting enables complex reasoning in large language models,” 2023. c. h. song, j. wu, c. washington, b. m. sadler, w.-l. chao, and y. su, “llm-planner: few-shot grounded planning for embodied agents with large language models,” in proceed- ings of the ieee/cvf international conference on computer vision , 2023, pp. 2998–3009. z. wang, s. cai, a. liu, x. ma, and y. liang, “describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents,” arxiv preprint arxiv:2302.01560 , 2023. s. yao, d. yu, j. zhao, i. shafran, t. l. griffiths, y. cao, and k. narasimhan, “tree of thoughts: deliberate prob- lem solving with large language models,” arxiv preprint arxiv:2305.10601 , 2023. b. liu, y. jiang, x. zhang, q. liu, s. zhang, j. biswas, and p . stone, “llm+ p: empowering large language mod- els with optimal planning proficiency,” arxiv preprint arxiv:2304.11477 , 2023. s. hao, y. gu, h. ma, j. j. hong, z. wang, d. z. wang, and z. hu, “reasoning with language model is planning with world model,” arxiv preprint arxiv:2305.14992 , 2023. m. hu, y. mu, x. yu, m. ding, s. wu, w. shao, q. chen, b. wang, y. qiao, and p . luo, “tree-planner: efficient close-loop task planning with large language models,” arxiv preprint arxiv:2310.08582 , 2023. b. y. lin, c. huang, q. liu, w. gu, s. sommerer, and x. ren, “on grounded planning for embodied tasks with language models,” in proceedings of the aaai conference on artificial intelligence , vol. 37, no. 11, 2023, pp. 13 192– 13 200. k. valmeekam, m. marquez, s. sreedharan, and s. kambhampati, “on the planning abilities of large language models - a critical investigation,” in thirty-seventh conference on neural informa- tion processing systems , 2023. [online]. available: https://openreview.net/forum?id=x6deqxisew t. sumers, k. marino, a. ahuja, r. fergus, and i. dasgupta, “distilling internet-scale vision-language models into em- bodied agents,” in proceedings of the 40th international conference on machine learning , ser. icml’23. jmlr.org, 2023. y. yang, t. zhou, k. li, d. tao, l. li, l. shen, x. he, j. jiang, and y. shi, “embodied multi-modal agent trained by an llm from a parallel textworld,” 2023. a. vaswani, n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez, ł. kaiser, and i. polosukhin, “attention is all you need,” advances in neural information processing systems , vol. 30, 2017. y. liu, m. ott, n. goyal, j. du, m. joshi, d. chen, o. levy, m. lewis, l. zettlemoyer, and v . stoyanov, “roberta: a robustly optimized bert pretraining approach,” 2019. j. li, l. gui, y. zhou, d. west, c. aloisi, and y. he, “dis- tilling chatgpt for explainable automated student answer assessment,” in emnlp (findings) . association for com- putational linguistics, 2023, pp. 6007–6026. r. tang, x. han, x. jiang, and x. hu, “does syntheticdata generation of llms help clinical text mining?” arxiv preprint arxiv:2303.04360 , 2023. x. he, i. nassar, j. kiros, g. haffari, and m. norouzi, “generate, annotate, and learn: nlp with synthetic text,” trans. assoc. comput. linguistics , vol. 10, pp. 826–842, 2022. [online]. available: https://transacl.org/ojs/index. php/tacl/article/view/3811 y. meng, j. huang, y. zhang, and j. han, “generating training data with language models: towards zero-shot language understanding,” in advances in neural information processing systems 35: annual conference on neural information processing systems 2022, neurips 2022, new orleans, la, usa, november 28 - december 9, 2022 , 2022. [online]. available: http://papers.nips.cc/paper files/ paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms generate high-quality synthetic note-oriented doctor- patient conversations?” in proceedings of the 5th clinical natural language processing workshop , t. naumann, a. ben abacha, s. bethard, k. roberts, and a. rumshisky, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 460–471. [online]. available: https://aclanthology.org/2023.clinicalnlp-1.49 z. yang, s. cherian, and s. vucetic, “data augmentation for radiology report simplification,” in findings of the association for computational linguistics: eacl 2023 , a. vlachos and i. augenstein, eds. dubrovnik, croatia: association for computational linguistics, may 2023, pp. 1922–1932. [online]. available: https: //aclanthology.org/2023.findings-eacl.144 z. cai, c. tao, t. shen, c. xu, x. geng, x. a. lin, l. he, and d. jiang, “hyper: multitask hyper-prompted training en- ables large-scale retrieval generalization,” in the eleventh international conference on learning representations , 2022. c. liu, c. tao, x. geng, t. shen, d. zhao, c. xu, b. jiao, and d. jiang, “adam: dense retrieval distillation with adaptive dark examples,” arxiv preprint arxiv:2212.10192 , 2022. j. feng, c. tao, x. geng, t. shen, c. xu, g. long, d. zhao, and d. jiang, “knowledge refinement via interaction be- tween search engines and large language models,” arxiv preprint arxiv:2305.07402 , 2023. t. shen, g. long, x. geng, c. tao, t. zhou, and d. jiang, “large language models are strong zero-shot retriever,” arxiv preprint arxiv:2304.14233 , 2023. x. ma, x. zhang, r. pradeep, and j. lin, “zero-shot listwise document reranking with a large language model,” 2023. z. qin, r. jagerman, k. hui, h. zhuang, j. wu, j. shen, t. liu, j. liu, d. metzler, x. wang, and m. bendersky, “large language models are effective text rankers with pairwise ranking prompting,” 2023. x. ma, y. gong, p . he, h. zhao, and n. duan, “query rewriting in retrieval-augmented large language models,” inproceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5303–5315. [online]. available: https://aclanthology.org/2023.emnlp-main.322 40 d. sachan, m. lewis, m. joshi, a. aghajanyan, w.- t. yih, j. pineau, and l. zettlemoyer, “improving passage retrieval with zero-shot question generation,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 3781–3797. [online]. available: https://aclanthology.org/2022.emnlp-main.249 d. s. sachan, m. lewis, d. yogatama, l. zettlemoyer, j. pineau, and m. zaheer, “questions are all you need to train a dense passage retriever,” transactions of the association for computational linguistics , vol. 11, pp. 600–616, 2023. [online]. available: https://aclanthology. org/2023.tacl-1.35 t. schick and h. sch ¨utze, “generating datasets with pretrained language models,” in proceedings of the 2021 conference on empirical methods in natural language processing , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. online and punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 6943–6951. [online]. available: https: //aclanthology.org/2021.emnlp-main.555 z. peng, x. wu, and y. fang, “soft prompt tuning for augmenting dense retrieval with large language models,” arxiv preprint arxiv:2307.08303 , 2023. j. saad-falcon, o. khattab, k. santhanam, r. florian, m. franz, s. roukos, a. sil, m. a. sultan, and c. potts, “udapdr: unsupervised domain adaptation via llm prompting and distillation of rerankers,” in proceedings of the 2023 conference on empirical methods in natural language processing, emnlp 2023, singapore, december 6-10, 2023 , 2023, pp. 11 265–11 279. [online]. available: https://aclanthology.org/2023.emnlp-main.693 v . jeronymo, l. bonifacio, h. abonizio, m. fadaee, r. lotufo, j. zavrel, and r. nogueira, “inpars-v2: large language models as efficient dataset generators for infor- mation retrieval,” arxiv preprint arxiv:2301.01820 , 2023. w. sun, z. chen, x. ma, l. yan, s. wang, p . ren, z. chen, d. yin, and z. ren, “instruction distillation makes large language models efficient zero-shot rankers,” 2023. c. raffel, n. shazeer, a. roberts, k. lee, s. narang, m. matena, y. zhou, w. li, and p . j. liu, “exploring the limits of transfer learning with a unified text-to-text transformer,” j. mach. learn. res. , vol. 21, no. 1, jan 2020. s. bruch, x. wang, m. bendersky, and m. najork, “an analysis of the softmax cross entropy loss for learning- to-rank with binary relevance,” in proceedings of the 2019 acm sigir international conference on theory of information retrieval, ictir 2019, santa clara, ca, usa, october 2-5, 2019 , 2019, pp. 75–78. [online]. available: https://doi.org/10.1145/3341981.3344221 c. burges, t. shaked, e. renshaw, a. lazier, m. deeds, n. hamilton, and g. hullender, “learning to rank using gradient descent,” in proceedings of the 22nd international conference on machine learning , ser. icml ’05. new york, ny, usa: association for computing machinery, 2005, p. 89–96. [online]. available: https: //doi.org/10.1145/1102351.1102363 x. wang, c. li, n. golbandi, m. bendersky, and m. najork, “the lambdaloss framework for ranking metricoptimization,” in proceedings of the 27th acm international conference on information and knowledge management , ser. cikm ’18. new york, ny, usa: association for computing machinery, 2018, p. 1313–1322. [online]. available: https://doi.org/10.1145/3269206.3271784 w. wang, x. lin, f. feng, x. he, and t.-s. chua, “generative recommendation: towards next-generation recommender paradigm,” 2023. s. dai, n. shao, h. zhao, w. yu, z. si, c. xu, z. sun, x. zhang, and j. xu, “uncovering chatgpt’s capabilities in recommender systems,” in proceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 y. xi, w. liu, j. lin, x. cai, h. zhu, j. zhu, b. chen, r. tang, w. zhang, r. zhang, and y. yu, “towards open- world recommendation with knowledge augmentation from large language models,” 2023. x. ren, w. wei, l. xia, l. su, s. cheng, j. wang, d. yin, and c. huang, “representation learning with large language models for recommendation,” 2023. w. wei, x. ren, j. tang, q. wang, l. su, s. cheng, j. wang, d. yin, and c. huang, “llmrec: large language models with graph augmentation for recommendation,” 2024. l. wang, s. zhang, y. wang, e.-p . lim, and y. wang, “llm4vis: explainable visualization recommendation using chatgpt,” in proceedings of the 2023 conference on empirical methods in natural language processing: industry track , m. wang and i. zitouni, eds. singapore: association for computational linguistics, dec. 2023, pp. 675–692. [online]. available: https://aclanthology.org/ 2023.emnlp-industry.64 z. cui, j. ma, c. zhou, j. zhou, and h. yang, “m6-rec: generative pretrained language models are open-ended recommender systems,” 2022. p . liu, l. zhang, and j. a. gulla, “pre-train, prompt and recommendation: a comprehensive survey of language modelling paradigm adaptations in recommender sys- tems,” 2023. k. papineni, s. roukos, t. ward, and w.-j. zhu, “bleu: a method for automatic evaluation of machine translation,” inproceedings of the 40th annual meeting on association for computational linguistics , ser. acl ’02. usa: association for computational linguistics, 2002, p. 311–318. [online]. available: https://doi.org/10.3115/1073083.1073135 c.-y. lin, “rouge: a package for automatic evaluation of summaries,” in text summarization branches out . barcelona, spain: association for computational linguistics, jul. 2004, pp. 74–81. [online]. available: https://aclanthology.org/w04-1013 c. su and c. mcmillan, “distilled gpt for source code summarization,” corr , vol. abs/2308.14731, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308. 14731 w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” corr , vol. abs/2312.15692, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.15692 o. sener and s. savarese, “active learning for convolutional neural networks: a core-set approach,” in 6th international 41 conference on learning representations, iclr 2018, vancouver, bc, canada, april 30 - may 3, 2018, conference track proceedings , 2018. [online]. available: https://openreview.net/forum?id=h1aiuk-rw h. liu, c. li, y. li, and y. j. lee, “improved baselines with visual instruction tuning,” 2023. s. zhang, p . sun, s. chen, m. xiao, w. shao, w. zhang, y. liu, k. chen, and p . luo, “gpt4roi: instruction tuning large language model on region-of-interest,” 2023. openai, “gpt-4v(ision) system card,” 2023. [online]. available: https://api.semanticscholar.org/corpusid: 263218031 b. a. plummer, l. wang, c. m. cervantes, j. c. caicedo, j. hockenmaier, and s. lazebnik, “flickr30k entities: collecting region-to-phrase correspondences for richer image-to-sentence models,” in proceedings of the ieee in- ternational conference on computer vision , 2015, pp. 2641– 2649. l. li, z. xie, m. li, s. chen, p . wang, l. chen, y. yang, b. wang, and l. kong, “silkie: preference distilla- tion for large visual language models,” arxiv preprint arxiv:2312.10665 , 2023. h. ha, p . florence, and s. song, “scaling up and distilling down: language-guided robot skill acquisition,” in con- ference on robot learning . pmlr, 2023, pp. 3766–3777. s. wu, z. liu, z. zhang, z. chen, w. deng, w. zhang, j. yang, z. yao, y. lyu, x. xin, s. gao, p . ren, z. ren, and z. chen, “fuzi.mingcha,” https://github.com/irlab- sdu/fuzi.mingcha, 2023. h. xiong, s. wang, y. zhu, z. zhao, y. liu, q. wang, and d. shen, “doctorglm: fine-tuning your chinese doctor is not a herculean task,” arxiv preprint arxiv:2304.01097 , 2023. x. zhang, c. tian, x. yang, l. chen, z. li, and l. r. pet- zold, “alpacare: instruction-tuned large language models for medical application,” arxiv preprint arxiv:2310.14558 , 2023. y. li, z. li, k. zhang, r. dan, s. jiang, and y. zhang, “chatdoctor: a medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge,” cureus , vol. 15, no. 6, 2023. t. han, l. c. adams, j. papaioannou, p . grundmann, t. oberhauser, a. l ¨oser, d. truhn, and k. k. bressem, “medalpaca - an open-source collection of medical conversational ai models and training data,” corr , vol. abs/2304.08247, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.08247 c. wu, w. lin, x. zhang, y. zhang, y. wang, and w. xie, “pmc-llama: towards building open-source language models for medicine,” arxiv preprint arxiv:2305.10415 , vol. 6, 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consultation,” corr , vol. abs/2308.14346, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.14346 z. gou, z. shao, y. gong, yelong shen, y. yang, m. huang, n. duan, and w. chen, “tora: a tool- integrated reasoning agent for mathematical problem solving,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=ep0ttjvoap e. perkowski, r. pan, t. d. nguyen, y. ting, s. kruk, t. zhang, c. o’neill, m. jablonska, z. sun, m. j. smith, h. liu, k. schawinski, k. iyer, i. ciuca, and universetbd, “astrollama-chat: scaling astrollama with conversational and diverse datasets,” corr , vol. abs/2401.01916, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2401.01916 j. gao, r. pi, j. zhang, j. ye, w. zhong, y. wang, l. hong, j. han, h. xu, z. li, and l. kong, “g-llava: solving geometric problem with multi-modal large language model,” corr , vol. abs/2312.11370, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.11370 h. zhao, s. liu, c. ma, h. xu, j. fu, z.-h. deng, l. kong, and q. liu, “gimlet: a unified graph-text model for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv a. n. rubungo, c. arnold, b. p . rand, and a. b. dieng, “llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions,” corr , vol. abs/2310.14029, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.14029 h. cao, z. liu, x. lu, y. yao, and y. li, “instructmol: multi-modal integration for building a versatile and reliable molecular assistant in drug discovery,” corr , vol. abs/2311.16208, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2311.16208 h. abdine, m. chatzianastasis, c. bouyioukos, and m. vazirgiannis, “prot2text: multimodal protein’s function generation with gnns and transform- ers,” in deep generative models for health workshop neurips 2023 , 2023. [online]. available: https://openreview.net/forum?id=ej7yngwyfj y. luo, j. zhang, s. fan, k. yang, y. wu, m. qiao, and z. nie, “biomedgpt: open multimodal generative pre-trained transformer for biomedicine,” arxiv preprint arxiv:2308.09442 , 2023. b. chen, x. cheng, p . li, y. geng, j. gong, s. li, z. bei, x. tan, b. wang, x. zeng, c. liu, a. zeng, y. dong, j. tang, and l. song, “xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein,” corr , vol. abs/2401.06199, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.06199 c. deng, t. zhang, z. he, y. xu, q. chen, y. shi, l. fu, w. zhang, x. wang, c. zhou, z. lin, and j. he, “k2: a foundation language model for geoscience knowledge understanding and utilization,” 2023. z. bi, n. zhang, y. xue, y. ou, d. ji, g. zheng, and h. chen, “oceangpt: a large language model for ocean science tasks,” corr , vol. abs/2310.02031, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.02031 z. zheng, j. zhang, t. vu, s. diao, y. h. w. tim, and s. yeung, “marinegpt: unlocking secrets of ocean to the public,” corr , vol. abs/2310.13596, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.13596 z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: 42 a scientific large language model in geoscience,” corr , vol. abs/2401.00434, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.00434 d. zhang, a. petrova, d. trautmann, and f. schilder, “un- leashing the power of large language models for legal applications,” in proceedings of the 32nd acm international conference on information and knowledge management , 2023, pp. 5257–5258. z. sun, “a short survey of viewing large language models in legal aspect,” arxiv preprint arxiv:2303.09136 , 2023. j. lai, w. gan, j. wu, z. qi, and p . s. yu, “large language models in law: a survey,” arxiv preprint arxiv:2312.03718 , 2023. s. yue, w. chen, s. wang, b. li, c. shen, s. liu, y. zhou, y. xiao, s. yun, w. lin et al. , “disc-lawllm: fine-tuning large language models for intelligent legal services,” arxiv preprint arxiv:2309.11325 , 2023. h. zhong, c. xiao, c. tu, t. zhang, z. liu, and m. sun, “jec-qa: a legal-domain question answering dataset,” in proceedings of the aaai conference on artificial intelligence , vol. 34, no. 05, 2020, pp. 9701–9708. k. singhal, t. tu, j. gottweis, r. sayres, e. wulczyn, l. hou, k. clark, s. pfohl, h. cole-lewis, d. neal, m. schaekermann, a. wang, m. amin, s. lachgar, p . a. mansfield, s. prakash, b. green, e. dominowska, b. a. y arcas, n. tomasev, y. liu, r. wong, c. semturs, s. s. mahdavi, j. k. barral, d. r. webster, g. s. corrado, y. matias, s. azizi, a. karthikesalingam, and v . natarajan, “towards expert-level medical question answering with large language models,” corr , vol. abs/2305.09617, 2023. [online]. available: https://doi. org/10.48550/arxiv.2305.09617 x. yang, j. gao, w. xue, and e. alexandersson, “pllama: an open-source large language model for plant science,” corr , vol. abs/2401.01600, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.01600 x. wang, g. h. chen, d. song, z. zhang, z. chen, q. xiao, f. jiang, j. li, x. wan, b. wang et al. , “cmb: a compre- hensive medical benchmark in chinese,” arxiv preprint arxiv:2308.08833 , 2023. w. zhu, x. wang, h. zheng, m. chen, and b. tang, “promptcblue: a chinese prompt tuning benchmark for the medical domain,” arxiv preprint arxiv:2310.14151 , 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consulta- tion,” arxiv preprint arxiv:2308.14346 , 2023. c. wu, x. zhang, y. zhang, y. wang, and w. xie, “pmc- llama: further finetuning llama on medical papers,” corr , vol. abs/2304.14454, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.14454 s. xue, f. zhou, y. xu, h. zhao, s. xie, q. dai, c. jiang, j. zhang, j. zhou, d. xiu, and h. mei, “weaverbird: empowering financial decision-making with large language model, knowledge base, and search engine,” corr , vol. abs/2308.05361, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.05361 s. wu, o. irsoy, s. lu, v . dabravolski, m. dredze, s. gehrmann, p . kambadur, d. s. rosenberg, and g. mann, “bloomberggpt: a large language modelfor finance,” corr , vol. abs/2303.17564, 2023. [online]. available: https://doi.org/10.48550/arxiv.2303.17564 d. lu, h. wu, j. liang, y. xu, q. he, y. geng, m. han, y. xin, and y. xiao, “bbt-fin: comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark,” corr , vol. abs/2302.09432, 2023. [online]. available: https://doi.org/10.48550/arxiv.2302. 09432 y. yang, y. tang, and k. y. tam, “investlm: a large language model for investment using financial domain instruction tuning,” corr , vol. abs/2309.13064, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.13064 q. xie, w. han, x. zhang, y. lai, m. peng, a. lopez- lira, and j. huang, “pixiu: a large language model, instruction data and evaluation benchmark for finance,” corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction tuning benchmark for open-source large language models in financial datasets,” corr , vol. abs/2310.04793, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310. 04793 r. taylor, m. kardas, g. cucurull, t. scialom, a. hartshorn, e. saravia, a. poulton, v . kerkez, and r. stojnic, “galactica: a large language model for science,” corr , vol. abs/2211.09085, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09085 j. yin, s. dash, f. wang, and m. shankar, “forge: pre-training open foundation models for science,” inproceedings of the international conference for high performance computing, networking, storage and analysis, sc 2023, denver, co, usa, november 12-17, 2023 , d. arnold, r. m. badia, and k. m. mohror, eds. acm, 2023, pp. 81:1–81:13. [online]. available: https: //doi.org/10.1145/3581784.3613215 z. azerbayev, h. schoelkopf, k. paster, m. d. santos, s. mcaleer, a. q. jiang, j. deng, s. biderman, and s. welleck, “llemma: an open language model for mathematics,” corr , vol. abs/2310.10631, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.10631 f. yu, a. gao, and b. wang, “outcome-supervised verifiers for planning in mathematical reasoning,” corr , vol. abs/2311.09724, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09724 t. d. nguyen, y. ting, i. ciuca, c. o’neill, z. sun, m. jablonska, s. kruk, e. perkowski, j. w. miller, j. li, j. peek, k. iyer, t. r ´ozanski, p . khetarpal, s. zaman, d. brodrick, s. j. r. m ´endez, t. bui, a. goodman, a. accomazzi, j. p . naiman, j. cranney, k. schawinski, and universetbd, “astrollama: towards specialized foundation models in astronomy,” corr , vol. abs/2309.06126, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2309.06126 j. roberts, t. l ¨uddecke, s. das, k. han, and s. albanie, “gpt4geo: how a language model sees the world’s ge- ography,” 2023. z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: a scientific large language model in geoscience,” 2023. 43 c. wang, d. engler, x. li, j. hou, d. j. wald, k. jaiswal, and s. xu, “near-real-time earthquake-induced fatality estimation using crowdsourced data and large-language models,” 2023. l. chen, s. li, j. yan, h. wang, k. gunaratna, v . yadav, z. tang, v . srinivasan, t. zhou, h. huang, and h. jin, “alpagasus: training a better alpaca with fewer data,” 2023. y. cao, y. kang, and l. sun, “instruction mining: high- quality instruction data selection for large language mod- els,” 2023. m. li, y. zhang, z. li, j. chen, l. chen, n. cheng, j. wang, t. zhou, and j. xiao, “from quantity to quality: boosting llm performance with self-guided data selection for instruction tuning,” arxiv , vol. abs/2308.12032, 2023. [online]. available: https://api.semanticscholar. org/corpusid:261076515 q. du, c. zong, and j. zhang, “mods: model-oriented data selection for instruction tuning,” 2023. y. li, b. hui, x. xia, j. yang, m. yang, l. zhang, s. si, j. liu, t. liu, f. huang, and y. li, “one shot learning as instruction data prospector for large language models,” 2023. e. frantar, s. p . singh, and d. alistarh, “optimal brain com- pression: a framework for accurate post-training quanti- zation and pruning,” 2023. t. dettmers, m. lewis, y. belkada, and l. zettlemoyer, “gpt3.int8(): 8-bit matrix multiplication for transformers at scale,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https://openreview.net/ forum?id=dxigwqboxad y. j. kim, r. henry, r. fahim, and h. h. awadalla, “finequant: unlocking efficiency with fine-grained weight-only quantization for llms,” 2023. c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre-trained language models via quantization,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4821–4836. [online]. available: https://aclanthology.org/2022.acl- long.331 z. yao, r. yazdani aminabadi, m. zhang, x. wu, c. li, and y. he, “zeroquant: efficient and affordable post-training quantization for large-scale transformers,” advances in neural information processing systems , vol. 35, pp. 27 168– 27 183, 2022. g. xiao, j. lin, m. seznec, h. wu, j. demouth, and s. han, “smoothquant: accurate and efficient post-training quan- tization for large language models,” 2023. x. ma, g. fang, and x. wang, “llm-pruner: on the struc- tural pruning of large language models,” 2023. m. zhang, h. chen, c. shen, z. yang, l. ou, x. yu, and b. zhuang, “loraprune: pruning meets low-rank parameter-efficient fine-tuning,” 2023. e. frantar and d. alistarh, “sparsegpt: massive language models can be accurately pruned in one-shot,” 2023. m. xu, y. l. xu, and d. p . mandic, “tensorgpt: efficient compression of the embedding layer in llms based on thetensor-train decomposition,” 2023. y. li, y. yu, q. zhang, c. liang, p . he, w. chen, and t. zhao, “losparse: structured compression of large lan- guage models based on low-rank and sparse approxima- tion,” 2023. z. hu, l. wang, y. lan, w. xu, e.-p . lim, l. bing, x. xu, s. poria, and r. k.-w. lee, “llm-adapters: an adapter family for parameter-efficient fine-tuning of large lan- guage models,” 2023. h. liu, d. tam, m. mohammed, j. mohta, t. huang, m. bansal, and c. raffel, “few-shot parameter- efficient fine-tuning is better and cheaper than in- context learning,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https: //openreview.net/forum?id=rbcvmg-jspd y. wang, s. agarwal, s. mukherjee, x. liu, j. gao, a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 5744–5760. [online]. available: https://aclanthology.org/2022.emnlp-main.388 e. j. hu, y. shen, p . wallis, z. allen-zhu, y. li, s. wang, l. wang, and w. chen, “lora: low-rank adaptation of large language models,” 2021. x. l. li and p . liang, “prefix-tuning: optimizing continuous prompts for generation,” in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: long papers) , c. zong, f. xia, w. li, and r. navigli, eds. online: association for computational linguistics, aug. 2021, pp. 4582–4597. [online]. available: https://aclanthology.org/2021.acl- long.353 x. liu, k. ji, y. fu, w. tam, z. du, z. yang, and j. tang, “p- tuning: prompt tuning can be comparable to fine-tuning across scales and tasks,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 2: short papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 61–68. [online]. available: https://aclanthology.org/2022.acl-short.8 t. dettmers, a. pagnoni, a. holtzman, and l. zettlemoyer, “qlora: efficient finetuning of quantized llms,” 2023. j. kim, j. h. lee, s. kim, j. park, k. m. yoo, s. j. kwon, and d. lee, “memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization,” 2023. s. malladi, t. gao, e. nichani, a. damian, j. d. lee, d. chen, and s. arora, “fine-tuning language models with just forward passes,” 2024. z. wan, x. wang, c. liu, s. alam, y. zheng, j. liu, z. qu, s. yan, y. zhu, q. zhang, m. chowdhury, and m. zhang, “efficient large language models: a survey,” 2024. y.-s. lee, m. sultan, y. el-kurdi, t. naseem, a. munawar, r. florian, s. roukos, and r. astudillo, “ensemble- instruct: instruction tuning data generation with a heterogeneous mixture of lms,” in findings of the 44 association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 12 561–12 571. [online]. available: https://aclanthology. org/2023.findings-emnlp.836 w. chen, y. zhou, n. du, y. huang, j. laudon, z. chen, and c. cui, “lifelong language pretraining with distribution- specialized experts,” in international conference on machine learning . pmlr, 2023, pp. 5383–5395. s. kotha, j. m. springer, and a. raghunathan, “under- standing catastrophic forgetting in language models via implicit inference,” arxiv preprint arxiv:2309.10105 , 2023. b. koloski, b. ˇskrlj, m. robnik- ˇsikonja, and s. pollak, “mea- suring catastrophic forgetting in cross-lingual transfer paradigms: exploring tuning strategies,” arxiv preprint arxiv:2309.06089 , 2023. t. wu, l. luo, y.-f. li, s. pan, t.-t. vu, and g. haffari, “continual learning for large language models: a sur- vey,” arxiv preprint arxiv:2402.01364 , 2024. y. luo, z. yang, f. meng, y. li, j. zhou, and y. zhang, “an empirical study of catastrophic forgetting in large language models during continual fine-tuning,” arxiv preprint arxiv:2308.08747 , 2023. j. kirkpatrick, r. pascanu, n. rabinowitz, j. veness, g. des- jardins, a. a. rusu, k. milan, j. quan, t. ramalho, a. grabska-barwinska et al. , “overcoming catastrophic forgetting in neural networks,” proceedings of the national academy of sciences , vol. 114, no. 13, pp. 3521–3526, 2017. m. rostami, s. kolouri, and p . k. pilly, “complementary learning for overcoming catastrophic forgetting using ex- perience replay,” arxiv preprint arxiv:1903.04566 , 2019. d. rolnick, a. ahuja, j. schwarz, t. lillicrap, and g. wayne, “experience replay for continual learning,” advances in neural information processing systems , vol. 32, 2019. s.-w. lee, j.-h. kim, j. jun, j.-w. ha, and b.-t. zhang, “overcoming catastrophic forgetting by incremental mo- ment matching,” advances in neural information processing systems , vol. 30, 2017. a. mallya, d. davis, and s. lazebnik, “piggyback: adapting a single network to multiple tasks by learning to mask weights,” in proceedings of the european conference on com- puter vision (eccv) , 2018, pp. 67–82. z. wang, z. zhang, c.-y. lee, h. zhang, r. sun, x. ren, g. su, v . perot, j. dy, and t. pfister, “learning to prompt for continual learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2022, pp. 139–149. z. hu, y. li, j. lyu, d. gao, and n. vasconcelos, “dense network expansion for class incremental learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2023, pp. 11 858–11 867. x. li, l. lin, s. wang, and c. qian, “unlock the power: competitive distillation for multi-modal large language models,” arxiv preprint arxiv:2311.08213 , 2023. m. zeng, w. xue, q. liu, and y. guo, “continual learning with dirichlet generative-based rehearsal,” arxiv preprint arxiv:2309.06917 , 2023. z. zhang, m. fang, l. chen, and m.-r. namazi-rad, “citb: a benchmark for continual instruction tuning,” arxiv preprint arxiv:2310.14510 , 2023. c. burns, p . izmailov, j. h. kirchner, b. baker, l. gao,l. aschenbrenner, y. chen, a. ecoffet, m. joglekar, j. leike, i. sutskever, and j. wu, “weak-to-strong generalization: eliciting strong capabilities with weak supervision,” corr , vol. abs/2312.09390, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.09390 m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to- strong data filtering for fast instruction-tuning,” corr , vol. abs/2402.00530, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.00530 j. ji, b. chen, h. lou, d. hong, b. zhang, x. pan, j. dai, and y. yang, “aligner: achieving efficient alignment through weak-to-strong correction,” corr , vol. abs/2402.02416, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.02416'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['technical']['user_prompt'].format(processed_content=processed_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_message = [{\"role\": \"system\", \"content\": file['technical']['system_prompt']}, \n",
    "            {\"role\": \"user\", \"content\": file['technical']['user_prompt'].format(processed_content=processed_content)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are an AI assistant skilled at distilling complex AI research papers into compelling, holistic summaries for data scientists, software engineers, and machine learning engineers. Your summaries are known for capturing the essence of groundbreaking research and sparking curiosity in technical minds.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Create an engaging, technical summary of the following AI research paper tailored for ML engineers and AI professionals. Your summary should:\\n\\nOpening:\\n\\nBegin with a powerful statement that captures the paper\\'s most innovative or impactful idea.\\nFollow with a brief explanation of the paper\\'s main contribution to the field of AI/ML and why it matters.\\nUse language that would intrigue technical practitioners and immediately convey the significance of the research.\\n\\n\\nKey Takeaways:\\nPresent 2-3 key takeaways. For each:\\n\\nExplain the main idea in clear terms, avoiding excessive technical jargon.\\nMention its potential impact on current ML practices or future research.\\nEnsure clear linkages between concepts, showing how ideas connect or build upon each other.\\nInclude enough detail to convey the importance and novelty of each takeaway, while maintaining overall brevity.\\n\\nImpactful Quote:\\nInclude one short, impactful quote from the paper. This should be the only instance of using the paper\\'s exact words. Explain its significance succinctly, focusing on its potential implications for AI/ML development.\\nConclusion:\\nHighlight the potential future impact of this research and why it\\'s exciting for the field, drawing on the ideas presented in the takeaways.\\n\\nThroughout the summary:\\n\\nUse your own words to convey the paper\\'s ideas. Do not copy exact sentences or substantial phrases from the paper, except for the single quote in section 3.\\nAim for a tone that balances technical insight with accessibility and enthusiasm.\\nFocus on sparking curiosity and conveying the potential importance of the research.\\nEnsure clear connections between different concepts and ideas presented.\\n                         \\nAim for a tone that balances technical precision with enthusiasm for the research\\'s potential. Focus on aspects that would most interest ML practitioners and researchers, always ensuring clear explanations of how concepts interact or build upon each other. Your goal is to leave technical readers thinking, \"This approach could significantly advance our current methods - I need to explore the full paper.\"\\nKeep the entire summary under 300 words to maintain engagement, remembering that detailed section-by-section summaries will follow.\\nNow, here\\'s the full paper:\\n1 a survey on knowledge distillation of large language models xiaohan xu1, ming li2, chongyang tao3, tao shen4, reynold cheng1, jinyang li1, can xu5, dacheng tao6, tianyi zhou2 1the university of hong kong2university of maryland3microsoft 4university of technology sydney5peking university6the university of sydney {shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk jl0725@connect.hku.hk abstract —in the era of large language models (llms), knowledge distillation (kd) emerges as a pivotal methodology for transferring advanced capabilities from leading proprietary llms, such as gpt -4, to their open-source counterparts like llama and mistral. additionally, as open-source llms flourish, kd plays a crucial role in both compressing these models, and facilitating their self- improvement by employing themselves as teachers. this paper presents a comprehensive survey of kd’s role within the realm of llm, highlighting its critical function in imparting advanced knowledge to smaller models and its utility in model compression and self- improvement. our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization – providing a comprehensive examination of kd mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. crucially, the survey navigates the intricate interplay between data augmentation (da) and kd, illustrating how da emerges as a powerful paradigm within the kd framework to bolster llms’ performance. by leveraging da to generate context- rich, skill-specific training data, kd transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. this work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. by bridging the gap between proprietary and open-source llms, this survey underscores the potential for more accessible, efficient, and powerful ai solutions. most importantly, we firmly advocate for compliance with the legal terms that regulate the use of llms, ensuring ethical and lawful application of kd of llms. an associated github repository is available at https://github.com/tebmer/awesome-knowledge-distillation-of-llms. index terms —large language models, knowledge distillation, data augmentation, skill distillation, supervised fine-tuning ✦ 1 i ntroduction in the evolving landscape of artificial intelligence (ai), proprietary1large language models (llms) such as gpt- 3.5 (ouyang et al., 2022), gpt-4 (openai et al., 2023), gemini (team et al., 2023) and claude2have emerged as groundbreaking technologies, reshaping our understand- ing of natural language processing (nlp). these models, characterized by their vast scale and complexity, have un- locked new realms of possibility, from generating human- like text to offering sophisticated problem-solving capa- bilities. the core significance of these llms lies in their emergent abilities (wei et al., 2022a,b; xu et al., 2024a), a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. their deep understanding of context, nuance, and the intrica- cies of human language enables them to excel in a wide array of applications, from creative content generation to 1. for simplicity, we use ‘proprietary’ to represent both versatile yet close-source llms like gpt-4 and open-source yet huge llms like llama-2-70b, which encapsulate rich knowledge with a large number of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond of parameters. 2. https://www.anthropic.com/claude-in-slackcomplex problem-solving (openai et al., 2023; liang et al., 2022). the potential of these models extends far beyond current applications, promising to revolutionize industries, augment human creativity, and redefine our interaction with technology. despite the remarkable capabilities of proprietary llms like gpt-4 and gemini, they are not without their shortcom- ings, particularly when viewed in light of the advantages offered by open-source models. a significant drawback is their limited accessibility and higher cost (openai et al., 2023). these proprietary models often come with substantial usage fees and restricted access, making them less attain- able for individuals and smaller organizations. in terms of data privacy and security (wu et al., 2023a), using these proprietary llms frequently entails sending sensitive data to external servers, which raises concerns about data pri- vacy and security. this aspect is especially critical for users handling confidential information. moreover, the general- purpose design of proprietary llms, while powerful, may not always align with the specific needs of niche applica- tions. the constraints of accessibility, cost, and adaptability thus present significant challenges in leveraging the full potential of proprietary llms. in contrast to proprietary llms, open-source modelsarxiv:2402.13116v3 [cs.cl] 8 mar 2024 2 like llama (touvron et al., 2023) and mistral (jiang et al., 2023a) bring several notable advantages. one of the primary benefits of open-source models is their accessibility and adaptability. without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader range of users, from individual re- searchers to smaller organizations. this openness fosters a more collaborative and inclusive ai research environment, encouraging innovation and diverse applications. addition- ally, the customizable nature of open-source llms allows for more tailored solutions, addressing specific needs that generic, large-scale models may not meet. however, the open-source llms also have their own set of drawbacks, primarily stemming from their relatively limited scale and resources compared to their proprietary counterparts. one of the most significant limitations is the smaller model scale, which often results in lower per- formance on real-world tasks with a bunch of instruc- tions (zheng et al., 2023a). these models, with fewer pa- rameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like gpt-4. ad- ditionally, the pre-training investment in these open-source models is typically less substantial. this reduced investment can lead to a narrower range of pre-training data, poten- tially limiting the models’ understanding and handling of diverse or specialized topics (liang et al., 2022; sun et al., 2024a). moreover, open-source models often undergo fewer fine-tuning steps due to resource constraints. fine-tuning is crucial for optimizing a model’s performance for spe- cific tasks or industries, and the lack thereof can hinder the model’s effectiveness in specialized applications. this limitation becomes particularly evident when these models are compared to the highly fine-tuned proprietary llms, which are often tailored to excel in a wide array of complex scenarios (openai et al., 2023). primarily, recognizing the disparities between propri- etary and open-source llms, kd techniques have surged as a means to bridge the performance gap between these models (gou et al., 2021; gupta and agrawal, 2022). knowl- edge distillation, in this context, involves leveraging the more advanced capabilities of leading proprietary models like gpt-4 or gemini as a guiding framework to enhance the competencies of open-source llms. this process is akin to transferring the ‘knowledge’ of a highly skilled teacher to a student, wherein the student (e.g., open-source llm) learns to mimic the performance characteristics of the teacher (e.g., proprietary llm). compared to traditional knowledge distillation algorithms (gou et al., 2021), data augmentation (da) (feng et al., 2021) has emerged as a prevalent paradigm to achieve knowledge distillation of llms, where a small seed of knowledge is used to prompt the llm to generate more data with respect to a specific skill or domain (taori et al., 2023). secondly, kd still retains its fundamental role in compressing llms, making them more efficient without significant loss in performance. (gu et al., 2024; agarwal et al., 2024). more recently, the strategy of employing open-source llms as teachers for their own self-improvement has emerged as a promising approach, enhancing their capabilities significantly (yuan et al., 2024a; chen et al., 2024a). figure 1 provides an illustration of these three key roles played by kd in the context of llms. closed-sourcellmsopen-sourcellmssmallerlmsadvancecompressself-improvement directionofkd ①②③fig. 1: kd plays three key roles in llms: 1) primarily enhancing capabilities, 2) offering traditional compression for efficiency, and 3) an emerging trend of self-improvement via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- via self-generated knowledge. a key aspect of the knowledge distillation is the en- hancement of skills such as advanced context following (e.g., in-context learning (huang et al., 2022a) and in- struction following (taori et al., 2023)), improved align- ment with user intents (e.g., human values/principles (cui et al., 2023a), and thinking patterns like chain-of-thought (cot) (mukherjee et al., 2023)), and nlp task specialization (e.g., semantic understanding (ding et al., 2023a), and code generation (chaudhary, 2023)). these skills are crucial for the wide array of applications that llms are expected to perform, ranging from casual conversations to com- plex problem-solving in specialized domains. for instance, in vertical domains like healthcare (wang et al., 2023a), law (law, 2023), or science (zhang et al., 2024), where accuracy and context-specific knowledge are paramount, knowledge distillation allows open-source models to sig- nificantly improve their performance by learning from the proprietary models that have been extensively trained and fine-tuned in these areas. the benefits of knowledge distillation in the era of llms are multifaceted and transformative (gu et al., 2024). through a suite of distillation techniques, the gap between proprietary and open-source models is significantly nar- rowed (chiang et al., 2023; xu et al., 2023a) and even filled (zhao et al., 2023a). this process not only streamlines computational requirements but also enhances the environ- mental sustainability of ai operations, as open-source mod- els become more proficient with lesser computational over- head. furthermore, knowledge distillation fosters a more accessible and equitable ai landscape, where smaller enti- ties and individual researchers gain access to state-of-the-art capabilities, encouraging wider participation and diversity in ai advancements. this democratization of technology leads to more robust, versatile, and accessible ai solutions, catalyzing innovation and growth across various industries and research domains. the escalating need for a comprehensive survey on the knowledge distillation of llms stems from the rapidly evolving landscape of ai (openai et al., 2023; team et al., 2023) and the increasing complexity of these models. as ai continues to penetrate various sectors, the ability to effi- ciently and effectively distill knowledge from proprietary llms to open-source ones becomes not just a technical aspiration but a practical necessity. this need is driven by 3 studentmodelllamagptvicunaopt…… seedknowledgesteerdrivegeneratedknowledgedataset demonstrationsrawdatainput set context followingalignmentagentnlp task specializationmulti-modalityskills lawmedical&healthcarefinancesciencemisc.verticaldomains teacherllm gpt-4 claude llama gemini instructions skill domain knowledgeelicitationdistillationalgorithmtraindivergenceandsimilarity feature featureguide reinforcementlearningoutputsreward rm!(·)distill supervisedfine-tuningx,y preferencerankoptimizationy,1y,2y3y1y2y3≻≻rank…… datacuration x,yrawdatasynthesizefeedbackfeedback input outputself-knowledge outputinputinput ylabellabelingexpansion x,ydemonstrationsexpandfeature featureinput,outputextractsec.4sec.5 sec.3.1sec.3.2 fig. 2: an overview of this survey on knowledge distillation of large language models. note that ‘section’ is abbreviated as ‘sec.’ in this figure. rm s(·)denotes the student reward model. the growing demand for more accessible, cost-effective, and adaptable ai solutions that can cater to a diverse range of applications and users. a survey in this field is vital for synthesizing the current methodologies, challenges, and breakthroughs in knowledge distillation. it may serve as a beacon for researchers and practitioners alike, guiding them through the intricate process of distilling complex ai capabilities into more manageable and accessible forms. moreover, such a survey can illuminate the path forward, identifying gaps in current techniques and proposing direc- tions for future research. survey organization. the remainder of this survey is orga- nized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm ofllms. following this intro- duction, §2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of llms and highlighting the role of data augmentation (da) in this context. §3 delves into the approaches to elicit knowledge from teacher llms and core distillation algorithms, examining methods from supervised fine-tuning to more complex strategies involving divergence and similarity, reinforcement learning, and ranking opti- mization. then, §4 focuses on skill distillation, exploring how student models can be enhanced to improve context understanding, alignment with user intentions, and perfor- mance across a variety of nlp tasks. this includes discus- sions on natural language understanding (nlu), genera- tion (nlg), information retrieval, recommendation systems, and the evaluation of text generation. in §5, we ventureinto domain-specific vertical distillation, showcasing how knowledge distillation techniques are applied within spe- cialized fields such as law, healthcare, finance, and science, illustrating the practical implications and transformative impact of these approaches. the survey suggests open problems in §6, identifying current challenges and gaps in knowledge distillation research that offer opportunities for future work. finally, the conclusion and discussion in §7 synthesize the insights gained, reflecting on the implica- tions for the broader ai and nlp research community and proposing directions for future research. figure 2 shows an overview of this survey. 2 o verview 2.1 comparing traditional recipe the concept of knowledge distillation in the field of ai and deep learning (dl) refers to the process of transferring knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) (gou et al., 2021). this technique is pivotal in mitigating the challenges posed by the computational demands and resource constraints of deploying large-scale models in practical applications. historically, knowledge distillation techniques, prior to the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh the era of llms, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (sanh et al., 2019; kim and rush, 2016). this process was largely driven by the need to deploy machine learning models in resource-constrained environments, such as mobile devices or edge computing platforms, where the computational 4knowledge distillation of llmskd algorithmsknowledgelabelingannollm (he et al., 2023a), pandalm (wang et al., 2023b), cot-distill (hsieh et al., 2023) orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), baize (xu et al., 2023b), mammoth (yue et al., 2023a), mixed distill (chenglin et al., 2023) expansionself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), code alpaca (chaudhary, 2023) self-align (sun et al., 2024b), wizardlm (xu et al., 2023a), wizardcoder (luo et al., 2023a), wizardmath (luo et al., 2023b), auggpt (dai et al., 2023a), tdg (he et al., 2023b) curationultrachat (ding et al., 2023b), phi-1 (gunasekar et al., 2023), phi-1.5 (li et al., 2023a), phi-2 (mar, 2023), magicoder (wei et al., 2023), wavecoder (yu et al., 2024) zerogen (ye et al., 2022), sungen (gao et al., 2023a), inpars (bonifacio et al., 2022) featurebabyllama (timiryasov and tastet, 2023), minillm (gu et al., 2024), gkd (agarwal et al., 2024), quantgpt (tao et al., 2022a), llm-qat (liu et al., 2023a), feedbackcai (bai et al., 2022a), wizardmath (luo et al., 2023b), ultrafeedback (cui et al., 2023a), zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), rlaif (lee et al., 2023a), lion (jiang et al., 2023b), persd (chen et al., 2023a), gkd (agarwal et al., 2024) self-knowledgeself-instruct (wang et al., 2022a), self-align (sun et al., 2024b), rlcd (yang et al., 2024a), impdistill (jung et al., 2023), lmsi (huang et al., 2023a), rest (gulcehre et al., 2023), self-rewarding (yuan et al., 2024a), baize (xu et al., 2023b), star (zelikman et al., 2022) distillationsupervised fine-tuningalpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), self-instruct (wang et al., 2022a), baize (xu et al., 2023b), star (zelikman et al., 2022), divergence and similaritydistilgpt (sanh et al., 2019), f-distill (wen et al., 2023), minillm (gu et al., 2024) ted (liang et al., 2023a), gkd (agarwal et al., 2024),babyllama(timiryasov and tastet, 2023) reinforcement learningcai (bai et al., 2022a), ultrafeedback (cui et al., 2023a), wizardmath (luo et al., 2023b), minillm (gu et al., 2024), gkd (agarwal et al., 2024), gpt3 reward (kwon et al., 2023) rank optimization zephyr (tunstall et al., 2023), cyclealign (hong et al., 2023), skill distillationcontext followinginstruction followingself-instruct (wang et al., 2022a), alpaca (taori et al., 2023), vicuna (chiang et al., 2023), wizardlm (xu et al., 2023a), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), wizardmath (luo et al., 2023b), llama-gpt4 (peng et al., 2023a), multi-turn dialoguevicuna (chiang et al., 2023), baize (xu et al., 2023b), ultrallama (ding et al., 2023b), camel (li et al., 2023b), openchat (wang et al., 2023c), zephyr (tunstall et al., 2023), rag capbility kard (kang et al., 2023a), sail (luo et al., 2023c), self-rag (asai et al., 2023), alignmentthinking patternselfee (ye et al., 2023), orca (mukherjee et al., 2023), orca 2 (mitra et al., 2023), aft (wang et al., 2023d), adaptllm (cheng et al., 2023), knowpat (zhang et al., 2023a), preferencecai (bai et al., 2022a), gpt-3 reward (kwon et al., 2023), ilf (scheurer et al., 2023), almost (kim et al., 2023a), rlef (roit et al., 2023), rlaif (lee et al., 2023a), zephy (tunstall et al., 2023), ultrafeedback (cui et al., 2023a), valuecai (bai et al., 2022a), align honesty (yang et al., 2023a), sandbox (liu et al., 2023b), self-align (sun et al., 2024b), ultrafeedback (cui et al., 2023a), rlcd (yang et al., 2024a) agenttool usingtoolformer (schick et al., 2023), graph-toolformer (zhang, 2023), gorilla (patil et al., 2023), toolalpaca (tang et al., 2023a), toolllm (qin et al., 2023a), craft (yuan et al., 2023a), confucius (gao et al., 2023b), mllm-tool (wang et al., 2024), α-umi (shen et al., 2024), planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task planningfireact (chen et al., 2023b), agenttuning (zeng et al., 2023a), lumos (yin et al., 2023a), autoact (qiao et al., 2024), tptu-v2 (kong et al., 2023), nlp task specializationnluauggpt (dai et al., 2023a), gpt annotation (gilardi et al., 2023), (ding et al., 2023a), tdg (he et al., 2023b), sungen (gao et al., 2023a), mix distill (chenglin et al., 2023), annollm (he et al., 2023a), udg (wang et al., 2021a), zerogen (ye et al., 2022), nlginheritsumm (xu et al., 2023c), recomp (xu et al., 2024b), mario (ramnath et al., 2023), id (jung et al., 2023), gpt-3 labeling (wang et al., 2021b), biogpt (guo et al., 2023a), chatgpt nmt (yang and nicolai, 2023), information retrievalquill (srinivasan et al., 2022), promptgator (dai et al., 2023b), inpars (bonifacio et al., 2022), augtriever (meng et al., 2023), (sun et al., 2023a), rankvicuna (pradeep et al., 2023a), rankzephyr (pradeep et al., 2023b), exaranker (ferraretto et al., 2023), recommendation ndr (mysore et al., 2023), instrcutrec (zhang et al., 2023b), once (liu et al., 2023c), text generation evaluationpandalm (wang et al., 2023b), prometheus (kim et al., 2024), instructscore (xu et al., 2023d), tigerscore (jiang et al., 2023c), auto-j (li et al., 2024a), codecodealpaca (chaudhary, 2023), codellama (rozi `ere et al., 2023), magicoder (wei et al., 2023) phi-1 (gunasekar et al., 2023), persd (chen et al., 2023a), mftcoder (liu et al., 2023d), wavecoder (yu et al., 2024), code clean (jain et al., 2023), multi-modalityllava (liu et al., 2023e), svit (zhao et al., 2023b), lvis-instruct4v (wang et al., 2023e), shikra (chen et al., 2023c), lskd (park et al., 2023), detgpt (pi et al., 2023; zhao et al., 2023c), lrv (liu et al., 2023f), next-gpt (wu et al., 2023b), valley (luo et al., 2023d), iluvui (jiang et al., 2023d), stablellava (li et al., 2023c), pointllm (xu et al., 2023e), verticalization distillationlaw (huang et al., 2023b; cui et al., 2023b); medical & healthcare (zhang et al., 2023c; chen et al., 2023d); finance (zhang and yang, 2023); science (xie et al., 2023a; zhang et al., 2024) and misc. (dan et al., 2023; guo et al., 2023b) fig. 3: taxonomy of knowledge distillation of large language models. the detailed taxonomy of verticalization distillation is shown in figure 7. 5 power and memory are limited. the focus was predomi- nantly on ad-hoc neural architecture selection and training objectives tailored for single tasks. these earlier methods involved training a smaller student network to mimic the output of a larger teacher network, often through techniques like soft target training, where the student learns from the softened softmax output of the teacher. please refer to the survey (gou et al., 2021) for more details on general knowledge distillation techniques in ai and dl. in contrast, the advent of llms has revolutionized the knowledge distillation landscape. the current era of knowledge distillation in llms shifts the focus from mere architecture compression to the more nuanced process of knowledge elicitation and transfer (taori et al., 2023; chaud- hary, 2023; tunstall et al., 2023). this paradigm change is largely due to the expansive and deep-seated knowledge that llms like gpt-4 and gemini possess. and the inacces- sible parameters of llms make it hard to compress them by using pruning (han et al., 2016) or quantization (liu et al., 2023a) techniques. unlike the earlier era, where the goal was to replicate the output behavior of the teacher model or reduce the model size , the current focus in llm-based knowledge distillation is to extract and transfer the rich, nuanced understanding that these models have developed. the key to this modern approach lies in heuristic and carefully designed prompts, which are used to elicit specific knowledge (ding et al., 2023b) or capabilities (chaudhary, 2023) from the llms. these prompts are crafted to tap into the llm’s understanding and capabilities in various domains, ranging from natural language understanding (he et al., 2023a) to more complex cognitive tasks like reason- ing (hsieh et al., 2023) and problem-solving (qiao et al., 2024). the use of prompts as a means of knowledge elici- tation offers a more flexible and dynamic approach to dis- tillation. it allows for a more targeted extraction of knowl- edge, focusing on specific skills or domains of interest. this method is particularly effective in harnessing the emergent abilities of llms, where the models exhibit capabilities beyond their explicit training objectives. furthermore, this era of knowledge distillation also em- phasizes the transfer of more abstract qualities such as reasoning patterns (mitra et al., 2023), preference align- ment (cui et al., 2023a), and value alignment (sun et al., 2024b). this is in stark contrast to the earlier focus on output replication (taori et al., 2023), indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. the current techniques involve not just the replication of outputs, but also the emulation of the thought processes (mitra et al., 2023) and decision-making (asai et al., 2023) patterns of the teacher model. this involves complex strategies like chain-of-thought prompting, where the student model is trained to learn the reasoning process of the teacher, thereby enhancing its problem-solving and decision-making capabilities. 2.2 relation to data augmentation (da) in the era of llms, data augmentation (da) (wang et al., 2022a; ye et al., 2022) emerges as a critical paradigm integral to the process of knowledge distillation. unlike traditional da techniques such as paraphrasing (gangal et al., 2022) orback-translation (longpre et al., 2019), which primarily aim at expanding the training dataset in a somewhat mechanical manner. da within the context of llms focuses on the generation of novel, context-rich training data tailored to specific domains and skills. this innovation is driven by the unique capabilities of llms to generate coherent, diverse, and intricate data samples that closely mimic the nuanced understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed understanding and cognitive abilities of human experts in various fields. the relationship between da and kd in llms is both symbiotic and foundational. by leveraging a set of seed knowledge, kd employs da to prompt llms to produce explicit data that encapsulates specific skills or domain expertise (chaudhary, 2023; west et al., 2022). this method stands out as a potent mechanism for bridging the knowl- edge and capability gap between proprietary and open- source models. through da, llms are prompted to create targeted, high-quality datasets that are not merely larger in volume but are also rich in diversity and specificity. this approach enables the distillation process to be more effec- tive, ensuring that the distilled models not only replicate the teacher model’s output behavior but also embody its deep-seated understanding and cognitive strategies. the significance and necessity of da for achieving kd in the llm era cannot be overstated. da acts as a force multiplier, enabling the distilled models to acquire and re- fine capabilities that would otherwise require exponentially larger datasets and computational resources. it facilitates a more nuanced and effective transfer of knowledge, fo- cusing on the qualitative aspects of learning rather than quantitative expansion. this strategic use of da within kd processes underscores a pivotal shift towards a more efficient, sustainable, and accessible approach to harnessing the power of llms. it empowers open-source models with the ability to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts, thereby democratizing access to advanced ai capabilities and fostering innovation across a broader spectrum of applications and users. 2.3 survey scope building on the discussions introduced earlier, this survey aims to comprehensively explore the landscape of knowl- edge distillation within the context of llms, following a meticulously structured taxonomy as in figure 3. the survey’s scope is delineated through three primary facets: kd algorithms, skill distillation, and verticalization dis- tillation. each facet encapsulates a range of subtopics and methodologies. it’s important to note that kd algorithms provide the technical foundations for skill distillation and verticalization distillation. kd algorithms. this segment focuses on the technical foundations and methodologies of knowledge distillation. it includes an in-depth exploration of the processes involved in constructing knowledge from teacher models (e.g., pro- prietary llms) and integrating this knowledge into student models (e.g., open-source llms). under the umbrella of ‘knowledge ’, we delve into strategies such as labeling (hsieh et al., 2023), expansion (taori et al., 2023), curation (gu- nasekar et al., 2023), feature understanding (agarwal et al., 6 2024), feedback mechanisms (tunstall et al., 2023), and self- knowledge generation (wang et al., 2022a). this exploration seeks to uncover the various ways in which knowledge can be identified, expanded, and curated for effective dis- tillation. the ‘ distillation ’ subsection examines learning ap- proaches like supervised fine-tuning (sft) (wang et al., 2022a), divergence minimization (agarwal et al., 2024), rein- forcement learning techniques (cui et al., 2023a), and rank optimization strategies (tunstall et al., 2023). this analysis aims to illuminate how these algorithms facilitate the trans- fer of knowledge, ensuring that open-source models can replicate and, in some cases, surpass the capabilities of their proprietary counterparts. skill distillation. this facet examines the specific compe- tencies and capabilities enhanced through kd. it encom- passes detailed discussions on context following (taori et al., 2023; luo et al., 2023c), with subtopics like instruction following and retrieval-augmented generation (rag) capa- bility. in the realm of alignment (mitra et al., 2023; tun- stall et al., 2023), the survey investigates thinking patterns, persona/preference modeling, and value alignment. the ‘agent’ category delves into skills such as tool using and planning. nlp task specialization (dai et al., 2023a; jung et al., 2023; chaudhary, 2023) is scrutinized through lenses like natural language understanding (nlu), natural lan- guage generation (nlg), information retrieval, recommen- dation systems, text generation evaluation, and code gen- eration. finally, the survey addresses multi-modality (liu et al., 2023e; zhao et al., 2023b), exploring how kd enhances llms’ ability to interpret and integrate multiple forms of input, enriching their utility and applicability across various contexts. verticalization distillation. this section assesses the ap- plication of kd across diverse vertical domains, offering insights into how distilled llms can be tailored for spe- cialized fields such as law (law, 2023), medical & health- care (wang et al., 2023a), finance (zhang and yang, 2023), science (zhang et al., 2024), among others. this exploration not only showcases the practical implications of kd tech- niques but also highlights their transformative impact on domain-specific ai solutions. through detailed analysis and examples, this part aims to demonstrate the versatility and efficacy of kd in adapting llms to meet the nuanced demands of different industries, thus contributing to the broader ai and ml ecosystem. by navigating through these facets, this survey en- deavors to provide an extensive and nuanced analysis of knowledge distillation in the era of llms. it serves as a guide for researchers, practitioners, and enthusiasts in the field, shedding light on current methodologies, challenges, and opportunities for innovation in this rapidly evolving domain. declaration. this survey represents our earnest effort to provide a comprehensive and insightful overview of knowl- edge distillation techniques applied to llms, focusing on algorithms, skill enhancement, and domain-specific appli- cations. given the vast and rapidly evolving nature of this field, especially with the prevalent practice of elic- iting knowledge from training data across academia, weacknowledge that this manuscript may not encompass every pertinent study or development. nonetheless, it endeavors to introduce the foundational paradigms of knowledge dis- tillation, highlighting key methodologies and their impacts across a range of applications. 2.4 distillation pipeline in llm era seedknowledgeskill/domain teacherllmknowledgeelicitationstudentmodeldistillationalgorithmsteer drivegeneratedknowledgelearningobjectivetrain fig. 4: an illustration of a general pipeline to distill knowl- edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge edge from a large language model to a student model. the general distillation pipeline of llms is a structured and methodical process aimed at transferring knowledge from a sophisticated teacher model to a less complex student model. this pipeline is integral for leveraging the advanced capabilities of models like gpt-4 or gemini in more acces- sible and efficient open-source counterparts. the outline of this pipeline can be broadly categorized into four distinct stages, each playing a crucial role in the successful distilla- tion of knowledge. an illustration is shown in figure 4. the detailed pipeline could also be seen in figure 2. i. target skill or domain steering teacher llm. the first stage involves directing the teacher llm towards a specific target skill or domain. this is achieved through care- fully crafted instructions or templates that guide the llm’s focus. these instructions are designed to elicit responses that demonstrate the llm’s proficiency in a particular area, be it a specialized domain like healthcare or law, or a skill such as reasoning or language understanding. the objective here is to utilize the teacher llm’s extensive training and nuanced capabilities to generate outputs that are rich in the specific knowledge or skills desired for the student model. ii. seed knowledge as input. once the target area is defined, the next step is to feed the teacher llm with seed knowledge. this seed knowledge typically comprises a small dataset or specific data clues relevant to the elicit skill or domain knowledge from the teacher llm. it acts as a catalyst, prompting the teacher llm to generate more elaborate and detailed outputs based on this initial infor- mation. the seed knowledge is crucial as it provides a foundation upon which the teacher model can build and expand, thereby creating more comprehensive and in-depth knowledge examples. iii. generation of distillation knowledge. in response to the seed knowledge and steering instructions, the teacher llm generates knowledge examples. these examples are predominantly in the form of question-and-answer (qa) dialogues or narrative explanations, aligning with the nat- ural language processing/understanding capabilities of the 7 llm. in certain specialized cases, the outputs may also in- clude logits or hidden features, although this is less common due to the complexity and specific requirements of such data forms. the generated knowledge examples constitute the core of the distillation knowledge, encapsulating the advanced understanding and skills of the teacher llm. iv . training the student model with a specific learn- ing objective. the final stage involves the utilization of the generated knowledge examples to train the student model. this training is guided by a loss function that aligns with the learning objectives. the loss function quantifies the student model’s performance in replicating or adapting the knowledge from the teacher model. by minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. the process involves iteratively adjusting the student model’s parameters to reduce the discrepancy be- tween its outputs and those of the teacher model, ensuring the effective transfer of knowledge. in essential, the above four stages can be abstracted as two formulations. the first formulation represents the process of eliciting knowledge: d(kd) i={parse( o, s)|o∼pt(o|i⊕s),∀s∼ s} , (1) where ⊕denotes fusing two pieces of text, idenotes an instruction or a template for a task, skill, or domain to steer the llm and elicit knowledge, s∼ s denotes an example of the seed knowledge, upon which the llm can explore to generate novel knowledge, parse( o, s)stands for to parse the distillation example ( e.g., (x, y)) from the teacher llm’s output o(plus the input sin some cases), andptrepresents the teacher llm with parameters θt. given the datasets d(kd) ibuilt for distillation, we then define a learning objective as l=x ili(d(kd) i;θs), (2) wherep idenotes there could be multiple tasks or skills being distilled into one student model, li(·;·)stands for a specific learning objective, and θsparameterizes the student model. following our exploration of the distillation pipeline and the foundational concepts underlying knowledge distilla- tion in the llm era, we now turn our focus to the specific algorithms that have gained prominence in this era. 3 k nowledge distillation algorithms this section navigates through the process of knowledge distillation. according to section 2.4, it is categorized into two principal steps: ‘knowledge,’ focusing on eliciting knowledge from teacher llms (eq.1), and ‘distillation,’ centered on injecting this knowledge into student models (eq.2). we will elaborate on these two processes in the subsequent sections. 3.1 knowledge this section focuses on the approaches to elicit knowledge from teacher llms. according to the manners to acquire knowledge, we divided them into labeling ,expansion ,datacuration ,feature ,feedback , and self-knowledge . figure 5 shows an illustration of these knowledge elicitation meth- ods. 3.1.1 labeling labeling knowledge refers to using a teacher llm to label the output yfor a given input xas the seed knowledge, according to the instruction ior demonstrations c, where c= (x1, y1), . . . , (xn, yn). this method of eliciting knowl- edge from teacher llms is straightforward yet effective and has been widely applied across various tasks and appli- cations. it requires only the collection of an input dataset and feeding it into llms to obtain the desired generations. moreover, the generation of yis controllable through the predefined iandc. this process can be formulated as follows: d(lab)={x, y|x∼ x, y∼pt(y|i⊕c⊕x)}. (3) input xcould be sourced from existing nlp task datasets, which serve as typical reservoirs for distillation efforts. numerous works have sought to harness the capa- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- bilities of powerful llms as teachers for annotating dataset samples across a range of tasks. for instance, efforts in natural language understanding involve using llms to cat- egorize text (gilardi et al., 2023; ding et al., 2023a; he et al., 2023a), while in natural language generation, llms assist in generating sequences for outputs (hsieh et al., 2023; jung et al., 2023; wang et al., 2021b). text generation evaluation tasks leverage llms to label evaluated results (li et al., 2024b; wang et al., 2023b), and reasoning tasks utilize llms for labeling chains of thought (cot) explanations (hsieh et al., 2023; li et al., 2022; ho et al., 2023; magister et al., 2023; fu et al., 2023; ramnath et al., 2023; li et al., 2023d; liu et al., 2023g), among others. rather than concentrating on specific tasks, many current works focus on labeling outputs based on instructions, thereby teaching student models to solve tasks in a more flexible way by following in- structions. collections of various nlp tasks, complemented by instructional templates, serve as valuable input sources forx. for instance, flan-v2 collections (longpre et al., 2023) offers extensive publicly available sets of tasks with instructions, which are labeled with responses generated by teacher llms in orca (mukherjee et al., 2023; mitra et al., 2023). the instructions from these nlp tasks are built from predefined templates, which lack diversity and may have gaps between human’s natural query. the real conversations between humans and chat models provide large-scale data with real queries and generations labeled by powerful llms, like sharegpt. additionally, xu et al. (2023b) and anand et al. (2023) label the real questions sampled from forums like quora and stack overflow. moreover, the process of labeling could be guided by instructions ior demonstrations c. a commonly used in- struction type for guiding labeling is chain-of-thought (cot) prompt (hsieh et al., 2023; fu et al., 2023; magister et al., 2023). mukherjee et al. (2023) add multiple system messages (e.g. “you must generate a detailed and long answer.” or “explain like i’m five, think step-by-step”) to elicit rich signals. yue et al. (2023a) and chenglin et al. (2023) la- bel a hybrid of knowledge of chain-of-thought (cot) and 8 𝑐𝐼labelingexpansion𝑥𝐼𝑦𝑥𝑦expandcompleteupdatedata curation 𝑚meta sources 𝐼𝑥𝑦 input set completecreatesamplegenerate 𝑚meta-information𝑐demonstrations𝑥𝐼 𝑦 filterfeedback extractfeature𝑥𝑦 distributionintermediatefeature 𝑥input𝑦output𝐼instruction𝑦! 𝑦\" 𝑦# 𝑥guidefeedback𝑦#∗ 𝑦# feedback self-knowledge studentteacher generate≻≻𝑦\" 𝑦! 𝑦# 𝑥 𝑥& correctexpand𝑐 fig. 5: an illustration of different knowledge elicitation methods from teacher llms. labeling : the teacher generates the output from the input; expansion : the teacher generates samples similar to the given demonstrations through in- context learning; data curation : the teacher synthesizes data according to meta-information, such as a topic or an entity; feature : feed the data into the teacher and extract its internal knowledge, such as logits and features; feedback : the teacher provides feedback on the student’s generations, such as preferences, corrections, expansions of challenging samples, etc; self-knowledge : the student first generates outputs, which is then filtered for high quality or evaluated by the student itself. program-of-thought (pot) rationales. xu et al. (2023b) pro- pose a self-chat technique that two teacher llms simulate the real conversational to generate multi-turn dialogues for a question from quora and stack overflow. 3.1.2 expansion while the labeling approach is simple and effective, it faces certain limitations. primarily, it is constrained by the scale and variety of the input data. in real-world applications, especially those involving user conversations, there are also concerns regarding the privacy of the data involved. to address these limitations, various expansion methods have been proposed (wang et al., 2022a; taori et al., 2023; chaud- hary, 2023; si et al., 2023; ji et al., 2023a; luo et al., 2023b,a; wu et al., 2023c; sun et al., 2024b; xu et al., 2023a; guo et al., 2023c; rozi `ere et al., 2023; west et al., 2022). these methods take the demonstrations as seed knowledge and aim to expand a large scale and various data by in-context learning. a key characteristic of these expansion methods is the utilization of the in-context learning ability of llms to gen- erate data similar to the provided demonstrations c. unlike in the labeling approach, where the input xis sampled from the existing dataset, in the expansion approach, both x andyare generated by teacher llms. this process can be formulated as follows: d(exp)={(x, y)|x∼pt(x|i⊕c), y∼pt(y|i⊕x)}.(4) in this formulation, xand yrepresent the new input- output pairs generated by the teacher llm. the input x is generated based on a set of input-output demonstrations c. the output yis then generated in response to the new input xunder the guidance of an instruction i. note thatthe demonstrations could be predefined or dynamically updated by adding the newly generated samples. expansion techniques have been widely utilized to extract extensive instruction-following knowledge from teacher llms. wang et al. (2022a) first introduces an iter- ative bootstrapping method, self-instruct, to utilize llms to generate a wide array of instructions based on sev- eral demonstrations sampled from 175 manually-written in- structions. the newly generated instructions are then added back to the initial pool, benefiting subsequent expansion iterations. subsequently, taori et al. (2023) applies this ex- pansion method to a more powerful teacher llm, text- davinci-003, to distill 52k high-quality data. to improve the diversity and coverage during expansion, wu et al. (2023c) and (sun et al., 2024b) prompt the teacher llm to generate instructions corresponding to some specific topics. xu et al. (2023a) propose an evol-instruct method to ex- pand the instructions from two dimensions: difficulty (e.g. rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). this evol- instruct method is domain-agnostic and has been used to expand the distillation of coding (luo et al., 2023a) and math (luo et al., 2023b). additionally, expansion methods can significantly augment nlp task datasets with similar samples, thereby enhancing task performance. for instance, auggpt (dai et al., 2023a) leverages a teacher llm to rephrase each sentence in the training samples into multi- ple conceptually similar, but semantically varied, samples to improve classification performance. similarly, tdg (he et al., 2023b) proposes the targeted data generation (tdg) framework, which automatically identifies challenging sub- groups within data and generates new samples for these subgroups using llms through in-context learning. in summary, the expansion method leverages the in- 9 context learning strengths of llms to produce more var- ied and extensive datasets with both inputs and outputs. however, the quality and diversity of the generated data are heavily reliant on the teacher llms and the initial seed demonstrations. this dependence can lead to a dataset with inherent bias from llms (yu et al., 2023a; wei et al., 2023) and a homogeneity issue where the generations may be prone to similarity ultimately, limiting the diversity this method seeks to achieve (ding et al., 2023b). moreover, the expansion process may inadvertently amplify any biases present in the seed data. 3.1.3 data curation the pursuit of high-quality and scalable data generation in knowledge distillation from llms has led to the emergence of the data curation approach. this method arises in re- sponse to the limitations observed in both the labeling and expansion approaches. these methods often yield data of variable quality and face constraints in quantity. in labeling, the seed knowledge is sourced from task datasets, leading to potential noise and dirty data. meanwhile, in expansion, the input xis derived from seed demonstrations, which can result in homogeneous data when generated in large quantities. to overcome these challenges, the data curation method curates high-quality or large-scale data by extensive meta-information as seed knowledge (ding et al., 2023b; gunasekar et al., 2023; li et al., 2023a; mar, 2023; liu et al., 2023d; wei et al., 2023; yu et al., 2024; ye et al., 2022; gao et al., 2023a; yang and nicolai, 2023). a distinct feature of data curation is its approach to synthesize data from scratch. numerous diverse meta- information, such as topics or knowledge points, could be incorporated into this process to generate controllable x andy. thus, this process can be meticulously controlled to yield datasets that are not only large in scale but also of high quality. the formulation for data curation can be represented as: d(cur)={(x, y)|x∼pt(x|i⊕m), y∼pt(y|i⊕x)}.(5) in this formulation, mrepresents the diverse meta- information used to guide the synthesis of x, and iis the instruction guiding teacher llms to generate xory. different studies primarily vary in their source and method of leveraging meta-information. ultrachat (ding et al., 2023b) effectively demonstrates the process of curating both high-quality and diverse data by distilled knowledge. they collect extensive meta-information across three do- mains: questions about the world, creation and generation , and assistance on existing materials . for example, under questions about the world , they explore 30 meta-topics like ”technology” and ”food and drink.” the teacher llms then use this meta-information to distill a broad array of instructions and conversations, achieving a substantial scale of 1.5 million instances. ultrachat stands out with its lexical and topical diversity. the ultrallama model, fine- tuned on this data, consistently surpasses other open-source models. another notable series, phi(gunasekar et al., 2023; li et al., 2023a; mar, 2023), focuses on distilling smaller, high-quality datasets akin to ”textbooks.” phi-1 (gunasekar et al., 2023) experiments with synthesizing ”textbook qual- ity” data in the coding domain. their approach involvesdistilling clear, self-contained, instructive, and balanced con- tent from llms, guided by random topics or function names to enhance diversity. the distilled data is a synthesis of 1 billion tokens of python textbooks, complete with natural language explanations and code snippets, as well as 180 mil- lion tokens of python exercises with solutions. remarkably, thephi-1 model, despite its smaller size, outperforms nearly all open-source models on coding benchmarks like hu- maneval and mbpp while being 10 times smaller in model size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in size and 100 times smaller in dataset size. mftcoder (liu et al., 2023d) utilizes hundreds of python knowledge points as meta-information to create a codeexercise dataset. in contrast, magicoder (wei et al., 2023) and wavecoder (yu et al., 2024) get raw code collections from open-source code datasets, using this as meta-information for generating instructional data. in the context of nlu tasks, certain studies (ye et al., 2022; gao et al., 2023a; wang et al., 2021a) explore the use of labels as meta-information to synthesize corresponding samples for data augmentation. similarly, in information retrieval tasks, there are efforts to utilize docu- ments as meta-information for generating potential queries, thereby constructing large-scale retrieval pairs (bonifacio et al., 2022; meng et al., 2023). in conclusion, data curation through teacher llms has emerged as a promising technique for synthesizing datasets that are not only high-quality and diverse but also large in scale. the success of models like phi-1 in specialized domains underscores the efficacy of this method. the ability to create synthetic datasets will become a crucial technical skill and a key area of focus in ai (li et al., 2023a). 3.1.4 feature the previously discussed knowledge elicitation methods are typically applied to powerful black-box models, which are expensive and somewhat unreproducible due to calling api. in contrast, white-box distillation offers a more trans- parent and accessible approach for researchers. it involves leveraging the output distributions, intermediate features, or activations from teacher llms, which we collectively refer to as feature knowledge. white-box kd approaches have predominantly been studied for smaller encoder-based lms, typically those with fewer than 1 billion parameters (cf. gou et al. (2021) for detail). however, recent research has begun to explore white-box distillation in the context of generative llms (timiryasov and tastet, 2023; liang et al., 2023a; gu et al., 2024; agarwal et al., 2024; liu et al., 2023a; wen et al., 2023; wan et al., 2024a; zhao and zhu, 2023; qin et al., 2023b; boizard et al., 2024; zhong et al., 2024). the typical method for acquiring this feature knowledge involves teacher llms annotating the output sequence y with its internal representations. these annotations are then distilled into the student model using methods such as kullback-leibler divergence (kld). the process of eliciting feature knowledge can be formulated as follows: d(feat)={(x, y, ϕ feat(x, y;θt))|x∼ x, y∼ y} . (6) in this formulation, yis the output set, which can be generated by teacher llms, the student model, or directly sourced from the dataset. ϕfeat(·;θt)represents the opera- tion of extracting feature knowledge (such as output distri- bution) from the teacher llm. 10 the most straightforward method to elicit feature knowl- edge of teacher is to label a fixed dataset of sequences with token-level probability distributions (sanh et al., 2019; wen et al., 2023). to leverage the rich semantic and syntactic knowledge in intermediate layers of the teacher model, ted (liang et al., 2023a) designs task-aware layer-wise distillation. they align the student’s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task. gu et al. (2024) and agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, termed ‘self- generated sequences.’ the student then learns by using feedback (i.e. output distribution) from teacher on these sequences. this method is particularly beneficial when the student model lacks the capacity to mimic teacher’s distri- bution. moreover, various llm-quantization methods with distilling feature knowledge from teacher llms have been proposed (tao et al., 2022a; liu et al., 2023a; kim et al., 2023b). these methods aim to preserve the original output distribution when quantizing the llms, ensuring minimal loss of performance. additionally, feature knowledge could serve as a potent source for multi-teacher knowledge distil- lation. timiryasov and tastet (2023) leverages an ensemble of gpt-2 and llama as teacher models to extract output distributions. similarly, fusellm (wan et al., 2024a) inno- vatively combines the capabilities of various llms through a weighted fusion of their output distributions, integrating them into a singular llm. this approach has the potential to significantly enhance the student model’s capabilities, surpassing those of any individual teacher llm. in summary, feature knowledge offers a more transpar- ent alternative to black-box methods, allowing for deeper insight into and control over the distillation process. by utilizing feature knowledge from teacher llms, such as output distributions and intermediate layer features, white- box approaches enable a more nuanced transfer of informa- tion. while showing promise, especially in smaller models, its application is not suitable for black-box llms where internal parameters are inaccessible. furthermore, student models distilled from white-box llms may underperform compared to their black-box counterparts, as the black-box teacher llms (e.g. gpt-4) tend to be more powerful. 3.1.5 feedback most previous works predominantly focus on one-way knowledge transfer from the teacher to the student for imitation, without considering feedback from the teacher on the student’s generation. the feedback from the teacher typically offers guidance on student-generated outputs by providing preferences, assessments, or corrective informa- tion. for example, a common form of feedback involves teacher ranking the student’s generations and distilling this preference into the student model through reinforcement learning from ai feedback (rlaif) (bai et al., 2022a). here is a generalized formulation for eliciting feedback knowledge: d(fb)={(x, y, ϕ fb(x, y;θt))|x∼ x, y∼ps(y|x)}, (7) where ydenotes the output generated by the student model in response to x, and ϕfb(·;θt))represents providing feedback from teacher llms. this operation evaluates thestudent’s output ygiven the input x, by offering assess- ment, corrective information, or other forms of guidance. this feedback knowledge can not only be distilled into the student to also generate feedback (such as creating a student preference model) but, more importantly, enable the student to refine its responses based on the feedback. various methods have been explored to elicit this advanced knowledge (bai et al., 2022a; luo et al., 2023b; cui et al., 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). 2023a; kwon et al., 2023; jiang et al., 2023b; chen et al., 2023a; gu et al., 2024; agarwal et al., 2024; chen et al., 2024b; guo et al., 2024; ye et al., 2023; hong et al., 2023; lee et al., 2023a). preference, as previously discussed, represents a notable form of feedback knowledge from teacher models. various knowledge of preferences could be distilled from teachers by prompting it with specific criteria. bai et al. (2022a) in- troduce rlaif for distilling harmlessness preferences from llms. this involves using an sft-trained llm to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. this dataset is distilled into a preference model (pm), which then guides the rl training of a more harmless llm policy. wizard- math (luo et al., 2023b) places emphasis on mathematical reasoning. they employ chatgpt as teacher to directly provide process supervision and evaluate the correctness of each step in the generated solutions. to scale up high- quality distilled preference data, cui et al. (2023a) develop a large-scale preference dataset for distilling better preference models, ultrafeedback. it compiles various instructions and models to produce comparative data. then, gpt-4 is used to score candidates from various aspects of preference, including instruction-following, truthfulness, honesty and helpfulness. beyond merely assessing student generations, teachers can also furnish extensive feedback on instances where students underperform. in lion (jiang et al., 2023b), teacher model pinpoints instructions that pose challenges to the student model, generating new, more difficult instructions aimed at bolstering the student’s abilities. persd (chen et al., 2023a) showcases a method where teacher offers tailored refinement feedback on incorrect code snippets gen- erated by students, guided by the specific execution errors encountered. similarly, selfee (ye et al., 2023) leverages chatgpt to generate feedback and revise the student’s answer based on the feedback. in contrast, figa (guo et al., 2024) revises the student’s response by comparing it to the ground-truth response. furthermore, teacher model’s distribution over the student’s generations can itself act as a form of feedback. minillm (gu et al., 2024) and gkd (agarwal et al., 2024) present an innovative strategy wherein the student model initially generates sequences, followed by teacher model producing an output distribution as feedback. this method leverages the teacher’s insight to directly inform and refine the student model’s learning process. 3.1.6 self-knowledge the knowledge could also be elicited from the student itself, which we refer to as self-knowledge . in this setting, the same model acts both as the teacher and the student, iteratively improving itself by distilling and refining its own previously 11 generated outputs. this knowledge uniquely circumvents the need for an external, potentially proprietary, powerful teacher model, such as gpt-series llms. furthermore, it allows the model to surpass the limitations or “ceiling” inherent in traditional teacher-student methods. eliciting self-knowledge could be formulated as: d(sk)={(x, y, ϕ sk(x, y))|x∼ s, y∼ps(y|i⊕x)},(8) where ϕsk(·)is a generalized function that represents an additional process to the self-generated outputs y, which could include but is not limited to filtering, rewarding, or any other mechanisms for enhancing or evaluating y. it could be governed by external tools or the student itself θs. recent research in this area has proposed various innovative methodologies to elicit self-knowledge, demonstrating its potential for creating more efficient and autonomous learn- ing systems. (allen-zhu and li, 2020; wang et al., 2022a; sun et al., 2024b; yang et al., 2024a; jung et al., 2023; huang et al., 2023a; gulcehre et al., 2023; yuan et al., 2024a; xu et al., 2023b; zelikman et al., 2022; chen et al., 2024a; zheng et al., 2024; li et al., 2024c; zhao et al., 2024; singh et al., 2023; chen et al., 2024c; hosseini et al., 2024) a notable example of this methodology is self- instruct (wang et al., 2022a), which utilizes gpt-3 for data augmentation through the expansion approach, gen- erating additional data samples to enhance the dataset. this enriched dataset subsequently fine-tunes the original model. other methods aim to elicit targeted knowledge from student models by modifying prompts, and leveraging these data for further refinement. in self-align (sun et al., 2024b), they find that models fine-tuned by self-instruct data tend to generate short or indirect responses. they prompt this model with verbose instruction to produce in- depth and detailed responses. then, they employ context- distillation (askell et al., 2021) to distill these responses paired with non-verbose instructions back to the model. similarly, rlcd (yang et al., 2024a) introduces the use of contrasting prompts to generate preference pairs from an unaligned llm, encompassing both superior and inferior examples. a preference model trained on these pairs then guides the enhancement of the unaligned model through reinforcement learning. several other approaches employ filtering methods to refine self-generated data. for exam- ple, impossible distillation (jung et al., 2023) targets sen- tence summarization tasks, implementing filters based on entailment, length, and diversity to screen self-generated summaries. lmsi (huang et al., 2023a) generates multiple cot reasoning paths and answers for each question, and then retains only those paths that lead to the most consistent answer. note that refined self-knowledge can be iteratively ac- quired as the student model continuously improves, further enhancing the student’s capabilities. this is gulcehre et al. (2023) introduces a reinforced self-training (rest) frame- work that cyclically alternates between grow andimprove stages to progressively obtain better self-knowledge and refine the student model. during the grow stage, the student model generates multiple output predictions. then, in the improve stage, these self-generated outputs are ranked and filtered using a scoring function. subsequently, the lan- guage model undergoes fine-tuning on this curated dataset,employing an offline rl objective. self-play (chen et al., 2024a) introduces a framework resembling iterative dpo, where the language model is fine-tuned to differentiate the self-generated responses from the human-annotated data. these self-generated responses could be seen as “negative knowledge” to promote the student to better align with the target distribution. self-rewarding (yuan et al., 2024a) explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for explores a novel and promising approach by utilizing the language model itself as a reward model. it employs llm- as-a-judge prompting to autonomously assign rewards for the self-generated responses. the entire process can then be iterated, improving instruction following and reward modeling capabilities. 3.2 distillation this section focuses on the methodologies for effectively transferring the elicited knowledge from teacher llms into student models. we explore a range of distillation tech- niques, from the strategies that enhance imitation by su- pervised fine-tuning ,divergence and similarity , to advanced methods like reinforcement learning and rank optimization , as shown in figure 3. 3.2.1 supervised fine-tuning supervised fine-tuning (sft), or called sequence-level kd (seqkd) (kim and rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-box llms. sft finetunes student model by maximizing the like- lihood of sequences generated by the teacher llms, aligning the student’s predictions with those of the teacher. this process can be mathematically formulated as minimizing the objective function: lsft=ex∼x,y∼pt(y|x)[−logps(y|x)], (9) where yis the output sequence produced by the teacher model. this simple yet highly effective technique forms the basis of numerous studies in the field. numerous re- searchers have successfully employed sft to train student models using sequences generated by teacher llms (taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; luo et al., 2023b). additionally, sft has been ex- plored in many self-distillation works (wang et al., 2022a; huang et al., 2023c; xu et al., 2023b; zelikman et al., 2022). due to the large number of kd works applying sft, we only list representative ones here. more detailed works can be found in §4. 3.2.2 divergence and similarity this section mainly concentrates on algorithms designed for distilling feature knowledge from white-box teacher llms, including distributions and hidden state features. these algorithms can be broadly categorized into two groups: those minimizing divergence in probability distributions and those aimed at enhancing the similarity of hidden states. divergence. divergence-based methods minimize diver- gence between the probability distributions of the teacher 12 divergence type d(p, q)function forward kldpp(t) logp(t) q(t) reverse kldpq(t) logq(t) p(t) js divergence1 2\\x10pp(t) log2p(t) p(t)+q(t)+pq(t) log2q(t) p(t)+q(t)\\x11 table 1: functional forms of dfor various divergence types. p: reference similarity function lf expression l2-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥2 l1-norm distance ∥φt(ft(x, y))−φs(fs(x, y))∥1 cross-entropy loss −pφt(ft(x, y)) log(φ s(fs(x, y))) maximum mean discrepancy mmd (φt(ft(x, y)),φs(fs(x, y))) table 2: summary of similarity functions in knowledge distillation. and student models, represented by a general divergence function d: ldiv= e x∼x,y∼y[d(pt(y|x), ps(y|x))], (10) the specific form of dvaries depending on the type of divergence employed. table 1 outlines the functional forms ofdfor different divergence measures. the commonly-used standard kd objectives essentially minimize the approxi- mated forward kullback-leibler divergence (kld) between the teacher and the student distribution (sanh et al., 2019; wen et al., 2023; timiryasov and tastet, 2023; liang et al., 2023a; chen et al., 2024d) , which forces psto cover all the modes of pt. however, when a student model is unable to learn all modes of a highly complex teacher, the re- sultant “mode-covering” behavior might cause the student to assign probability mass to tokens with low probability under the teacher’s distribution (cf. figure 6 blue curve). this mode-covering phenomenon can potentially lead to hallucinations and low-quality generations. alternatively, mode-seeking divergences like reverse kl prioritize tokens where the teacher assigns high probabilities (cf. figure 6 green curve). this approach can mitigate the risk of low- quality outputs, fostering more accurate generations. how- ever, it often does so at the cost of reduced diversity. gu et al. (2024) adopt reverse kl divergence to prevent students from overestimating low-probability regions of the teacher’s distribution, employing policy gradient methods for opti- mization. both agarwal et al. (2024) and sason and verd ´u (2016) assess the efficacy of different divergence functions in llm distillation, finding the optimal divergence to be task-dependent. for instance, forward kl divergence is more suitable for tasks like machine translation, where the output has fewer modes or variations, while reverse kl divergence is preferable for tasks like dialogue generation and instruction tuning, which involve multiple modes and a wider range of potential responses. thus, the nature of the task significantly influences the selection of the divergence function for optimal performance. similarity. similarity-based methods in knowledge distilla- tion aim to align the hidden states or features of the student pargminqkl(p||q)argminqkl(q||p)fig. 6: comparison of forward and reverse kl diver- gences in approximating a target distribution . forward kl divergence approach tends to cover all modes of the target distribution but is less precise, i.e. “mode-covering” behavior. reverse kl divergence method focuses predom- inantly on the most prominent mode, thereby exhibiting a “mode-seeking” behavior. model with those of the teacher. these methods use various similarity metrics to measure and optimize the congruence of internal representations between the two models. the objective is to ensure that the student model not only produces similar outputs to the teacher but also processes information in a comparable manner. the formulation for a similarity-based objective might look like this: lsim= e x∼x,y∼y[lf(φt(ft(x, y)),φs(fs(x, y)))],(11) where ft(x, y)andfs(x, y)are the feature maps of the teacher and student models, respectively. the transforma- tion functions φtandφsare applied to these feature maps to ensure they are in the same shape, facilitating direct comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based comparison. the similarity function lfis used to match these transformed feature maps. table 2 shows common choices for lf. few works have employed similarity-based methods in the kd of llms. among them, liang et al. (2023a) propose task-aware layer-wise distillation (ted), a method that utilizes task-aware filters. these filters are designed to selectively capture the most pertinent informa- tion for a specific task from the teacher model. the key objective is to minimize the discrepancy between the filtered representations in both teacher and student models. while similarity-based approaches are common in encoder-based lms (sun et al., 2019, 2020; jiao et al., 2020; hou et al., 2020; zuo et al., 2022; liang et al., 2021), their application in llm knowledge distillation is not as widespread. however, considering their effectiveness, we anticipate an increase in research exploring these methods for llm distillation in the near future. 3.2.3 reinforcement learning this section explores advanced methods of distilling knowl- edge into student models using reinforcement learning (rl). this approach is especially relevant for leveraging the feed- back from teacher to train student models (bai et al., 2022a; cui et al., 2023a; luo et al., 2023b; agarwal et al., 2024; chen et al., 2024b; ma et al., 2023a; pang et al., 2023; du et al., 2023a). the rl-based distillation process typically involves two main stages: 13 distilled reward model training. the first stage involves training a reward model rϕusing the feedback data d(fd) generated by teacher llms. preference data, as one of the typical feedback, is employed to train the student reward model (bai et al., 2022a; cui et al., 2023a; lee et al., 2023a; kim et al., 2023a). they usually consist of input-output pairs (x, yw, yl). here, ywandylrepresent “winning” and “losing” outputs relative to the teacher’s preferences. the loss function for the reward model is defined as: lrm(rϕ,d(fd)) =− e (x,yw,yl)∼d(fd)[logσ(rϕ(x, yw)−rϕ(x, yl))] (12) this formulation guides the reward model to correctly distinguish between more and less preferable outputs based on the teacher’s criteria. instead of learning the instance- level rewards, rlmec (chen et al., 2024b) adopts a dif- ferent approach by training a generative reward model. it is trained on an erroneous solution rewriting data distilled from a teacher llm. this distilled reward model can pro- duce token-level rewards for rl training. reinforcement learning optimization. in the second stage, the student model, represented by a policy πθ, is optimized to maximize the expected reward as per the trained reward model. simultaneously, it minimizes the divergence from a reference policy πref, typically the initial policy of the student model trained by sft, controlled by a factor β. the rl objective is given by: max πθe x∼x,y∼πθ(y|x)[rϕ(x, y)]−βdkl[πθ(y|x)∥πref(y|x)] (13) this rl framework not only ensures that the student model learns the explicit content from the teacher but also effec- tively adopts the teacher’s preference patterns. the use of rl, particularly with the ppo (schulman et al., 2017) algo- rithm, offers a robust mechanism for aligning the student model’s outputs with the teacher. alternatively, the teacher llm can also serve as the reward model to directly assign rewards during rl, circumventing the need for training a reward model (lee et al., 2023a; kwon et al., 2023). while this approach may exhibit superior performance, it comes at a higher computational cost compared to employing a smaller distilled reward model. 3.2.4 ranking optimization ranking optimization presents a stable and computationally efficient alternative to rl for injecting preference feedback into language models (rafailov et al., 2023; song et al., 2023a; yuan et al., 2023b). this method, diverging from traditional rl approaches, directly incorporates ranking information into language models from a fixed preference dataset during fine-tuning. intuitively, it directly updates policy to increase the relative likelihood of preferred over less favored responses. this direct optimization of prefer- ences, without the need for sampling outputs, makes the process more stable and efficient. recently, some works have been proposed to explore using ranking optimization todistill teacher’s preferences into student models (tunstall et al., 2023; hong et al., 2023; yuan et al., 2024a). zephyr (tunstall et al., 2023) utilizes direct preference optimization (dpo) (rafailov et al., 2023) to distill the preference alignment in teacher llms. dpo streamlines the objective of reinforcement learning (as in eq. 13), which involves reward maximization with a kl-divergence constraint, into a single-stage policy training. specifically, dpo’s training goal is to maximize the following expecta- tion: e (x,yw,yl)∼d(fd)\\x14 logσ\\x12 βlogπθ(yw|x) πref(yw|x)−βlogπθ(yl|x) πref(yl|x)\\x13\\x15 , (14) where ywis preferred over ylaccording to the teacher llm. hong et al. (2023) (hong et al., 2023) adopt two ranking-based optimization objectives, rank responses to align human feedback (rrhf) (yuan et al., 2023b) and preference ranking optimization (pro) (song et al., 2023a), for preference distillation. rrhf (yuan et al., 2023b) focuses on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj on a ranking loss defined as: lrrhf =x ri<rjmax(0 , pi−pj), (15) where riandrjare the reward scores assigned by the teacher llm for responses yiandyj, respectively, and pi,pj are their corresponding conditional log probabilities under the policy πθ. this approach emphasizes direct comparison and ranking of responses based on the teacher’s preferences. pro (song et al., 2023a) expands the concept of pairwise comparison to handle preference rankings of any length. for a given instruction xand a sequence of responses ordered by teacher preference as y1≻y2≻...≻yn, the rpo training objective is: lpro=−n−1x k=1logexp (pk)pn i=kexp (pi), (16) where pkrepresents the conditional log probabilities for ykunder the student policy πθ. by iteratively contrasting the likelihood of generating responses, pro optimizes the student lm to prioritize the most preferred response while progressively ranking the rest in the order of diminishing preference. 4 s kill distillation building upon the foundation laid out in section 3 about eliciting knowledge and distillation algorithms, we shift our focus to how these techniques facilitate the distillation of specific skills in llms. our exploration will encompass a diverse range of skills exhibited by llms, including context following ,alignment ,agent ,nlp task specializa- tion and multi-modality .context following focuses on the student’s ability to comprehend and respond effectively to input information. alignment delves into the student’s capability to align its output with the teacher’s responses. moving forward, agent underscores the autonomous nature of language models. nlp task specialization highlights the llm’s versatility in specializing across various natural language processing tasks, demonstrating its adaptability. 14 methods skill seed knowledge teacher llm student model knowledge elicitation objective context following self-instruct (wang et al., 2022a) if 175 human-curated tasks gpt3 gpt3 expansion + self-knowledge sft alpaca (taori et al., 2023) if 175 human-curated tasks gpt3 llama expansion + self-knowledge sft lamini-lm (wu et al., 2023c) if3.5k wikipedia categories + mixed datasetchatgpt various models expansion sft wizardlm (xu et al., 2023a) if alpaca data chatgpt llama expansion sft lion (jiang et al., 2023b) if alpaca cata chatgpt llama labeling + expansion + feedback - babyllama (timiryasov and tastet, 2023) if 10m-word babylm dataset gpt-2 + small llama 58m-parameter llama feature d&s minillm (gu et al., 2024) if dolly dataset gpt2 + opt + llama gpt2 + opt + llama feature d&s self-align (sun et al., 2024b) if human-written principles llama llama expansion + self-knowledge sft self-rewarding (yuan et al., 2024a) if human-written samples llama llama self-knowledge sft + rl star (zelikman et al., 2022) if arithmetic + commonsenseqa + gsm8k gpt-j gpt-j self-knowledge sft llama-gpt4 (peng et al., 2023a) if alpaca dataset gpt4 llama labeling sft reflection-tuning (li et al., 2023e) if alpaca/wizardlm dataset chatgpt llama labeling sft selective reflection-tuning (li et al., 2024d) if alpaca/wizardlm dataset chatgpt llama labeling sft vicuna (chiang et al., 2023) if/md human conversation chatgpt + gpt4 llama labeling sft koala (geng et al., 2023) if/md human conversation chatgpt llama labeling sft baize (xu et al., 2023b) if/md quora + stack overflow chatgpt llama expansion + self-knowledge sft ultrachat (ding et al., 2023b) if/md wikidata + text material + c4 chatgpt llama curation sft orca (mukherjee et al., 2023) if/tp flan-v2 chatgpt + gpt4 llama labeling sft orca2 (mitra et al., 2023) if/tp flan-v2 + few-shot/math/synthetic gpt4 llama labeling sft selfee (ye et al., 2023) if/tp human conv, flan/code/math collection chatgpt llama labeling sft cot-distill (hsieh et al., 2023) if/tp e-snli + anli + cqa + svamp palm t5 labeling sft knowpat (zhang et al., 2023a) if/tp cpkg + qa data chatgpt + chatglm + vicuna-7b llama labeling sft debatune (li et al., 2024e) if/tp controversial topics chatgpt llama labeling sft phi-1 (gunasekar et al., 2023) if/code - gpt3.5 phi-1 curation sft phi-1.5 (li et al., 2023a) if/code 20k topics from web gpt3.5 phi-1 curation + labeling sft sail (luo et al., 2023c) if/rag alpaca data + web content gpt4 llama label sft kard (kang et al., 2023b) if/rag medqausmle chatgpt t5 + opt label sft + d&s self-rag (asai et al., 2023) if/rag open-instruct gpt4 llama labeling sft alignment openchat (wang et al., 2023c) if/preference human conversation chatgpt + gpt4 llama labeling sft + rl zephyr (tunstall et al., 2023) if/preference mixed datasets gpt4 mistral labeling + feedback sft + ro almost (kim et al., 2023a) if/preference human-written prompts llama llama expansion + labeling sft + rl rlcd (yang et al., 2024a) if/preference human-written prompts llama llama labeling sft + rl rlaif (lee et al., 2023a) if/preference human-written prompts palm 2 palm 2 labeling + feedback rl gpt3 reward (kwon et al., 2023) preference human-written prompts gpt3 gpt3 labeling rl ilf (scheurer et al., 2023) preference task-specific datasets gpt3 + feedme gpt3 labeling rl ultrafeedback (cui et al., 2023a) preference mixed datasets gpt4 llama labeling rl constitutional ai (bai et al., 2022a) preference/value human-written prompts self-defined student model self-defined model labeling + expansion + feedback sft + rl sandbox (liu et al., 2023b) value simulationtext-davinci-002/-003 + gpt4 + chatgptllama data curation sft + rl agent toolformer (schick et al., 2023) tool ccnet gpt-j gpt-j labeling sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft graph-toolformer (zhang, 2023) tool mixed graph dataset chatgpt gpt-j + llama labeling sft gorilla (patil et al., 2023) tool online api documentation gpt4 llama expansion sft gpt4tools (yang et al., 2023b) tool image content chatgpt llama curation + expansion sft toolalpaca (tang et al., 2023a) tool public-apis repository chatgpt llama curation sft toolllm (qin et al., 2023a) tool real-world apis chatgpt llama curation sft mllm-tool (wang et al., 2024) tool huggingface model cards gpt4 llama curation sft fireact (chen et al., 2023b) planning mixed qa dataset gpt4 llama labeling sft agenttuning (zeng et al., 2023a) planning 6 agent tasks gpt4 + chatgpt llama labeling + expansion sft lumos (yin et al., 2023a) planning mixed interactive tasks gpt4 llama labeling sft autoact (qiao et al., 2024) planning mixed qa tasks llama llama labeling sft nlp task specialization auggpt (dai et al., 2023a) nlu amazon/symptoms/pubmed20k dataset chatgpt bert label sft tdg (he et al., 2023b) nlu sst + qqp + mnli gpt3 bert expansion sft sungen (gao et al., 2023a) nlu text classification tasks gpt2 distilbert curation sft udg (wang et al., 2021a) nlu nlu tasks gpt3 bert expansion sft inheritsumm (xu et al., 2023c) nlg pile + arxiv + cnn/dm + wikihow gpt3.5 zcode++ label sft dimsum+ (jung et al., 2023) nlg none gpt2 + ctrl + biogpt t5 curation + self-knowledge sft genie (yehudai et al., 2024) nlg eli5 + asqa + nq + cnn/dm falcon + llama flan + llama label sft gkd (agarwal et al., 2024) nlg/nlu/if xsum+wmt14 en-de+gsm8k+flan2021 t5-xl t5 feature + feedback d&s + rl quill (srinivasan et al., 2022) ir ir datasets t5 4-layer transformer internal knowledge d&s rankvicuna (pradeep et al., 2023a) ir ir datasets chatgpt llama labeling sft rankzephyr (pradeep et al., 2023b) ir ir datasets chatgpt + gpt4 mistral labeling sft ndr (mysore et al., 2023) recommendation recommendation datasets gpt3 mpnet-110m labeling sft instrcutrec (zhang et al., 2023b) recommendation 39 instruction templates chatgpt flan-t5 expansion + self-knowledge sft once (liu et al., 2023c) recommendation recommendation dataset chatgpt llama labeling sft pandalm (wang et al., 2023b) evaluation alpaca data chatgpt llama labeling sft prometheus (kim et al., 2024) evaluation 50 seed rubrics gpt4 llama labeling sft instructscore (xu et al., 2023d) evaluation mixed dataset gpt4 llama labeling sft wizardmath (luo et al., 2023b) math gsm8k + math chatgpt llama expansion + feedback sft + rl mammoth (yue et al., 2023a) math/tp mixed math dataset gpt4 llama labeling sft mixed distill (chenglin et al., 2023) math/tp svamp + gsm8k + asdiv + strategyqa chatgpt llama labeling sft wizardcoder (luo et al., 2023a) code code alpaca data chatgpt starcoder expansion sft magicoder (wei et al., 2023) code existing source codes chatgpt llama curation sft wavecoder (yu et al., 2024) code existing source codes gpt4 llama curation sft code alpaca (chaudhary, 2023) code code instructions chatgpt llama expansion + self-knowledge sft code llama (rozi `ere et al., 2023) code human-written instructions llama llama expansion + self-knowledge sft code clean (jain et al., 2023) code code datasets chatgpt llama labeling sft multi-modality llava (liu et al., 2023e) vision-language coco gpt4 llama labeling sft svit (zhao et al., 2023b) vision-language visual genome + coco gpt4 llama labeling sft lvis-instruct4v (wang et al., 2023e) vision-language lvis gpt4v llama labeling sft llavar (zhang et al., 2023d) vision-language laion gpt4 llama labeling sft macaw-llm (lyu et al., 2023) multiple modalities image/video with caption chatgpt llama labeling sft mimic-it (li et al., 2023f) multiple modalities image/video dataset chatgpt llama labeling sft chatbridge (zhao et al., 2023d) multiple modalities task-specific/multimodal-chat data gpt4 + chatgpt llama labeling sft table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, table 3: a summary of skill distillation works. if: instruction following, md: multi-turn dialoue, tp: think pattern, rag: retrieval-augmented generation, nlu: natural language understanding, nlg: natural language generation, ir: information retrieval, sft: supervised fine-tuning, d&s: divergence and similarity, rl: reinforcement learning, ro: ranking optimization. finally, multi-modality encompasses the knowledge trans- fer from teacher llms to multi-modal models. table 3 summarizes the representative works, encompassing details such as the skills involved, seed knowledge, teacher llm, student model, knowledge elicitation method, and training objectives.4.1 context following this part concentrates on the distillation of context follow- ing skills from llms. this process involves transferring the ability of llms to handle a variety of complex contexts — such as few-shot demonstrations, intricate instructions, dia- logue history, and retrieval-augmented information — into smaller models. many research efforts in this domain aim to imbue smaller models with these sophisticated, context- 15 following capabilities. our discussion here will dissect this facet of skill distillation, categorizing it based on different types of context and elaborating on how each is distilled and incorporated into smaller, efficient models. 4.1.1 instruction following instruction-following capacity enables llms to understand and follow user-given instructions. this ability significantly enhances human-ai interaction, allowing for seamless un- derstanding and execution of tasks as directed by users. a primary method for acquiring this skill involves construct- ing instruction-like prompt-response pairs and employing supervised fine tuning (sft) for model training. data for this purpose can be manually curated by human experts or transformed from existing nlp tasks into instructional formats with templates, such as prefacing machine transla- tion data with ”translate this sentence to spanish:” . however, these approaches have limitations. manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input. llms like gpt-4 offer an efficient alternative for creating diverse and controlled sft data by their capabil- ities of in-context learning and instruction following. most relevant works use openai’s gpt series models to generate prompt-response data pairs and then train the student llms by supervised fine-tuning (wang et al., 2022a; taori et al., 2023; chiang et al., 2023; wu et al., 2023c; xu et al., 2023a; mukherjee et al., 2023; mitra et al., 2023; luo et al., 2023b; peng et al., 2023a). basic instructions. self-instruct (wang et al., 2022a) lever- ages the in-context learning capability of gpt-3 to expand a seed pool of 175 tasks to 52k task-agnostic instructions, ensuring a broad spectrum of general instructions. addi- tionally, a filtering and post-processing stage is introduced to eliminate redundant or similar instructions. notably, through training with this enriched dataset, gpt-3 acquires the ability to follow instructions, enabling it to perform comparably to instructgpt in zero-shot instruction tasks and when provided with expert-written instructions for novel tasks. based on the self-instruct method, taori et al. (2023) train an alpaca model using the llama 7b model on 52k instruction-following demonstrations, generated in a similar style as self-instruct but utilizing the more robust text-davinci-003 model. to enhance the diversity of instruc- tional data, wu et al. (2023c) introduce a technique known astopic-guided instruction generation . this method involves gathering 3.5k common topics from wikipedia to serve as guidance during the generation process. complex instructions. some works promote students to solve more complex instructions (xu et al., 2023a; luo et al., 2023b,a; guo et al., 2023c). according to xu et al. (2023a), in- struction datasets derived from human-written seeds often exhibit low to moderate complexity. to enhance the com- plex instruction-following capabilities of smaller models, wizardlm (xu et al., 2023a) introduces evol-instruct . this method gradually transforms instructions into more com- plex forms through a multi-step evolution process, focusing on both increasing difficulty levels and expanding the di- versity of topics. they conducted four rounds of evolution using the openai chatgpt api, resulting in a dataset of250k complex instructions. subsequently, they trained the llama 7b model, referred to as wizardlm, on this dataset. in the high-difficulty section of test instructions, wizardlm even outperformed chatgpt, achieving a win rate 7.9% higher than chatgpt. zhao et al. (2023e) further conduct preliminary studies revealing the effectiveness of increasing instruction complexity. instruction fusion (guo et al., 2023c) further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to further uses teacher llms to increase the complexity by fusing two distinct evolved instructions. furthermore, this concept of “evolving” instructions has been extended to distill specific skills such as coding (luo et al., 2023a) and mathematics (luo et al., 2023b). human instructions. in contrast to works that rely on gener- ating instructions from chatgpt, which may lack diversity and have gaps with real human instructions, vicuna (chiang et al., 2023) and koala (geng et al., 2023) showcase impres- sive performance by using human conversations and natu- ral instructions from community-contributed conversations. these conversations, found in platforms like sharegpt, pro- vide a forum for users to share their interactions with chat- gpt. it’s important to note, however, that models trained on such natural conversations might mimic the style but may not fully capture the reasoning process of the original teacher (gudibande et al., 2023; mukherjee et al., 2023). system instructions. to encourage student models to learn the reasoning process, orca and orca 2 (mukherjee et al., 2023; mitra et al., 2023) enhance the prompt, response data pairs by introducing a system message (e.g., ”explain like i’m five, think step-by-step”) to encourage student mod- els to grasp the reasoning process. this system message prompts gpt-4 to provide explanation traces that eluci- date the teacher’s reasoning process. orca 2 (mitra et al., 2023) further trains the student model to identify the most effective solution strategy for each task, guided by orca’s performance. this approach significantly improves the abil- ity of smaller models to follow instructions that involve reasoning. high-quality instructions. as demonstrated in zhou et al. (2023a) and (li et al., 2024f), the data quality is crucial for instruction following training. ultrachat (ding et al., 2023b) distills large-scale data with high-quality and di- verse instructions from teacher llms by various meta- information. the ultrallama model, fine-tuned on this data, consistently surpasses other open-source models. the phi series models (gunasekar et al., 2023; li et al., 2023a; mar, 2023) prioritize data quality and employ synthetic methods to generate data of “textbook quality” to enhance the learning experience for smaller models. notably, phi exhibits the ability to follow instructions effectively even without specific instruction fine-tuning. what’s particularly remarkable is that phi-2, with just 2.7 billion parameters, outperforms mistral and llama-2 models with 7b and 13b parameters across various benchmark evaluations. improved instructions. another line of work focuses on improving the quality of existing instruction data, including both the improvement of instruction and corresponding response. selfee (ye et al., 2023) utilizes the chatgpt to iter- atively improve the quality of responses. expertllama (xu et al., 2023f) improves the quality of responses by augment- 16 ing vanilla instructions with specialized expert identity descriptions. reflection-tuning (li et al., 2023e) improves both the instruction and response sequentially by reflecting on specific criteria. deita (liu et al., 2023h) proposes to enhance and score instructions in three directions includ- ing complexity, quality, and diversity to get high-quality distillation data. muffin (lou et al., 2023) proposes to scale the instruction according to the input by diversifying these tasks with various input facets. selective reflection- tuning (li et al., 2024d) first involves the student model in the data improvement pipeline with a novel student- selection module, in which the student model is able to decide the data learn from. in summary, distilling instruction data from teachers presents a promising avenue for training cheap and re- producible instruction-following language models. cur- rent small models have made strides in enhancing var- ious aspects of instruction-following ability, like diver- sity, complexity and explanation. however, student mod- els trained on instruction data expanded by chatgpt of- ten mimic chatgpt’s style without replicating its factual accuracy (gudibande et al., 2023). achieving a more ca- pable instruction-following capability requires a stronger teacher llm (gudibande et al., 2023) and access to di- verse, high-quality instruction data, such as the one used in orca (mukherjee et al., 2023; mitra et al., 2023), which incorporates extensive task instructions from the flan 2022 collection (longpre et al., 2023). 4.1.2 multi-turn dialogue while instruction following focuses on single-instance com- mand execution, multi-turn dialogue extends this to com- prehend and maintain context through ongoing interactions. this skill is vital for models to engage meaningfully in human-like conversations and respond coherently over suc- cessive dialogue turns. some works have been dedicated to train to small chat models by distilling multi-turn knowl- edge from teacher llms (chiang et al., 2023; xu et al., 2023b; ding et al., 2023b; li et al., 2023b; wang et al., 2023c; tunstall et al., 2023). sharegpt serves as a platform for users to share their conversations with chatgpt, offering a vast repository of multi-turn conversations readily available. some small chat models are trained using this data to acquire the capability for engaging in multi-turn dialogues (chiang et al., 2023; ye et al., 2023; wang et al., 2023c). for example, vicuna (chiang et al., 2023) is a chat model exclusively trained on sharegpt data. despite its sole training source being sharegpt, vi- cuna achieves a high mt-bench (zheng et al., 2023a) score assigned by gpt-43. in the study conducted by wang et al. (2023c), gpt-3.5 and gpt-4 are employed to generate mixed responses using sharegpt data. they assign higher rewards to responses generated by gpt-4, aiming to incentivize student models to produce high-quality responses. addi- tionally, ye et al. (2023) enhance the quality of multi-turn data from sharegpt by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. 3. mt-bench: a multi-turn question set, where the generations of models are evaluated by llm, like gpt-4.to enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversa- tional datasets through self-chat and using them to train smaller models (xu et al., 2023b; ding et al., 2023b; tunstall et al., 2023). for instance, xu et al. (2023b) initiate their work by using questions sourced from quora and stack overflow as seeds, resulting in the collection of 111.5k dialogues through self-chat. subsequently, they employ parameter- efficient tuning to train a chat model named baize. ding et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions et al. (2023b) first construct a significantly larger dataset called ultrachat, comprising 1.5 million high-quality multi- turn dialogues. they achieve this by distilling instructions and dialogues from chatgpt. notably, ultrachat encom- passes a wide range of topics and instructions. building upon the ultrachat dataset, they fine-tune a llama model, resulting in the creation of a powerful chat model known as ultrallama. ultrallama consistently outperforms other open-source chat models, including vicuna and baize. fur- thermore, ultrachat is employed in conjunction with an ai preference-aligned chat model named zephyr (tunstall et al., 2023). zephyr enhances intent alignment through the application of distilled direct preference optimization (ddpo). 4.1.3 rag capbility llms are known to lack the ability to utilize up-to-date knowledge, and often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge. retrieval-augmented generation (rag) is a promising technique to decrease this issue. handling the augmented context of retrieved information is also a non- trivial skill of llms. several approaches to distill rag capabilities have been proposed (kang et al., 2023a; luo et al., 2023c; asai et al., 2023). sail (luo et al., 2023c) starts by retrieving search results for each training case using search apis, creating search- augmented instructions that include both the instruction and grounding information. to encourage the language model to prioritize informative retrieval results, they input each retrieved passage along with the ground truth response into the entailment model to label each retrieval result for relevance. subsequently, the search-augmented instructions and relevance labels are fed into teacher llms (like gpt- 4) for generating responses. following fine-tuning on this training set, the student model becomes proficient at de- noising search results and generating accurate responses. kard (kang et al., 2023b) distills rationales rfrom the teacher llm in response to questions x. these rationales are then utilized to train two models: a student lm and a reranker. for training the student lm, the rationales serve as a means to retrieve relevant knowledge d, and the student lm is subsequently fine-tuned using the rationales along- side questions and knowledge. however, during inference, only questions are available. to address this, the reranker is trained to mimic how the retriever scores passages with the rationale by minimizing the kl divergence between retriever (d|r)andreranker (d|x). however, the integra- tion of a fixed number of passages in language models, without considering their necessity or relevance, can reduce versatility and lead to the generation of unhelpful responses. to equip student lms with adaptive rag capabilities, self- 17 rag (asai et al., 2023) distills this adaptive ability from teacher llms into a small critic model. this critic model determines whether retrieval is necessary and evaluates the quality of the retrieved results by generating ‘reflection to- kens.’ for instance, self-rag initiates the retrieval operation when generating the reflection token retrieve . to distill this critic data, gpt-4 is prompted to assess the need for retrieval using few-shot demonstrations i, the task input x, and output yto predict a reflection token ras follows: p(r|i, x, y ). 4.2 alignment 4.2.1 thinking pattern most existing methods mainly focus on directly aligning the direct responses of the student models to the responses of teacher models (taori et al., 2023). though effective, these models might suffer the problems that they tend to learn to imitate the response style of the teacher models, but not the reasoning process (mukherjee et al., 2023). thus in order to better distill from the teacher models, methods are proposed that not only imitate the pure responses but some novel thinking patterns (ye et al., 2023; mukherjee et al., 2023; mitra et al., 2023; wang et al., 2023d; cheng et al., 2023; zhang et al., 2023a). motivated by the effectiveness of llms in generat- ing their own feedback without relying on external mod- els (schick et al., 2022; madaan et al., 2023; saunders et al., 2022), selfee (ye et al., 2023) proposes to train a model that has been fine-tuned to continuously revise its own answer until it provides a high-quality response in a single inference. during training, it utilizes both the final response and feedback chain as the fitting target. this pat- tern, response with the revision process, shows a promising performance gain. following selfee, reflection-tuning (li et al., 2023e, 2024d) also utilizes the reflection process as the learning pattern. noticing the lack of reasoning imitation of the previous methods, orca (mukherjee et al., 2023) first proposes explanation tuning, which aims to learn the reasoning steps, including explanation traces, step-by-step thought processes, and other complex instructions, from the teacher model, rather than just the vanilla styles. extensive experiments verify the effectiveness of distilling with this thinking pattern. the following orca2 (mitra et al., 2023) further presents to equip the student models with the ability to utilize different solution strategies for different tasks, mo- tivated by the capability discrepancies between the smaller and larger models. by employing this training pattern, the student models are able to gain a better reasoning ability. be- sides learning with the corresponding revision or reflection process, another thinking pattern that recently appeared is generating both responses and preferences. zhang et al. (2023a) propose to learn both the knowledge and corre- sponding preference for domain-specific qa with llms. recently, debatune (li et al., 2024e) proposes to improve the controllability of llms in generating statements on controversial topics. by engaging two agents in a structured multi-round debate on controversial topics, salient and in- depth statements can be obtained and further distilled into the student models.4.2.2 preference the previously mentioned methods primarily focus on the basic capability of student models to produce outcomes that are strictly accurate but may not align with human preferences, reaching alignment at this level enables these models to aid in various tasks without meeting higher-level demands. early methods mainly utilize human feedback for the alignment of human preferences (ziegler et al., 2019; stiennon et al., 2020; wu et al., 2021; ouyang et al., 2022; bai et al., 2022b; k ¨opf et al., 2023; yuan et al., 2023b). however, obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon obtaining human feedback is costly and labor-intensive, thus methods that learn from ai feedback are also proposed to align with human preferences (bai et al., 2022a; kwon et al., 2023; scheurer et al., 2023; kim et al., 2023a; roit et al., 2023; yang et al., 2024a; lee et al., 2023a; tunstall et al., 2023; cui et al., 2023a; wang et al., 2023f). the concept of rlaif, introduced by bai et al. (2022a), involves the integration of preferences labeled by llms with those labeled by humans. this approach is designed to simultaneously optimize two key objectives: ensuring the helpfulness of the output and minimizing any potential harm, making the responses of llms more aligned with human preferences. kwon et al. (2023) develop a proxy reward function using llms like gpt-3, which is created by first providing the llm with a description of the behaviors desired by the user, along with a small number of examples. the llm then produces rewards by evaluating how closely the outputs of a model align with the provided descrip- tions, essentially measuring their relevance to the estab- lished ground truth. scheurer et al. (2023) propose imitation learning from language feedback, in which a language model is utilized to improve various outputs generated by a model. this refinement is based on a reference provided by a human. following this process, the most effectively refined output is chosen to be used in further supervised fine-tuning. as outlined by kim et al. (2023a), almost in- volves condensing human preferences into a set of heuristic guidelines. an example of such a rule is the idea that larger llms that utilize more comprehensive and higher-quality prompts are likely to yield superior responses. based on these established guidelines, comparison data is generated using responses from llms of different sizes and with varying prompts. this data is then used to train a reward model. yang et al. (2024a) propose reinforcement learning from contrast distillation, which aims to align language models without relying on human feedback. this approach involves training a preference model using simulated pairs of preferences, including both high-quality and low-quality examples which are generated through contrasting prompts, positive and negative. lee et al. (2023a) further highlight the effectiveness of rlaif. this work proposes that rlaif not only matches but in some cases surpasses rlhf, and interestingly, rlaif can also enhance the performance of supervised fine-tuning. another notable discovery is that directly prompting the llm for reward scores during reinforcement learning can be more effective than the conventional approach of training a reward model based on llm preferences. wang et al. (2023f) propose conditioned-rlft, which treats different data sources as coarse-grained reward labels and develops 18 a class-conditioned policy to effectively utilize the varying qualities of data, which is a reinforcement learning-free supervised learning approach. cui et al. (2023a) propose a large-scale, high-quality, and diversified preference dataset labeled by gpt4 for comprehensive feedback. tunstall et al. (2023), by proposing distilled direct preference optimiza- tion (rafailov et al., 2023) on ultrafeedback, obtaining a small by powerful llm. 4.2.3 value attaining alignment with human preferences allows large models to optimize human satisfaction by operating in a manner that aligns with human preferences. however, to establish trustworthy llms, the notion of ’aligning llms with human values’ is proposed and the key principles of alignment are often summarized as the “hhh” criteria: helpful, harmless, honest (weidinger et al., 2021; askell et al., 2021). numerous methods have been undertaken for building trustworthy llms. however, due to the intrinsic difficulty of this aim, which is still an unsolved problem for proprietary models (sun et al., 2024a), most existing methods rely on constructing high-quality human prefer- ence datasets (ji et al., 2023b; solaiman and dennison, 2021; bai et al., 2022b; qiu et al., 2022; kiesel et al., 2022; liu et al., 2022a), utilizing human-written rules as constrains (glaese et al., 2022; sun et al., 2023b, 2024b), etc. for detailed progress on trustworthy llms, please further refer to yao et al. (2023a); liu et al. (2023i); sun et al. (2024a). though slightly under-explored, aligning llms with human values by distilling is still possible (bai et al., 2022a; cui et al., 2023a; yang et al., 2024a; sun et al., 2024b). for instance, bai et al. (2022a) propose rlaif, utilizing ai- generated labels to interactively improve both helpfulness and harmlessness. sun et al. (2024b) prompt the student model with 16 principles as guidelines for generating help- ful, ethical, and reliable responses. similarly, both harmless and harmful generations could be elicited by modifying the prompts, and then are used to train the preference model (yang et al., 2024a). cui et al. (2023a) utilize gpt- 4 to rank generations regarding helpfulness, truthfulness, and honesty. liu et al. (2023b) advance the alignment of llms with societal values by incorporating simulated social interactions into the training process. this approach encom- passes a range of elements, including demonstrations that are both in alignment and in conflict with social norms, as well as collective ratings, in-depth feedback, and responses that are revised iteratively. 4.3 agent 4.3.1 tool using while recent llms have shown proficiency in solving var- ious tasks, they still tend to make mistakes when handling large numerical values or executing intricate mathematical calculations (qian et al., 2022; she et al., 2023; manikandan et al., 2023; liang et al., 2023b; mialon et al., 2023). thus equipping llm agents with the capability to utilize tools has been increasingly focused on. commonly used methods mainly relied on human-curated data for training (parisi et al., 2022; nakano et al., 2022; qin et al., 2023c; song et al., 2023b) or prompt designing(cai et al., 2023; shenet al., 2023a; hao et al., 2024). recently, distillation-based methods are also proposed (schick et al., 2023; zhang, 2023; patil et al., 2023; tang et al., 2023a; qin et al., 2023a; yuan et al., 2023a; gao et al., 2023b; wang et al., 2024; shen et al., 2024; yuan et al., 2024b). toolformer (schick et al., 2023) utilizes a self-supervised manner, avoiding large human annotations, to obtain the most required apis to use and further distill this capability to the model itself. the performance of the gpt-j-based toolformer surpasses opt (66b) (zhang et al., 2022) and gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed gpt3 (175b) (brown et al., 2020) greatly. graph-toolformer (zhang, 2023) aims to equip llms with the ability to process and reason over complex graph data, which is designed to enhance llms with graph reasoning skills using exter- nal graph reasoning api tools by adopting chatgpt to annotate and augment a larger graph reasoning statement dataset for training. gorilla (patil et al., 2023) addresses the limitations of current llms in generating accurate input arguments and reduces the problem of ”hallucination” or generating incorrect api usage and it collects thousands of models from platforms like huggingface and torch hub as the api calls and utilizes gpt4 to generate synthetic instruction data for training. gpt4tools (yang et al., 2023b) introduces to enable open-source llms like llama and opt to use multimodal tools, a capability previously limited to advanced proprietary models like chatgpt and gpt-4. the approach involves generating an instruction-following dataset by prompting an advanced teacher model with mul- timodal contexts, using the low-rank adaptation optimiza- tion. toolalpaca (tang et al., 2023a) proposes a framework aimed at enhancing the tool-use capabilities of compact language models for embodied intelligence. it creates a dataset with 3938 instances from over 400 real-world tool apis across 50 categories and utilizes chatgpt to generate documentation for each prompt for later training. toolllm (qin et al., 2023a) proposes a comprehensive framework for enhancing llms with tool-use proficiency, focusing on data creation, model training, and evaluation by distilling from chatgpt. their toolllama shows impressive performance in executing complex instructions and handling new apis, rivaling chatgpt. craft (yuan et al., 2023a) builds a general tool creation and retrieval framework, which uti- lizes gpt4 to generate code snippets as the created tools. during the inference, other small llms could select and retrieve from the generated code snippets to execute or generate other methods conditioned on the given snippets. confucius (gao et al., 2023b) introduces a tiered training strategy for llms to master tool usage through a graduated curriculum and an innovative method called iterative self- instruction from introspective feedback (isif) for dynamic dataset enhancement to handle complex tools. mllm-tool (wang et al., 2024) is a multi-modal tool agent capable of interpreting instructions embedded in visual or audio content through the integration of multi-modal encoders with open-source large language models. as a trainable method, the initial instruction-answer pairs are generated by utilizing gpt4. shen et al. (2024) demonstrate that small llms are weak tool learners and proposes a multi-llm framework that decomposes the tool-use ability of a single model into a planner, caller, and summarizer for the tool using, leading to a supreme performance. the two-stage 19 training strategy introduced by this work is powered by chatgpt and gpt4 for collecting execution trajectories for the training set. yuan et al. (2024b) notice the potential issue of the current lengthy tool documentation, which hinders llms from understanding how to utilize a tool, thus proposing easytool to purify the important infor- mation from extensive documentation. the ground truth summarization of the training documents is obtained by using chatgpt. 4.3.2 planning another important aspect for llm agents is the ability to decompose high-level tasks to a chosen set of actionable steps (huang et al., 2022b), which is especially useful when acting in interactive environments. huang et al. (2022b) first demonstrate that llms can generate plausible goal-driven action plans without training, introduces non-invasive tools to enhance model executability, and assesses these methods through human evaluation to balance executability and semantic accuracy. most existing methods utilize prompting strategies for task planning (singh et al., 2022; zhou et al., 2023b; song et al., 2023c; wang et al., 2023g; yao et al., 2023b; liu et al., 2023j; hao et al., 2023; hu et al., 2023a), or building human-curated data for training (lin et al., 2023a; valmeekam et al., 2023). recently, there have also been some distilling methods emerging (chen et al., 2023b; zeng et al., 2023a; yin et al., 2023a; qiao et al., 2024; kong et al., 2023). fireact (chen et al., 2023b) introduces an innovative ap- proach for refining llms. this method involves fine-tuning smaller-scale llms using agent trajectories that are derived from a variety of tasks and prompting techniques. applying this method with trajectories generated by gpt4 has been shown to consistently enhance performance. agenttuning (zeng et al., 2023a) aims to enhance the performance of llms in executing agent tasks without sacrificing their wide-ranging capabilities. by utilizing a new dataset called agentinstruct, which includes high-quality interaction tra- jectories, it applies a hybrid instruction-tuning approach that merges these trajectories with general domain instruc- tions. lumos (yin et al., 2023a) pertains to a novel frame- work designed to train agents using a unified data format and modular architecture based on open-source llms. this system comprises three key modules: planning, grounding, and execution, enabling the decomposition of tasks into subgoals and actionable steps. tptu-v2 (kong et al., 2023) focuses on improving the task planning and tool usage abili- ties of llms in real-world scenarios, by utilizing data gener- ated by human experts or llms. it introduces a framework comprising three components: an api retriever, an llm finetuner, and a demo selector. autoact (qiao et al., 2024) proposes an agent learning framework that does not require large-scale annotated data or synthetic trajectories from high-resource models like gpt-4. instead, it uses a self- instruct method to generate its own planning trajectories with limited initial data. it then applies a division-of-labor strategy, creating sub-agents specialized in different aspects of the task completion process. distillation also works out for the training of embodied multi-modal agents (sumers et al., 2023; yang et al., 2023c; ma et al., 2023a; du et al., 2023a; sumers et al., 2023). for instance, sumers et al. (2023) aim to enhance the ability ofai agents to follow instructions by using pretrained vision- language models to provide supervision for understanding and acting upon language within their operational environ- ment, leveraging model distillation and hindsight experi- ence replay to teach them contextually relevant interactions in a simulated 3d setting. emma (yang et al., 2023c) evalu- ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using ates the challenges and inefficiency of training an embodied agent in a noisy visual world without expert guidance, and proposes to train them in a simulated environment using imitation learning, guided by an expert language model (like chatgpt), which operates in a corresponding text- based simulation, focusing on the same tasks. 4.4 nlp task specialization nlp tasks often grapple with challenges like data scarcity, interpretability issues, privacy concerns, and noisy data. the “knowledge” section of our survey illustrates various methods for distilling knowledge from llms, effectively setting the stage for student models to adapt to a range of nlp tasks. this knowledge provides supervision for the training of student models through information aug- mentation (e.g., cot and explanation), data augmentation, and semantic representation. by transferring the distilled knowledge from llms, student models can better handle diverse nlp challenges, improving task performance and addressing data limitations more robustly. 4.4.1 natural language understanding natural language understanding (nlu) is a fundamen- tal nlp task that involves comprehending and interpret- ing human language. the knowledge distilled from llms, such as through data labeling or augmentation, is typi- cally transferred into encoder-based language models like bert (vaswani et al., 2017) and roberta (liu et al., 2019). regarding the task of classification, certain studies have been noteworthy (dai et al., 2023a; gilardi et al., 2023; he et al., 2023b; gao et al., 2023a; chenglin et al., 2023; li et al., 2023g). auggpt (dai et al., 2023a) focuses on both general and clinical domain text classification. to address the limitations of small-scale clinical datasets, which often lack expert annotation and are subject to stringent privacy regulations, auggpt utilizes knowledge from teacher llms to rephrase each sentence in the training samples. this process creates multiple conceptually similar but seman- tically distinct samples, enhancing the dataset’s richness and diversity. another approach is demonstrated by gilardi et al. (2023), who employ chatgpt as an annotator to cate- gorize inputs. this method has been shown to outperform crowd-workers in several tasks, including relevance, stance, topics, and frame detection. furthermore, he et al. (2023b) propose targeted data generation (tdg), a novel approach for identifying challenging subgroups within a dataset. tdg leverages llms, along with human-in-the-loop, to generate new data specifically tailored for these subgroups, thereby enriching the dataset and improving model performance in sentiment analysis and natural language inference tasks. to facilitate the clinical information extraction task, tang et al. (2023b) elicit diverse samples from llms by providing examples and different seeds of clinical entities, i.e. the curation manner. 20 several studies have also focused on multiple nlu tasks (ding et al., 2023a; he et al., 2023a; wang et al., 2021a; he et al., 2022; ye et al., 2022; meng et al., 2022). for example, he et al. (2023a) utilize the knowledge in gpt-3.5 to annotate inputs with labels and explanations for various nlu tasks, including user input and keyword relevance assessment, boolq, and wic. wang et al. (2021a) employ few-shot prompts to expand high-quality training data using gpt-3, i.e. the expansion manner. beyond merely employing a single approach to elicit nlp task knowledge, ding et al. (2023a) explore a combination of labeling ,ex- pansion , and curation methods to extract knowledge from gpt-3 for distilling data for both sequence- and token-level nlp tasks. 4.4.2 natural language generation natural language generation (nlg) is a key aspect of eval- uating the capabilities of llms, encompassing tasks such as summarization, machine translation, and other open-ended text generation tasks. known for their potent generative abilities and creativity, llms excel in these areas, making them prime sources for distilling knowledge into student models tailored for nlg tasks (xu et al., 2023c, 2024b; ramnath et al., 2023; agarwal et al., 2024). additionally, the knowledge distilled from llms can be effectively used for nlg task-specific data augmentation (jung et al., 2023; wang et al., 2021b; guo et al., 2023a; yang and nicolai, 2023; wang et al., 2023h; yang et al., 2023d). while the previous sections have focused on the works about open- ended generation and multi-turn dialogue, this part will specifically highlight the distillation techniques relevant to other nlg tasks. although automatic metrics often favor smaller, fine- tuned models in summarization tasks, human evaluators tend to prefer the summaries generated by llms. address- ing this discrepancy, xu et al. (2023c) develop a student sum- marization model by distilling a gptsumm dataset, which comprises over 4 million paragraph-summary pairs gener- ated by querying gpt-3.5. in a different approach, jung et al. (2023) introduce ‘impossible distillation,’ a method that creates high-quality summarization-specific dataset from weak teacher llms. this method involves training a stu- dent model on the generated dataset and enhancing its capabilities through self-knowledge. turning to the task of machine translation, where creating parallel corpora is tra- ditionally expensive and time-consuming, yang and nicolai (2023) propose a three-step distillation process. this process involves generating seeds of verbs and nouns, forming sen- tences, and then translating these sentences. their findings suggest that while the distilled dataset may lack diversity, it effectively improves the translation signal for training student translation models. to distill high-quality content- grounded data automatically, genie (yehudai et al., 2024) proposes a general methodology containing three key steps: (a) preparation of the content, (b) distillation of responses from a teacher llm corresponding to the content, and (c) filtering mechanism to ensure the quality and faithfulness of the generated data. genie demonstrates that student models trained through this distilled data can match or even surpass models trained on human-generated data.4.4.3 information retrieval information retrieval (ir) represents a crucial branch of computer science, focused on efficiently retrieving infor- mation relevant to user queries from extensive reposito- ries (cai et al., 2022; liu et al., 2022b; feng et al., 2023; shen et al., 2023b). a typical ir system encompasses three main components: the query rewriter, the retriever, and the reranker. recent studies have highlighted the effective- ness of employing llms in ir systems, e.g. in enhancing the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in the reranking stage through both point-wise and list-wise ranking methods (ma et al., 2023b; sun et al., 2023a; qin et al., 2023d). however, the practical application of llms in ir systems faces challenges, primarily due to their slower generation speed, which conflicts with the low-latency re- quirements of ir tasks (sun et al., 2023a). as a result, the kd of llms emerges as a more promising approach for ir, offering a way to infuse the distilled knowledge from llms into various stages of the ir pipeline without compromising on speed. there has been a significant body of work demonstrating how knowledge distilled from llms can benefit each component of the ir system, including the query rewriter (srinivasan et al., 2022; ma et al., 2023c), the retriever (dai et al., 2023b; sachan et al., 2022, 2023; schick and sch ¨utze, 2021; meng et al., 2023; peng et al., 2023b), and thereranker (bonifacio et al., 2022; sun et al., 2023a; pradeep et al., 2023a,b; saad-falcon et al., 2023; ferraretto et al., 2023; jeronymo et al., 2023; sun et al., 2023c). query rewriter. the query rewriter (qr) is a pivotal com- ponent in ir systems, tasked with enhancing the precision and expressiveness of user queries by refining or modifying the initial query to more accurately align with the user’s information needs. one notable approach is quill (srini- vasan et al., 2022), which introduces a two-stage distillation method for query intent understanding. initially, a retrieval- augmented llm, serving as the ‘professor,’ is distilled into a non-retrieval augmented teacher llm, aiming to bolster its understanding capabilities. subsequently, this enhanced teacher llm is distilled into a final student model using a large dataset, further refining the process. incorporating the qr into ir systems, ma et al. (2023c) develop a ’rewrite- retrieve-read’ framework. this process begins with an llm rewriting the queries via prompting, followed by a retrieval-augmented reading stage. to integrate the rewrit- ten queries effectively into the ir system, the knowledge gleaned from the llm is distilled into a compact student rewriter. this rewriter is then fine-tuned using feedback from the llm reader through reinforcement learning. retriever and reranker. in ir systems, the retriever is designed to efficiently locate the top-k relevant texts from a large corpus. it encodes both queries and documents into vector representations and performs retrieval by computing the dot product between these vectors. the reranker further refines the order of the retrieved documents to improve the overall quality of the output. this is achieved in two primary ways, including pointwise reranker and listwise reranker . pointwise reranker takes both the query and a single candidate document as input to directly generate a relevance score. listwise reranker directly reorders a list of input documents in terms of their relevance. 21 retriever and pointwise reranker. for the retriever and pointwise reranker, a common application of kd from llms is the generation of pseudo-queries for given documents. this approach aims to expand the pairwise data, enhancing the training of dense retrievers or rerankers. for example, inpars (bonifacio et al., 2022) utilizes gpt-3 to generate multiple pseudo-queries for an unlabeled document. to ensure the relevance of these queries, the system filters them based on the highest log probabilities of generating a query conditioned on the documents. subsequently, inpars fine-tunes a reranker based on monot5 (raffel et al., 2020). another similar approach, promptagator (dai et al., 2023b), introduces a few-shot dense retrieval method that leverages a small number of demonstrations from the target domain for pseudo-query generation. diverging from the reliance on unlabeled documents, sachan et al. (2022) distill knowl- edge from gpt-4 to curate diverse synthetic data for text embedding tasks across nearly 100 languages. they fine- tune powerful decoder-only llms, such as mistral-7b (jiang et al., 2023a), on this synthetic data using standard con- trastive loss. remarkably, this method demonstrates strong performance on text embedding and multilingual retrieval benchmarks without any labeled data. beyond generating pseudo-queries, teacher llms can also be employed to gen- erate relevance scores as soft labels. these scores are used to train the retriever by minimizing the kl-divergence loss between the teacher and student distributions, as explored by sachan et al. (2023). listwise reranker. a distinct set of studies focuses on listwise reranking, where its advantage lies in compar- ing multiple documents simultaneously to determine the optimal reorder. rankgpt (sun et al., 2023a) leverages gpt-4 to generate permutations for a group of candidate passages. to distill this listwise ranking knowledge into a pointwise student reranker, various training loss functions are employed, such as listwise cross-entropy (bruch et al., 2019), ranknet (burges et al., 2005), and lambdaloss (wang et al., 2018). building upon rankgpt’s framework, rankvi- cuna (pradeep et al., 2023a) and rankzephyr (pradeep et al., 2023b) further refine this approach by directly fine- tuning a listwise reranker using teacher-generated textual permutations. this enables the student reranker to produce sequences of ranked results directly, bypassing the interme- diate step of calculating individual relevance scores. 4.4.4 recommendation recommender systems are integral to enhancing user ex- perience in various online services, providing personalized content based on user preferences and behaviors. many works have demonstrated that llms could be directly used as recommenders without fine-tuning (wang et al., 2023i; dai et al., 2023c) or generate auxiliary textual features to benefit recommender systems (xi et al., 2023; ren et al., 2023; wei et al., 2024). (wang et al., 2023j; ren et al., 2023; wei et al., 2024). however, the real-time nature of online rec- ommender systems demands rapid response times, posing a challenge with the inherent inference latency associated with llms. to address this, several studies have explored ways to distill and integrate the knowledge from llms into recommender systems, thereby leveraging their advanced capabilities while mitigating latency issues for efficient real-time recommendations (mysore et al., 2023; zhang et al., 2023b; liu et al., 2023c). mysore et al. (2023) tackle data scarcity in narrative- driven recommendation (ndr), where users provide de- tailed descriptions of their preferences. they utilize gpt-3 to create synthetic narrative queries from user-item interac- tions via few-shot prompting, then distill this data into re- trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized trieval models for ndr. similarly, genre (liu et al., 2023c) employs gpt-3.5 to augment datasets with new knowledge about news summarization, user profiles, and personalized content, aiding the training of content-based recommenda- tion models. to bridge the gap between language models and recommender systems, some research views behavior modeling as an extension of language modeling (cui et al., 2022; liu et al., 2023k). instructrec (zhang et al., 2023b), for instance, interprets recommendation as instruction fol- lowing. they use chatgpt to distill a wealth of user- personalized instruction data reflecting diverse preferences and intentions based on real historical interactions. this data is then used to fine-tune a 3b student language model specifically for recommendation purposes. 4.4.5 text generation evaluation text generation evaluation, i.e. nlg evaluation, focuses on assessing the quality of generated content. unlike tradi- tional nlg evaluation metrics like bleu (papineni et al., 2002) or rouge (lin, 2004), which primarily rely on surface-level text comparisons, llms, trained on extensive corpora and refined through techniques like rlhf, offer a more nuanced and human-aligned assessment. this so- phistication has led to the increasing use of llms in nlg evaluation (detailed further in (li et al., 2024b)). through kd of llms, student evaluators could enhance inference efficiency and achieve more flexible and highly customized evaluation (wang et al., 2023b; kim et al., 2024; xu et al., 2023d; jiang et al., 2023c; li et al., 2024a). pandalm (wang et al., 2023b) concentrates on a pairwise evaluator designed to compare two pieces of generated content. it utilizes a teacher llm (gpt-3.5) to judge which response is better for a given instruction and input, provid- ing reasons for its decision. addressing the need for cus- tomized and flexible criteria to meet realistic user demands, prometheus (kim et al., 2024) distills gpt-4 to construct a training dataset that includes reference answers and a vari- ety of customized scoring rubrics. this dataset is then used to tune llama for evaluating model-generated responses. instructscore (xu et al., 2023d) takes a more fine-grained ap- proach by using gpt-4 to create detailed analysis data. this data is employed to tune llama, enabling it to perform error analysis on generated texts compared to reference texts. the system further refines its evaluation capabilities through self-training with real model-generated response- reference pairs. for reference-free evaluation across diverse domains, tigerscore (jiang et al., 2023c) samples data from a variety of text generation datasets, such as summariza- tion, translation, and data-to-text. it distills error analysis knowledge from gpt-4 and uses this to fine-tune llama for more nuanced evaluation. lastly, to adapt evaluation to real-world scenarios beyond conventional nlp tasks, auto-j (li et al., 2024a) collects real-world user queries and their evaluations from a teacher llm. this massive dataset 22 of real-world scenarios is then used to distill evaluation knowledge into llama through fine-tuning, enhancing its practical applicability. 4.4.6 code llms, trained on extensive corpora containing code, are highlighted for their proficiency in code-related tasks. their capabilities extend beyond direct code generation to include the provision of external knowledge and data, which is crucial in distilling their expertise into smaller, more effi- cient models. several works have successfully distilled code knowledge from llms into those compact and specialized code models (chaudhary, 2023; rozi `ere et al., 2023; gu- nasekar et al., 2023; wei et al., 2023; chen et al., 2023a; liu et al., 2023d; yu et al., 2024; jain et al., 2023; su and mcmillan, 2023; guo et al., 2023d). a primary focus in these student code models is on code generation, a task of both common utility and practical significance. for instance, code alpaca (chaudhary, 2023) fine-tunes llama using self-instruct with chatgpt-distilled instructions specifically for code generation tasks. similarly, code llama-instruct (rozi `ere et al., 2023) is fine-tuned via self-instruct, prompting llama-2 (touvron et al., 2023) with coding problems, and further refined with unit tests. phi- 1 (gunasekar et al., 2023) aims to enhance the quality of dis- tilled code data by extracting “textbook quality” data from a teacher llm, incorporating python textbook and exercise data. magicoder (wei et al., 2023) addresses potential biases in teacher llms by referencing a wealth of open-source code, yielding more diverse and grounded data for code generation. to consider the capability of the student model and leverage the feedback of the teacher, persd (chen et al., 2023a) introduces a personalized distillation method where the teacher llm refines the student’s generated code based on the execution feedback of the executor. however, these models primarily target the code gener- ation task, lacking generalizability across a broader range of code-related tasks. to address this issue, mftcoder (liu et al., 2023d) utilizes self-instruct to distill diverse code data from teacher llms for various tasks, such as code comple- tion and text-to-code generation, training a student model via multi-task learning. wavecoder (yu et al., 2024), in contrast, creates a comprehensive instruction tuning dataset covering four universal code-related tasks distilled from gpt-3.5-turbo. wavecoder first selects a diverse coreset of raw data using the kcentergreedy (sener and savarese, 2018) clustering method, then employs the teacher llm for generating task definitions and outputs. the teacher model also plays a role in evaluating and filtering this data. notably, wavecoder demonstrates superior generalization across different code-related tasks compared to other open- source models. 4.5 multi-modality multimodal large language models (mllms) surpass tra- ditional language-only llms by understanding and pro- cessing information across multiple modalities, more closely mirroring human perception and enabling a broader range of real-world applications. there is a growing trend towards developing mllms that follow multimodal instructions,facilitating tasks with enhanced levels of interactivity. to ad- dress the scarcity of multimodal instruction-following data and to harness the commonsense and world knowledge embedded in teacher llms, numerous studies have focused on multimodal knowledge distillation from llms (liu et al., 2023e; zhao et al., 2023b; wang et al., 2023e; chen et al., 2023c; park et al., 2023; pi et al., 2023; zhao et al., 2023c; liu et al., 2023f; wu et al., 2023b; luo et al., 2023d; jiang et al., 2023d; li et al., 2023c; xu et al., 2023e). vision-language. in the vision-language domain, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, llava (liu et al., 2023e) pioneers the extension of the self-instruct approach from the language to the multimodal field. it translates images into textual descriptions, including captions and bounding boxes, and distills gpt-4 for generating new data in the context of seed examples. this approach creates a llava-instruct-150k dataset, which serves as the foundation for further developments like llava-1.5 (liu et al., 2023l) and gpt4roi (zhang et al., 2023e), enhancing the instruction- following capabilities of mllms. to expand the dataset’s scale, svit (zhao et al., 2023b) introduces a 4.2 million image dataset, distilled from gpt-4 by leveraging manual image annotations. it employs a novel data recipe to select an informative, diverse, and balanced subset of training data. lvis-instruct4v (wang et al., 2023e) leverages gpt- 4v (openai, 2023), a powerful large multimodal model, as a teacher to distill a more accurate and context-aware instruction-following dataset, focusing on fine-grained understanding. further advancements include integrating specific region referencing in image-based instruction following. for instance, shikra (chen et al., 2023c) uses gpt-4 to distill referential question-answer pairs from the flickr30k (plummer et al., 2015) dataset, enhancing the understanding of referential regions within images. lskd (park et al., 2023) introduces localized references to specific image regions, prompting the teacher llm to generate commonsense inferences about these areas. to enhance the visual instruction tuning pipeline with text-rich images, llavar (zhang et al., 2023d) employs the text-only gpt-4 as a teacher, using recognized texts and image captions to generate 16k conversation pairs for text-rich images. the resultant student mllm demonstrates enhanced interaction skills in content that combines both text and imagery. multiple modalities. to extend knowledge distillation of llms to encompass more modalities, such as audio and video, several innovative approaches have been in- troduced. these methods typically involve transforming these modalities into a textual format comprehensible to teacher llms, followed by the distillation of the teacher. macaw-llm (lyu et al., 2023) leverages gpt-4 to generate instruction-response pairs corresponding to the content of images or videos. mimic-it (li et al., 2023f) aims to broaden the scope to language, image, and video understanding, creating a substantial dataset with 2.8 million multimodal instruction-response pairs distilled from chatgpt. chat- bridge (zhao et al., 2023d), on the other hand, represents a novel approach in multimodal language modeling. it translates various non-textual modalities into text, combin- ing fine-grained and global descriptions. this information 23 verticalization distillationlaw lawyerllama (huang et al., 2023b), lawgpt (cui et al., 2023b), fuzi (wu et al., 2023d) medical and healthcarehuatuogpt (zhang et al., 2023c), huatuogpt-ii (chen et al., 2023d), doctorglm (xiong et al., 2023), alpacare (zhang et al., 2023f), huatuo (wang et al., 2023a), chatdoctor (li et al., 2023i), medalpaca (han et al., 2023), pmc-llama (wu et al., 2023e), disc-medllm (bao et al., 2023a) finance xuanyuan (zhang and yang, 2023) sciencedarwin (xie et al., 2023a), sciglm (zhang et al., 2024), wizardmath (luo et al., 2023b), mammoth (yue et al., 2023a), tora (gou et al., 2024), astrollama-chat (perkowski et al., 2024), g-llava (gao et al., 2023c), gimlet (zhao et al., 2023f), llm-prop (rubungo et al., 2023), instructmol (cao et al., 2023a), prot2text (abdine et al., 2023), biomedgpt (luo et al., 2023e), xtrimopglm (chen et al., 2024e), k2 (deng et al., 2023), oceangpt (bi et al., 2023), marinegpt (zheng et al., 2023b), geogalactica (lin et al., 2024), miscellaneous educhat (dan et al., 2023), owl (guo et al., 2023b) fig. 7: taxonomy of verticalization distillation. is then used to distill responses from chatgpt or gpt-4 through an in-context learning process, effectively bridging the gap between different modalities. others. beyond distilling instruction-following data, sev- eral methods have emerged that concentrate on harnessing different aspects of knowledge from llms. for instance, emma (yang et al., 2023c) trains an mllm to act as an embodied reflex agent within a visual environment. it achieves this by distilling gpt-4’s skills in a parallel textual world, generating actions and providing reflective feedback. silkie (li et al., 2023h) takes a unique approach by distilling preferences from gpt-4v , focusing on criteria like helpfulness and visual faithfulness. ha et al. (2023) represent another innovative direction, where it generates, labels, and distills diverse robot-centric exploration experiences by llms into a multi-task visuo-linguo-motor policy. 5 d omain -specified vertical distillation this section shifts from skill distillation to examine kd of llms in various vertical domains, including law, medical & healthcare, finance, and science, etc. it delves into cus- tomizing distilled llms for these fields, showing its signifi- cant role in enhancing domain-specific ai applications. the taxonomy of these works is shown in figure 7. 5.1 law law holds a crucial position in molding societies, over- seeing human interactions, and ensuring justice prevails. informed decision-making, legal interpretation, and the pro- vision of legal advice by professionals hinge on precise and current information. legal intelligent applications in different scenarios usually require combinations of multiple fundamental capabilities of legal text retrieval, understand- ing, reasoning and generating (zhang et al., 2023g; sun, 2023; lai et al., 2023). addressing the intricacies of legal ter- minology, subtle interpretations, and the constant evolution of legislation presents distinctive challenges that demand customized resolutions. to handle the above challenges, several studies have investigated the customization of llms for intelligent legal services (cui et al., 2023b; yue et al., 2023b; huang et al., 2023b; wu et al., 2023d). this involves a continued pre-training process on extensive legal corpora, followed by fine-tuning with self-constructed instructions or augmented data using advanced llms.huang et al. (2023b) have unveiled a chinese legal large model named lawyerllama. the model undergoes an initial pre-training phase on an extensive legal corpus, systematically assimilating knowledge of the chinese legal system. subsequently, fine-tuning occurs through the analy- sis of objective questions from the chinese national judicial examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge examination (zhong et al., 2020) and the gathering of re- sponses to legal consultations using chatgpt. this process equips the model with the ability to apply legal knowledge to specific scenarios. cui et al. (2023b) present lawgpt, built upon the foundation of openllama. the model is trained using a construction process that incorporates real- world legal text, legal regulations, judicial interpretations, and actual legal consultation data. additionally, the authors utilize the chatgpt api for assisted construction, enabling the generation of supplementary data derived from the existing dataset. wu et al. (2023d) have developed a large- scale chinese legal model (named fuzi) with chatglm as its foundation. this model undergoes training on an extensive chinese legal corpus, which incorporates unsu- pervised judicial language data, including diverse judgment documents and legal regulations. additionally, it undergoes supervised judicial fine-tuning with data encompassing le- gal qa and case retrieval. fuzi’s training also involves both general instruction fine-tuning datasets, such as alpaca, and domain-specific instruction fine-tuning datasets from lawyerllama (huang et al., 2023b) and lawgpt (cui et al., 2023b). 5.2 medical and healthcare the integration of llms carries substantial promise in fun- damentally reshaping the landscape of medical data anal- ysis, comprehension, and smart medical services (singhal et al., 2023; yang et al., 2024b). significant research endeav- ors have been dedicated to adapting general-purpose llms to the medical domain, given the ever-expanding wealth of information encompassing electronic health records, med- ical literature, and clinical data. especially in healthcare, llms are revolutionizing patient care, research, and admin- istrative efficiency. they enhance diagnostic accuracy by an- alyzing patient data and medical literature, offering person- alized recommendations, and identifying potential drug in- teractions. llms also streamline administrative tasks by au- tomating patient documentation and processing insurance claims, reducing the burden on healthcare providers and 24 improving patient experiences. furthermore, they facilitate medical research by synthesizing vast amounts of data to uncover new insights into diseases and treatments (will be discussed later). this integration of llms into healthcare is paving the way for more informed clinical decision-making, improved patient outcomes, and more efficient healthcare systems. these adaptations extend across a spectrum, ranging from refining the precision of medical diagnoses (wang et al., 2023k) and providing personalized treatment rec- ommendations (zhu et al., 2023) to automating routine administrative processes within healthcare settings. while existing studies predominantly concentrate on training using dedicated medical dialogue datasets com- prising medical textbooks (wu et al., 2023e), biomedical papers (luo et al., 2023e) medical knowledge-graphs (bao et al., 2023b), or authentic doctor-patient interactions (bao et al., 2023b), an expanding body of research is delving into the augmentation of medical instruction-following data with advanced llms to enhance the alignment of the intricacies within practical user instructions. zhang et al. (2023c) introduce huatuogpt specifically tailored for med- ical consultations. the model leverages both distilled data from chatgpt and real-world data from doctors during the supervised fine-tuning stage. in a parallel effort, xiong et al. (2023) construct a dataset of medical dialogues in chinese, employing chatgpt’s assistance. their method- ology encompassed various techniques to train doctor- glm, an easily deployable llm designed for tasks such as diagnoses, drug recommendations, and other medical advice. zhang et al. (2023f) fine-tune llama-series models using 52k diverse, machine-generated, medical instruction- following data named medinstruct-52k. this effort resulted in the development of alpacare, a model demonstrating robust medical proficiency and generalizability across both general and medical-specific domain free-form instruction evaluations. in a different vein, wang et al. (2023a) propose huatuo, a llama-based model that undergoes supervised fine-tuning with generated qa instances. this refinement process enhances the model’s possession of more reliable medical knowledge. li et al. (2023i) introduce chatdoctor, which was first trained as a generic conversation model based on llama. it utilized 52k instruction-following data from stanford university’s alpaca project (taori et al., 2023). subsequently, the conversation model underwent fine-tuning on a dataset of 100k patient-physician conver- sations collected from an online medical consultation web- site. this two-step training process underscores the model’s adaptability to diverse conversational contexts, particularly those specific to patient-physician interactions. built upon existing datasets, medalpaca (han et al., 2023) proposes to reconstruct the data with gpt-3.5-turbo, which is then used to fine-tune llms for effective medical applications. furthermore, pmc-llama (wu et al., 2023f) proposes a training framework (i.e., continual pre-training and domain-specific multi-task supervised fine-tuning) to adapt a general llm to the medicine domain, where gpt- 4 is leveraged to write synonymous sentences for data augmentation in the sft. to adapt llms to real-world medical consultation, disc-medllm (bao et al., 2023a) leverages gpt-3.5 to 1) construct 50k qa pairs in a few-shot manner and 2) re-generate the 420k dialogues based on real cases, which are then used to train llms in a supervised fine-tuning manner. more recently, huatuogpt- ii (chen et al., 2023d) proposes a one-stage training with instruction-formatting unification of domain data collection for medical adaption upon llms, where gpt-4 is used to formulate medical questions to fine-tuning instructions. these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- these diverse studies collectively contribute to the ad- vancing field of the medical domain, facilitated by knowl- edge distillation from advanced llms. through the ex- ploration of various methodologies, these approaches pro- vide valuable insights into the challenges and potential breakthroughs at the intersection of cutting-edge language models and medical applications. 5.3 finance the application of llms to the finance domain (xue et al., 2023) significantly transforms how financial data is ana- lyzed, decisions are made, and customer interactions are managed. in finance, llms offer unprecedented capabil- ities in understanding complex financial documents, pre- dicting market trends, and automating risk assessment, thus enabling more informed and faster decision-making processes. by processing and analyzing vast amounts of unstructured financial data, such as news articles, reports, and real-time market feeds, llms can identify patterns and insights that were previously inaccessible, leading to more accurate forecasts and strategic financial planning. furthermore, llms enhance customer experiences through personalized financial advice, automated customer service, and sophisticated chatbots that can handle intricate queries. this level of automation and insight has the potential to increase efficiency, reduce operational costs, and improve compliance and risk management practices in financial insti- tutions, making llms a transformative force in the finance sector. knowledge distillation from a proprietary llm is still under-explored, and most existing works focus on adapting llms to finance applications by continual pre-training on finance-specific corpora (wu et al., 2023g; lu et al., 2023) or fine-tuning in a supervised manner on multi-task finance- specific instructions (yang et al., 2023e; xie et al., 2023b; wang et al., 2023l). specifically, xuanyuan (zhang and yang, 2023) lever- ages self-instruct over seed data and self-qa over struc- tured/unstructured data to generate instruction data in the finance domain, which is used to train a finance llm. 5.4 science the integration of llms into the science domain (taylor et al., 2022; yin et al., 2023b) represents a paradigm shift in research, knowledge discovery, and the dissemination of scientific information. in science, llms are leveraged to digest and synthesize vast amounts of literature, aiding in the identification of new research opportunities and the ac- celeration of scientific breakthroughs. they facilitate the un- derstanding of complex scientific concepts by summarizing research papers, generating hypotheses, and even drafting research proposals and manuscripts, thus significantly re- ducing the time researchers spend on literature review and 25 enabling them to focus more on experimental work. llms also democratize access to scientific knowledge by pro- viding layperson summaries of complex research findings, making science more accessible to non-experts and fostering a broader public understanding of scientific advancements. by enhancing the efficiency of research workflows and fostering interdisciplinary collaborations, llms are poised to accelerate the pace of scientific discovery and innovation across various fields. to distill knowledge from an llm, darwin series (xie et al., 2023a) utilizes a semi self- instruct for instruction generation for science papers, which is then used to fine-tune an llm. sciglm (zhang et al., 2024) proposes to train a scientific llm, which prompts a teacher llm to generate detailed answers for unlabelled scientific questions, as well as a self-reflective critic-and- revise to improve data quality. besides the above knowledge distillation methods to adapt llms to science, we will also delve into how the distillation happens in sub-domains, e.g., mathematics, as- tronautics, chemistry, etc. mathematics. the application of llms within the sub- domain of mathematics heralds a transformative era in mathematical research, education, and problem-solving (azerbayev et al., 2023; yu et al., 2023b). llms in mathemat- ics facilitate the exploration and understanding of complex mathematical theories and problems by providing intuitive explanations, proofs, and solutions that can bridge the gap between advanced mathematical concepts and learn- ers at various levels. these models have shown potential in conjecturing new mathematical theorems and patterns, thus opening new avenues for research and discovery that might not have been readily accessible to humans alone. in education, they serve as personalized tutors, offering students step-by-step guidance through mathematical prob- lems and adapting explanations to the learner’s level of un- derstanding. this democratizes access to high-quality math- ematical education and fosters a deeper appreciation and understanding of mathematics among a broader audience. by enhancing collaborative efforts through the generation of new ideas and the simplification of complex concepts, llms are poised to significantly advance the field of math- ematics, making it more accessible, efficient, and innova- tive. wizardmath (luo et al., 2023b) enhances the mathe- matical reasoning capabilities of llama-2 by applying the novel reinforcement learning from evol-instruct feedback (rleif) method, significantly outperforming other open- source llms on the gsm8k and math benchmarks, as well as surpassing several closed-source llms including chatgpt-3.5 and minerva. mammoth (yue et al., 2023a) is a series of open-source llms specifically developed for gen- eral math problem-solving, achieving superior performance on nine mathematical reasoning datasets. utilizing a novel instruction tuning dataset called mathinstruct, which com- bines chain-of-thought and program-of-thought rationales, mammoth models demonstrate substantial improvements over existing models. tora (gou et al., 2024), a series of tool-integrated reasoning agents, significantly advances mathematical problem-solving by combining natural lan- guage reasoning with the use of external computational tools. it markedly outperforms existing open-source modelson 10 mathematical reasoning datasets, showcasing notable improvements over both rationale-based and program- based approaches, and introduces innovative training tech- niques such as output space shaping to enhance model rea- soning capabilities. g-llava (gao et al., 2023c) introduces a significant advancement in geometric problem-solving for llms by leveraging a multimodal approach that combines text and image data. this model, utilizing the geo170k dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. dataset comprising over 170,000 geometric image-caption and question-answer pairs, demonstrates remarkable im- provements over gpt-4v on the mathvista benchmark. astronautics. the application of llms in astronau- tics (nguyen et al., 2023) propels the field forward. astrollama-chat (perkowski et al., 2024) is an ad- vancement of the astrollama model, leveraging a 7b- parameter llama-2 model and targeted continual pre- training on a curated astronomy corpus to enhance per- formance in astronomy-focused question-answering. this model demonstrates significant improvements in special- ized topic comprehension and introduces a chat-enabled version for the astronomy community, highlighting the effectiveness of domain-specific knowledge distillation in achieving superior performance on specialized topics. chemistry and materials science. the integration of llms into chemistry and materials science has revolutionized the way researchers approach the discovery and develop- ment of new compounds and materials. by analyzing vast datasets and scientific literature, llms can predict the prop- erties and behaviors of substances, significantly accelerating the innovation cycle. gimlet (zhao et al., 2023f), graph instruction based molecule zero-shot learning, is a novel approach to molecule property prediction that integrates graph and text data within a single language model framework, aiming to improve instruction-based zero-shot learning for molec- ular tasks. by leveraging a transformer mechanism with generalized position embedding and decoupled attention, gimlet significantly outperforms traditional molecule-text baselines in zero-shot learning scenarios, demonstrating the model’s effectiveness in generalizing from instructions to a broad range of molecule-related tasks without prior explicit task-specific training. llm-prop (rubungo et al., 2023), leveraging the t5 model, showcases how llms can outperform sota graph neural networks in predicting the physical and electronic properties of crystalline solids from text descriptions. this approach underscores the potential of text-based methods in materials science, offering significant improvements in prediction accuracy while also contribut- ing a benchmark dataset, textedge, to foster further re- search in this emerging field. instructmol (cao et al., 2023a) integrates multi-modal data, aligning molecular structures with natural language instructions for drug discovery tasks. through a novel two-stage instruction-tuning approach, it significantly enhances performance in molecule-related tasks, establishing a reliable molecular assistant that outper- forms existing llms and reduces the performance gap with specialized models. this demonstrates the value of multi- modal integration in developing versatile tools for complex domains like drug discovery. 26 biology. in the field of biology, particularly in the study of pro- teins, dna, and rna, llms are revolutionizing our under- standing of the fundamental molecules of life. by analyzing vast datasets of biological sequences and structures, llms can predict the three-dimensional shapes of proteins, poten- tial functions, and interactions at a scale and speed beyond traditional computational methods. this capability is critical for unraveling the complexities of biological systems, ad- vancing drug discovery by identifying targets and designing molecules with high precision, and understanding genetic diseases through the interpretation of genomic variations. prot2text (abdine et al., 2023) introduces a novel multi- modal framework for generating protein function descrip- tions in free text by combining gnns and llms. this approach, which integrates structural and sequential protein information, highlights the transformative impact of knowl- edge distillation through the fusion of gnns and llms for accurate protein function prediction, potentially revolu- tionizing research in bioinformatics and biological sciences. biomedgpt (luo et al., 2023e) introduces a multimodal generative pre-trained transformer specifically designed for the biomedicine domain, emphasizing the significance of aligning molecular, protein, and natural language modal- ities to enhance biomedical question-answering, molecule, and protein qa tasks. this framework showcases the critical role of knowledge distillation in bridging the gap between complex biological data and human language, thereby fa- cilitating groundbreaking advancements in drug discovery and therapeutic target identification. xtrimopglm (chen et al., 2024e), a unified 100b-scale pre-trained transformer model, addresses both protein understanding and genera- tion tasks by integrating autoencoding and autoregressive pre-training objectives. its significant advancements over existing models in 18 protein understanding benchmarks and its capability in de novo protein sequence generation highlight the model’s importance in advancing the field of protein science through knowledge distillation. geography, geology, and environmental science. the inte- gration of llms into geography, geology, and environmen- tal science is revolutionizing these fields by enhancing data analysis, predictive modeling, and interdisciplinary research (roberts et al., 2023; lin et al., 2023b; wang et al., 2023m). k2 (deng et al., 2023), the first-ever llm specialized in the geoscience domain, demonstrates the significant impact of knowledge distillation in vertical domain specialization. by adapting the general-domain llama-7b model with a 5.5b token geoscience corpus and introducing the geosig- nal instruction tuning dataset, k2 showcases enhanced performance in geoscience knowledge understanding and utilization. the model’s development highlights a novel approach to efficiently gather domain-specific data and align model responses to specialized user queries, underpin- ning the importance of domain-specified vertical distillation in advancing research and applications within geoscience. oceangpt (bi et al., 2023), introduced as the first llm for ocean science tasks, underscores the vital role of knowl- edge distillation in the vertical domain of oceanography. it leverages doinstruct, a novel framework for generating domain-specific instruction data through multi-agent col-laboration, and establishes oceanbench, a benchmark for evaluating llms in the ocean domain. the model’s comprehensive experiments demonstrate its superior capa- bility in understanding and generating knowledge for ocean science, showcasing the significant potential of targeted knowledge distillation in enhancing domain-specific model performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language performance. marinegpt (zheng et al., 2023b) showcases the transformative potential of knowledge distillation in the marine domain by leveraging a novel vision-language model tailored for marine science. utilizing the marine-5m dataset, which includes over 5 million marine image-text pairs, marinegpt excels in providing detailed, accurate, and domain-specific responses. this advancement underscores the model’s ability to significantly enhance marine knowl- edge comprehension and application, emphasizing the crit- ical role of domain-specific distillation in bridging the gap between general-purpose models and specialized domain requirements. geogalactica (lin et al., 2024) represents a pioneering step in specializing llms for geoscience, lever- aging a 30 billion parameter model pre-trained on a vast geoscience corpus. this model, notable for being the largest of its kind within the geoscience domain, showcases the significant potential of knowledge distillation in fostering scientific discoveries by bridging artificial intelligence with geoscience research and applications. 5.5 miscellaneous the expansion of llms into various verticals beyond the ones previously discussed showcases their versatility and transformative potential across numerous industries and societal sectors. llms are being tailored to meet the spe- cific needs and challenges of different domains, from legal and governmental to entertainment and beyond, providing sophisticated natural language understanding, generation, and decision-making capabilities. education. educhat (dan et al., 2023) is a large-scale lan- guage model-based chatbot system designed for the educa- tion domain. it aims to revolutionize intelligent education by providing personalized, fair, and compassionate support to teachers, students, and parents. knowledge distillation is emphasized through the pre-training on an educational corpus and fine-tuning on custom instructions to activate education-specific functions like open question answering, essay assessment, and emotional support. educhat demon- strates the importance of domain-specified knowledge dis- tillation in enhancing the performance of llms within specific verticals, offering a significant contribution to in- telligent education technology. it operations. owl (guo et al., 2023b) is a specialized llm tailored for it operations, focusing on enhancing efficiency and analysis within this domain. the model leverages a unique owl-instruct dataset covering a broad range of it-related information, employing a mixture-of- adapter strategy for efficient domain-specific tuning. this approach significantly improves it operation tasks’ perfor- mance, highlighting the critical role of knowledge distilla- tion in adapting general llms to specialized fields such as it operations, thereby pushing forward the frontier in specialized ai applications within this sector. 27 6 o penproblems further data selection how much data is required for llm distillation and how to filter out the low-quality data remain open-domain questions. in the field of instruction tuning, one of the most commonly used methods for distillation, zhou et al. (2023a) propose that only 1000 human-curated high-quality data is enough for the alignment of llms, hypothesizing that llms have learned the required knowl- edge from pretraining and only a small amount of data is required for the alignment. its finding further raises a new question, how to automatically select the data for better distillation? chen et al. (2023e) directly apply chatgpt to rate each data sample together with explanations, and then the data is selected based on the rating. cao et al. (2023b) split the existing instruction-tuning datasets and trains a linear function to select the most effective data based on their statistical properties. li et al. (2023j) propose a data selection pipeline similar to self-distillation, in which the llm firstly learns from a small subset of the data to get the basic ability, and then further uses this learned model to rate for the original dataset. du et al. (2023b) propose to consider three aspects including quality, coverage, and necessity for the filtering process. li et al. (2023k) select instruction data by evaluating their one-shot improvement on a hold-out set. li et al. (2024f) recently propose superfiltering, which is able to utilize small language models like gpt2 to filter out the high-quality subset from a given high-quality dataset. despite the emergence of these works working on data fil- tering, how to efficiently select the optimal distillation data for llms, and how much data is required for distillation are still unsolved. reduce the distillation cost (lightweight methods) de- spite the remarkable abilities of the latest llms, their sig- nificant resource requirements underscore the urgent need to find efficient solutions to overcome these challenges. common ways to further reduce the distillation cost include model compression and efficient fine-tuning. in the realm of model compression, quantization (frantar et al., 2023; dettmers et al., 2022; kim et al., 2023c; tao et al., 2022b; yao et al., 2022; xiao et al., 2023), parameter pruning (ma et al., 2023d; zhang et al., 2023h; frantar and alistarh, 2023), and low-rank approximation (xu et al., 2023g; li et al., 2023l) are commonly utilized. in the realm of efficient fine-tuning, parameter efficient fine-tuning (hu et al., 2023b; liu et al., 2022c; wang et al., 2022b; hu et al., 2021; li and liang, 2021; liu et al., 2022d), and memory efficient fine-tuning (dettmers et al., 2023; kim et al., 2023d; malladi et al., 2024) are utilized. a detailed survey on efficient large language models can be found here in wan et al. (2024b). the problem that remains is how can we further compress the model and build effective distillation algorithms. multi-teacher distillation most of the existing distilled models are distilled from a single teacher model, how- ever, it is widely accepted that models trained with dif- ferent sources of data have various capabilities. thus a question arises: is it possible to distill knowledge from different teacher models into one student model? babyl- lama (timiryasov and tastet, 2023) proposes to distill the knowledge from both the gpt2 and llama into the small-size student models. ensemble-instruct (lee et al., 2023b) tries to generate both instructions and responses ensembled from several different llms with rougel as the indicator. fusellm (wan et al., 2024a) externalizes the collective knowledge and unique strengths by leveraging the genera- tive distributions of different llms aiming to train a student model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated model beyond those of any individual source llm. despite the recent progress in this topic, it still remains an under- explored topic. explore richer knowledge from teacher llms as indicated in table 3, the majority of teacher llms are closed-source due to their advanced capabilities. consequently, current methodologies primarily focus on using the generations from these models as hard labels, training student models through simple supervised fine-tuning. however, beyond the straightforward imitation of output behaviors via hard labels, there is a growing interest in harnessing richer knowledge from teacher llms, including feedback and feature knowledge, as well as exploring diverse combina- tions of knowledge elicitation methods. as highlighted in thefeedback section, teachers can provide various types of feedback based on the student’s outputs (lee et al., 2023a; jiang et al., 2023b; chen et al., 2023a). similarly, the feature section discusses how knowledge based on features, such as logits serving as soft labels, can offer deeper, intrinsic insights into the teacher model (gu et al., 2024; agarwal et al., 2024). these explorations have demonstrated promis- ing outcomes, suggesting that access to a broader spectrum of knowledge can significantly enhance student model per- formance beyond what is achievable through simple sft distillation alone. this highlights the critical need for further research into varied knowledge extraction methods from teacher llms to augment the effectiveness of kd processes. overcoming catastrophic forgetting during distillation previous research has delved into the fine-tuning of llms to acquire the ability to follow instructions or transfer knowledge for forthcoming tasks, skills, or domains, lever- aging advancements in llm technology. nevertheless, in- vestigations have revealed that the continual fine-tuning of llms on particular datasets (skills, domains) can lead to a phenomenon known as catastrophic forgetting, wherein previously acquired knowledge and problem-solving abil- ities for earlier tasks are compromised (chen et al., 2023f; kotha et al., 2023; koloski et al., 2023; wu et al., 2024; luo et al., 2023f). earlier studies in machine learning and deep learning have investigated various techniques to help mitigate forgetting during the fine-tuning or continue learn- ing process, such as rehearsal, which entails periodically revisiting and training on past data (kirkpatrick et al., 2017; rostami et al., 2019; rolnick et al., 2019), as well as reg- ularization methods like elastic weight consolidation (lee et al., 2017), or dynamic architecture methods (mallya et al., 2018; wang et al., 2022c; hu et al., 2023c; chen et al., 2023f). to address the challenges of catastrophic forgetting and to enhance the diversity of generated instructions in knowl- edge distillation for llms, jiang et al. (2023b) randomly sample an instruction from the easy instructions and also prompt the generator to generate a new instruction that belongs to the same domain as the sampled one. in a similar vein, li et al. (2023m) study the problem of instruction- 28 tuning in multi-modal llms knowledge distillation and introduce a competitive distillation framework. the model tries to produce new instructions that differ in content but are similar in difficulty to the original pictures in the multi- modal augmentation phase, so as to alleviate catastrophic forgetting of the model and enhance the diversity of the instruction tuning pool. chen et al. (2023f) propose the lifelong-moe (mixture-of experts) architecture based on general language models, which dynamically adds model capacity via adding experts with regularized pretraining. additionally, the model also introduces implicit regulariza- tion via distillation of the knowledge from old experts and gatings to effectively preserve old knowledge. zeng et al. (2023b) propose a new generative-based rehearsal method as dirichlet continual learning (dcl). this method com- bines task distribution modeling and knowledge distillation to mitigate catastrophic forgetting without requiring access to the old data. to evaluate the effectiveness of instruction tuning in the context of continuous learning tasks, zhang et al. (2023i) introduce a more challenging yet practical problem called continual instruction tuning (cit) and also establish a benchmark suite consisting of learning and eval- uation protocols. although current research has explored some simple methods to alleviate knowledge forgetting dur- ing model fine-tuning or knowledge distillation processes, effectively avoiding catastrophic forgetting across domains and skills remains a challenging issue. how to retain the original model’s capabilities effectively during knowledge distillation or transfer processes is still a challenging prob- lem. trustworthy knowledge distillation trustworthiness in llms is paramount, encompassing attributes such as truth- fulness, safety, fairness, robustness, privacy, and adherence to machine ethics (sun et al., 2024a). the rapid advancement of llms brings to the forefront concerns regarding their trustworthiness, stemming from their complex outputs, the biases present in vast training datasets, and the potential inclusion of private information. current efforts in kd of llms primarily focus on distilling various skills from llms, with relatively little attention paid to trustworthiness aspects. existing studies tend to concentrate on a subset of trustworthiness aspects, such as helpfulness, honesty, and harmlessness (bai et al., 2022a; yang et al., 2024a; cui et al., 2023a). consequently, in the distillation process, student models may inherit issues related to trustworthiness from their teacher llms. as assessed in sun et al. (2024a), smaller open-source llms generally fall short of their proprietary counterparts in trustworthiness metrics. therefore, consid- ering trustworthiness alongside the distillation of capabil- ities into student models is crucial. it is imperative that future research on kd not only enhances the capabilities of student models but also ensures that broader aspects of trustworthiness are meticulously addressed. weak-to-strong distillation. the concept of “weak-to- strong generalization” in llms (burns et al., 2023) empha- sizes the potential to leverage weak supervision to elicit the advanced capabilities of more powerful models. this approach challenges the traditional distillation paradigm by suggesting that even with limited or imperfect supervision, it is possible to enhance the performance of llms sig-nificantly. this necessitates exploring innovative strategies that enable weaker models to guide the learning process of stronger ones effectively, highlighting the importance of developing methods that can bridge the gap between these models. such research could unlock new avenues for improving llms’ efficiency and effectiveness, making the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision the pursuit of “weak-to-strong distillation” a crucial area for future investigations in this llm era. initially, burns et al. (2023) investigate whether weak model supervision can unlock the full capabilities of much stronger models. through experiments with pre-trained language models in the gpt-4 family across nlp , chess, and reward modeling tasks, it finds that finetuning strong models on weak labels leads to better performance than their weak supervisors, demonstrating weak-to-strong generalization. then, li et al. (2024g) introduce superfiltering, a method that employs smaller, weaker models like gpt-2 to select high-quality data for fine-tuning larger, more capable models such as llama2. this approach is rooted in discovering a strong consistency in evaluating instruction tuning data difficulty across models of varying sizes. more recently, ji et al. (2024) introduce aligner, a novel approach for aligning llms with human values and intentions by utilizing weak supervisory signals from smaller models to improve the performance of larger models. however, burns et al. (2023) find that achieving the full capabilities of strong models requires more than naive finetuning, suggesting the need for further research in this area. therefore, open questions still remain about 1) what are the theoretical and practical limits of weak-to-strong distillation? can weak supervision reliably extract and enhance the full spectrum of capabilities in stronger models across all domains, or are there inherent limitations based on model architecture or task specificity? 2) how do we identify or design the optimal weak su- pervisors for distilling knowledge into stronger models? is there a framework or criteria to predict which weak models would be most effective in guiding the learning process of more complex models for specific tasks? 3) to what extent are weak-to-strong distillation techniques transferable and scalable across different sizes and types of models? how can these methods be adapted to ensure efficacy and ef- ficiency in distilling knowledge from very large models to significantly smaller ones, especially in resource-constrained environments? self-alignment. aligning llms traditionally relies heavily on human or teacher llms to supply extensive preference data. consequently, the alignment of the student model is limited by the quantity of distilled preference data and the teacher’s capabilities. self-alignment offers a promising alternative, aiming to enhance alignment beyond the con- straints of teacher-provided preferences. in self-alignment, the student model endeavors to autonomously improve and align its responses with desired behaviors, including generating model-written feedback, critiques, and explana- tions. several studies have explored utilizing the student model’s inherent capabilities to generate knowledge for alignment (bai et al., 2022a; sun et al., 2024b; li et al., 2024c; yuan et al., 2024a). beyond merely producing improved responses (bai et al., 2022a; sun et al., 2024b), implemen- tations of self-alignment include employing the student as 29 its reward model to offer feedback (yuan et al., 2024a), a strategy that merges self-knowledge with feedback methods of eliciting knowledge. we advocate for increasingly lever- aging the student model itself to provide feedback, thereby enhancing self-alignment capabilities. this approach not only facilitates moving beyond traditional human/teacher preference-based rewards but also opens avenues for con- tinual self-improvement and alignment. 7 c onclusion and discussion this survey has traversed the expansive domain of knowl- edge distillation applied to llms, shedding light on the myriad techniques, applications, and emerging challenges in this vibrant field. we have underscored the pivotal role of kd in democratizing access to the advanced capabilities of proprietary llms, thereby fostering a more equitable ai landscape. through meticulous examination, we have highlighted how kd serves as a bridge, enabling resource- constrained entities to benefit from the profound advance- ments in llms without the prohibitive costs associated with training and deploying state-of-the-art models. our exploration delineates the multifaceted approaches to kd, ranging from algorithmic innovations and skill en- hancement to domain-specific distillations. each segment reveals the nuanced complexities and potentialities inherent in tailoring distilled models to emulate the sophisticated un- derstandings and functionalities of their more cumbersome counterparts. notably, the integration of data augmentation strategies within kd processes emerges as a critical lever for enhancing distillation in this llm era, underscoring the synergistic potential between generating context-rich training data and the distillation endeavor. as we project into the future, several avenues for re- search beckon. the evolving landscape of ai, marked by rapid advancements in model architectures and training methodologies, presents both challenges and opportunities for kd. the quest for more efficient, transparent, and ethical ai models necessitates continued innovation in kd tech- niques, especially those that can navigate the delicate bal- ance between model fidelity, computational efficiency, and ethical considerations. furthermore, the exploration of kd in nascent areas such as weak-to-strong generalization, self- alignment, multi-modal llms, real-time adaptation, and personalized ai services promises to expand the horizons of what distilled models can achieve. therefore, knowledge distillation of llms stands at a critical juncture, embodying the potential to significantly influence the trajectory of ai development and application. as this survey elucidates, the concerted efforts of the re- search community in pushing the boundaries of kd will be instrumental in realizing the vision of accessible, efficient, and responsible ai for all. legal considerations for using llm outputs: impor- tantly, it’s crucial to note the legal implications of utilizing llm outputs, such as those from chatgpt4, llama5, etc. we strongly advocate compliance with the terms of use specified by the model providers, such as the restrictions on developing competitive products, and so on. 4. https://openai.com/policies/business-terms 5. https://llama.meta.com/llama-downloads/references l. ouyang, j. wu, x. jiang, d. almeida, c. wainwright, p . mishkin, c. zhang, s. agarwal, k. slama, a. ray et al. , “training language models to follow instructions with human feedback,” advances in neural information processing systems , vol. 35, pp. 27 730–27 744, 2022. openai, :, j. achiam, s. adler, s. agarwal, l. ahmad, i. akkaya, f. l. aleman, d. almeida, j. altenschmidt, s. altman, s. anadkat, r. avila, i. babuschkin, s. balaji, v . balcom, p . baltescu, h. bao, m. bavarian, j. belgum, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, i. bello, j. berdine, g. bernadett-shapiro, c. berner, l. bog- donoff, o. boiko, m. boyd, a.-l. brakman, g. brockman, t. brooks, m. brundage, k. button, t. cai, r. campbell, a. cann, b. carey, c. carlson, r. carmichael, b. chan, c. chang, f. chantzis, d. chen, s. chen, r. chen, j. chen, m. chen, b. chess, c. cho, c. chu, h. w. chung, d. cummings, j. currier, y. dai, c. decareaux, t. degry, n. deutsch, d. deville, a. dhar, d. dohan, s. dowling, s. dunning, a. ecoffet, a. eleti, t. eloundou, d. farhi, l. fedus, n. felix, s. p . fishman, j. forte, i. fulford, l. gao, e. georges, c. gibson, v . goel, t. gogineni, g. goh, r. gontijo-lopes, j. gordon, m. grafstein, s. gray, r. greene, j. gross, s. s. gu, y. guo, c. hallacy, j. han, j. harris, y. he, m. heaton, j. heidecke, c. hesse, a. hickey, w. hickey, p . hoeschele, b. houghton, k. hsu, s. hu, x. hu, j. huizinga, s. jain, s. jain, j. jang, a. jiang, r. jiang, h. jin, d. jin, s. jomoto, b. jonn, h. jun, t. kaf- tan, łukasz kaiser, a. kamali, i. kanitscheider, n. s. keskar, t. khan, l. kilpatrick, j. w. kim, c. kim, y. kim, h. kirchner, j. kiros, m. knight, d. kokotajlo, łukasz kondraciuk, a. kondrich, a. konstantinidis, k. kosic, g. krueger, v . kuo, m. lampe, i. lan, t. lee, j. leike, j. leung, d. levy, c. m. li, r. lim, m. lin, s. lin, m. litwin, t. lopez, r. lowe, p . lue, a. makanju, k. mal- facini, s. manning, t. markov, y. markovski, b. mar- tin, k. mayer, a. mayne, b. mcgrew, s. m. mckin- ney, c. mcleavey, p . mcmillan, j. mcneil, d. medina, a. mehta, j. menick, l. metz, a. mishchenko, p . mishkin, v . monaco, e. morikawa, d. mossing, t. mu, m. murati, o. murk, d. m ´ely, a. nair, r. nakano, r. nayak, a. nee- lakantan, r. ngo, h. noh, l. ouyang, c. o’keefe, j. pa- chocki, a. paino, j. palermo, a. pantuliano, g. parascan- dolo, j. parish, e. parparita, a. passos, m. pavlov, a. peng, a. perelman, f. de avila belbute peres, m. petrov, h. p . de oliveira pinto, michael, pokorny, m. pokrass, v . pong, t. powell, a. power, b. power, e. proehl, r. puri, a. rad- ford, j. rae, a. ramesh, c. raymond, f. real, k. rimbach, c. ross, b. rotsted, h. roussez, n. ryder, m. saltarelli, t. sanders, s. santurkar, g. sastry, h. schmidt, d. schnurr, j. schulman, d. selsam, k. sheppard, t. sherbakov, j. shieh, s. shoker, p . shyam, s. sidor, e. sigler, m. simens, j. sitkin, k. slama, i. sohl, b. sokolowsky, y. song, n. staudacher, f. p . such, n. summers, i. sutskever, j. tang, n. tezak, m. thompson, p . tillet, a. tootoonchian, e. tseng, p . tuggle, n. turley, j. tworek, j. f. c. uribe, a. vallone, a. vijayvergiya, c. voss, c. wainwright, j. j. wang, a. wang, b. wang, j. ward, j. wei, c. weinmann, a. welihinda, p . welinder, j. weng, l. weng, m. wiethoff, d. willner, c. winter, s. wolrich, h. wong, l. workman, s. wu, j. wu, m. wu, k. xiao, t. xu, s. yoo, k. yu, 30 q. yuan, w. zaremba, r. zellers, c. zhang, m. zhang, s. zhao, t. zheng, j. zhuang, w. zhuk, and b. zoph, “gpt- 4 technical report,” 2023. g. team, r. anil, s. borgeaud, y. wu, j.-b. alayrac, j. yu, r. soricut, j. schalkwyk, a. m. dai, a. hauth et al. , “gemini: a family of highly capable multimodal models,” arxiv preprint arxiv:2312.11805 , 2023. j. wei, y. tay, r. bommasani, c. raffel, b. zoph, s. borgeaud, d. yogatama, m. bosma, d. zhou, d. metzler, e. h. chi, t. hashimoto, o. vinyals, p . liang, j. dean, and w. fedus, “emergent abilities of large language models,” trans. mach. learn. res. , vol. 2022, 2022. [online]. available: https://openreview.net/forum?id=yzksu5zdwd j. wei, x. wang, d. schuurmans, m. bosma, f. xia, e. chi, q. v . le, d. zhou et al. , “chain-of-thought prompting elicits reasoning in large language models,” advances in neural information processing systems , vol. 35, pp. 24 824– 24 837, 2022. x. xu, c. tao, t. shen, c. xu, h. xu, g. long, and j. guang lou, “re-reading improves reasoning in large language models,” 2024. p . liang, r. bommasani, t. lee, d. tsipras, d. soylu, m. yasunaga, y. zhang, d. narayanan, y. wu, a. kumar, b. newman, b. yuan, b. yan, c. zhang, c. cosgrove, c. d. manning, c. r ´e, d. acosta-navas, d. a. hudson, e. zelikman, e. durmus, f. ladhak, f. rong, h. ren, h. yao, j. wang, k. santhanam, l. j. orr, l. zheng, m. y ¨uksekg ¨on¨ul, m. suzgun, n. kim, n. guha, n. s. chatterji, o. khattab, p . henderson, q. huang, r. chi, s. m. xie, s. santurkar, s. ganguli, t. hashimoto, t. icard, t. zhang, v . chaudhary, w. wang, x. li, y. mai, y. zhang, and y. koreeda, “holistic evaluation of language models,” corr , vol. abs/2211.09110, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09110 x. wu, r. duan, and j. ni, “unveiling security, privacy, and ethical concerns of chatgpt,” journal of information and intelligence , 2023. h. touvron, l. martin, k. stone, p . albert, a. almahairi, y. babaei, n. bashlykov, s. batra, p . bhargava, s. bhosale, d. bikel, l. blecher, c. c. ferrer, m. chen, g. cucurull, d. esiobu, j. fernandes, j. fu, w. fu, b. fuller, c. gao, v . goswami, n. goyal, a. hartshorn, s. hosseini, r. hou, h. inan, m. kardas, v . kerkez, m. khabsa, i. kloumann, a. korenev, p . s. koura, m.-a. lachaux, t. lavril, j. lee, d. liskovich, y. lu, y. mao, x. martinet, t. mihaylov, p . mishra, i. molybog, y. nie, a. poulton, j. reizen- stein, r. rungta, k. saladi, a. schelten, r. silva, e. m. smith, r. subramanian, x. e. tan, b. tang, r. taylor, a. williams, j. x. kuan, p . xu, z. yan, i. zarov, y. zhang, a. fan, m. kambadur, s. narang, a. rodriguez, r. stojnic, s. edunov, and t. scialom, “llama 2: open foundation and fine-tuned chat models,” 2023. a. q. jiang, a. sablayrolles, a. mensch, c. bamford, d. s. chaplot, d. de las casas, f. bressand, g. lengyel, g. lam- ple, l. saulnier, l. r. lavaud, m.-a. lachaux, p . stock, t. l. scao, t. lavril, t. wang, t. lacroix, and w. e. sayed, “mistral 7b,” 2023. l. zheng, w. chiang, y. sheng, s. zhuang, z. wu, y. zhuang, z. lin, z. li, d. li, e. p . xing, h. zhang, j. e. gonzalez, and i. stoica, “judging llm-as-a-judge with mt-bench and chatbot arena,” corr , vol. abs/2306.05685, 2023. [online].available: https://doi.org/10.48550/arxiv.2306.05685 l. sun, y. huang, h. wang, s. wu, q. zhang, c. gao, y. huang, w. lyu, y. zhang, x. li, z. liu, y. liu, y. wang, z. zhang, b. kailkhura, c. xiong, c. xiao, c. li, e. xing, f. huang, h. liu, h. ji, h. wang, h. zhang, h. yao, m. kellis, m. zitnik, m. jiang, m. bansal, j. zou, j. pei, j. liu, j. gao, j. han, j. zhao, j. tang, j. wang, j. mitchell, k. shu, k. xu, k.-w. chang, l. he, l. huang, m. backes, n. z. gong, p . s. yu, p .-y. chen, q. gu, r. xu, r. ying, s. ji, s. jana, t. chen, t. liu, t. zhou, w. wang, x. li, x. zhang, x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. x. wang, x. xie, x. chen, x. wang, y. liu, y. ye, y. cao, y. chen, and y. zhao, “trustllm: trustworthiness in large language models,” 2024. j. gou, b. yu, s. j. maybank, and d. tao, “knowledge distillation: a survey,” international journal of computer vision , vol. 129, pp. 1789–1819, 2021. m. gupta and p . agrawal, “compression of deep learning models for text: a survey,” acm transactions on knowledge discovery from data (tkdd) , vol. 16, no. 4, pp. 1–55, 2022. s. y. feng, v . gangal, j. wei, s. chandar, s. vosoughi, t. mi- tamura, and e. hovy, “a survey of data augmentation approaches for nlp,” arxiv preprint arxiv:2105.03075 , 2021. r. taori, i. gulrajani, t. zhang, y. dubois, x. li, c. guestrin, p . liang, and t. b. hashimoto, “stanford alpaca: an instruction-following llama model,” https://github.com/ tatsu-lab/stanford alpaca, 2023. y. gu, l. dong, f. wei, and m. huang, “minillm: knowledge distillation of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=5h0qf7ibzz r. agarwal, n. vieillard, y. zhou, p . stanczyk, s. r. garea, m. geist, and o. bachem, “on-policy distillation of language models: learning from self-generated mistakes,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=3zktaqxlhw w. yuan, r. y. pang, k. cho, s. sukhbaatar, j. xu, and j. weston, “self-rewarding language models,” 2024. z. chen, y. deng, h. yuan, k. ji, and q. gu, “self-play fine-tuning converts weak language models to strong language models,” 2024. y. huang, y. chen, z. yu, and k. mckeown, “in-context learning distillation: transferring few-shot learning abil- ity of pre-trained language models,” 2022. g. cui, l. yuan, n. ding, g. yao, w. zhu, y. ni, g. xie, z. liu, and m. sun, “ultrafeedback: boosting lan- guage models with high-quality feedback,” arxiv preprint arxiv:2310.01377 , 2023. s. mukherjee, a. mitra, g. jawahar, s. agarwal, h. palangi, and a. awadallah, “orca: progressive learning from complex explanation traces of gpt-4,” arxiv preprint arxiv:2306.02707 , 2023. b. ding, c. qin, l. liu, y. k. chia, b. li, s. joty, and l. bing, “is gpt-3 a good data annotator?” in acl (1) . asso- ciation for computational linguistics, 2023, pp. 11 173– 11 195. s. chaudhary, “code alpaca: an instruction-following llama model for code generation,” https://github.com/ sahil280114/codealpaca, 2023. h. wang, c. liu, n. xi, z. qiang, s. zhao, b. qin, and 31 t. liu, “huatuo: tuning llama model with chinese medi- cal knowledge,” arxiv preprint arxiv:2304.06975 , 2023. lawgpt . github, 2023. d. zhang, z. hu, s. zhoubian, z. du, k. yang, z. wang, y. yue, y. dong, and j. tang, “sciglm: training scientific language models with self-reflective instruction annotation and tuning,” corr , vol. abs/2401.07950, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401. 07950 w.-l. chiang, z. li, z. lin, y. sheng, z. wu, h. zhang, l. zheng, s. zhuang, y. zhuang, j. e. gonzalez, i. stoica, and e. p . xing, “vicuna: an open-source chatbot impressing gpt-4 with 90%* chatgpt quality,” march 2023. [online]. available: https://lmsys.org/blog/2023-03-30- vicuna/ c. xu, q. sun, k. zheng, x. geng, p . zhao, j. feng, c. tao, and d. jiang, “wizardlm: empowering large language models to follow complex instructions,” arxiv preprint arxiv:2304.12244 , 2023. w. x. zhao, k. zhou, j. li, t. tang, x. wang, y. hou, y. min, b. zhang, j. zhang, z. dong, y. du, c. yang, y. chen, z. chen, j. jiang, r. ren, y. li, x. tang, z. liu, p . liu, j.-y. nie, and j.-r. wen, “a survey of large language models,” 2023. x. he, z. lin, y. gong, a. jin, h. zhang, c. lin, j. jiao, s. m. yiu, n. duan, w. chen et al. , “annollm: making large language models to be better crowdsourced annotators,” arxiv preprint arxiv:2303.16854 , 2023. y. wang, z. yu, z. zeng, l. yang, c. wang, h. chen, c. jiang, r. xie, j. wang, x. xie, w. ye, s. zhang, and y. zhang, “pandalm: an automatic evaluation benchmark for llm instruction tuning optimization,” 2023. c. hsieh, c. li, c. yeh, h. nakhost, y. fujii, a. ratner, r. krishna, c. lee, and t. pfister, “distilling step-by-step! outperforming larger language models with less training data and smaller model sizes,” in acl (findings) . associ- ation for computational linguistics, 2023, pp. 8003–8017. a. mitra, l. d. corro, s. mahajan, a. codas, c. simoes, s. agarwal, x. chen, a. razdaibiedina, e. jones, k. aggar- wal, h. palangi, g. zheng, c. rosset, h. khanpour, and a. awadallah, “orca 2: teaching small language models how to reason,” 2023. c. xu, d. guo, n. duan, and j. j. mcauley, “baize: an open- source chat model with parameter-efficient tuning on self- chat data,” in emnlp . association for computational linguistics, 2023, pp. 6268–6278. x. yue, x. qu, g. zhang, y. fu, w. huang, h. sun, y. su, and w. chen, “mammoth: building math generalist mod- els through hybrid instruction tuning,” arxiv preprint arxiv:2309.05653 , 2023. l. chenglin, c. qianglong, w. caiyu, and z. yin, “mixed distillation helps smaller language model better reason- ing,” 2023. y. wang, y. kordi, s. mishra, a. liu, n. a. smith, d. khashabi, and h. hajishirzi, “self-instruct: aligning language model with self generated instructions,” arxiv preprint arxiv:2212.10560 , 2022. z. sun, y. shen, q. zhou, h. zhang, z. chen, d. cox, y. yang, and c. gan, “principle-driven self-alignment of language models from scratch with minimal human supervision,” advances in neural information processingsystems , vol. 36, 2024. z. luo, c. xu, p . zhao, q. sun, x. geng, w. hu, c. tao, j. ma, q. lin, and d. jiang, “wizardcoder: empowering code large language models with evol-instruct,” arxiv preprint arxiv:2306.08568 , 2023. h. luo, q. sun, c. xu, p . zhao, j. lou, c. tao, x. geng, q. lin, s. chen, and d. zhang, “wizardmath: empower- ing mathematical reasoning for large language models via reinforced evol-instruct,” arxiv preprint arxiv:2308.09583 , 2023. h. dai, z. liu, w. liao, x. huang, y. cao, z. wu, l. zhao, s. xu, w. liu, n. liu, s. li, d. zhu, h. cai, l. sun, q. li, d. shen, t. liu, and x. li, “auggpt: leveraging chatgpt for text data augmentation,” 2023. z. he, m. t. ribeiro, and f. khani, “targeted data generation: finding and fixing model weaknesses,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8506–8520. [online]. available: https://aclanthology.org/2023.acl-long.474 n. ding, y. chen, b. xu, y. qin, s. hu, z. liu, m. sun, and b. zhou, “enhancing chat language models by scaling high-quality instructional conversations,” in emnlp . as- sociation for computational linguistics, 2023, pp. 3029– 3051. s. gunasekar, y. zhang, j. aneja, c. c. t. mendes, a. d. giorno, s. gopi, m. javaheripi, p . kauffmann, g. de rosa, o. saarikivi, a. salim, s. shah, h. s. behl, x. wang, s. bubeck, r. eldan, a. t. kalai, y. t. lee, and y. li, “textbooks are all you need,” 2023. y. li, s. bubeck, r. eldan, a. del giorno, s. gunasekar, and y. t. lee, “textbooks are all you need ii: phi-1.5 technical report,” arxiv preprint arxiv:2309.05463 , 2023. phi-2: the surprising power of small lan- guage models , december 2023. [online]. avail- able: https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/ y. wei, z. wang, j. liu, y. ding, and l. zhang, “magicoder: source code is all you need,” 2023. z. yu, x. zhang, n. shang, y. huang, c. xu, y. zhao, w. hu, and q. yin, “wavecoder: widespread and versatile en- hanced instruction tuning with refined data generation,” 2024. j. ye, j. gao, q. li, h. xu, j. feng, z. wu, t. yu, and l. kong, “zerogen: efficient zero-shot learning via dataset generation,” in emnlp . association for computational linguistics, 2022, pp. 11 653–11 669. j. gao, r. pi, y. lin, h. xu, j. ye, z. wu, w. zhang, x. liang, z. li, and l. kong, “self-guided noise-free data generation for efficient zero-shot learning,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=h5opjgd lo6 l. h. bonifacio, h. q. abonizio, m. fadaee, and r. f. nogueira, “inpars: data augmentation for information retrieval using large language models,” corr , vol. abs/2202.05144, 2022. [online]. available: https://arxiv. org/abs/2202.05144 i. timiryasov and j.-l. tastet, “baby llama: knowledge 32 distillation from an ensemble of teachers trained on a small dataset with no performance penalty,” in proceedings of the babylm challenge at the 27th conference on computational natural language learning , a. warstadt, a. mueller, l. choshen, e. wilcox, c. zhuang, j. ciro, r. mosquera, b. paranjabe, a. williams, t. linzen, and r. cotterell, eds. singapore: association for computational linguistics, dec. 2023, pp. 279–289. [online]. available: https://aclanthology.org/2023.conll- babylm.24 c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre- trained language models via quantization,” arxiv preprint arxiv:2203.10705 , 2022. z. liu, b. oguz, c. zhao, e. chang, p . stock, y. mehdad, y. shi, r. krishnamoorthi, and v . chandra, “llm-qat: data-free quantization aware training for large language models,” arxiv preprint arxiv:2305.17888 , 2023. y. bai, s. kadavath, s. kundu, a. askell, j. kernion, a. jones, a. chen, a. goldie, a. mirhoseini, c. mckinnon, c. chen, c. olsson, c. olah, d. hernandez, d. drain, d. gan- guli, d. li, e. tran-johnson, e. perez, j. kerr, j. mueller, j. ladish, j. landau, k. ndousse, k. lukosuite, l. lovitt, m. sellitto, n. elhage, n. schiefer, n. mercado, n. das- sarma, r. lasenby, r. larson, s. ringer, s. johnston, s. kravec, s. e. showk, s. fort, t. lanham, t. telleen- lawton, t. conerly, t. henighan, t. hume, s. r. bow- man, z. hatfield-dodds, b. mann, d. amodei, n. joseph, s. mccandlish, t. brown, and j. kaplan, “constitutional ai: harmlessness from ai feedback,” 2022. l. tunstall, e. beeching, n. lambert, n. rajani, k. rasul, y. belkada, s. huang, l. von werra, c. fourrier, n. habib et al. , “zephyr: direct distillation of lm alignment,” arxiv preprint arxiv:2310.16944 , 2023. j. hong, q. tu, c. chen, x. gao, j. zhang, and r. yan, “cyclealign: iterative distillation from black-box llm to white-box models for better human alignment,” arxiv preprint arxiv:2310.16271 , 2023. h. lee, s. phatale, h. mansoor, k. lu, t. mesnard, c. bishop, v . carbune, and a. rastogi, “rlaif: scaling reinforcement learning from human feedback with ai feedback,” arxiv preprint arxiv:2309.00267 , 2023. y. jiang, c. chan, m. chen, and w. wang, “lion: adversarial distillation of closed-source large language model,” arxiv preprint arxiv:2305.12870 , 2023. h. chen, a. saha, s. hoi, and s. joty, “personalized distillation: empowering open-sourced llms with adaptive learning for code generation,” in the 2023 conference on empirical methods in natural language processing , 2023. [online]. available: https://openreview. net/forum?id=alxwmbcnvn k. yang, d. klein, a. celikyilmaz, n. peng, and y. tian, “rlcd: reinforcement learning from contrastive distilla- tion for lm alignment,” in the twelfth international confer- ence on learning representations , 2024. [online]. available: https://openreview.net/forum?id=v3xxtxwki6 j. jung, p . west, l. jiang, f. brahman, x. lu, j. fisher, t. sorensen, and y. choi, “impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing,” 2023. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, andj. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 c. gulcehre, t. l. paine, s. srinivasan, k. konyushkova, l. weerts, a. sharma, a. siddhant, a. ahern, m. wang, c. gu, w. macherey, a. doucet, o. firat, and n. de freitas, “reinforced self-training (rest) for language modeling,” 2023. e. zelikman, y. wu, j. mu, and n. d. goodman, “star: boot- strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and strapping reasoning with reasoning,” in neurips , 2022. v . sanh, l. debut, j. chaumond, and t. wolf, “distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,” arxiv preprint arxiv:1910.01108 , 2019. y. wen, z. li, w. du, and l. mou, “f-divergence minimization for sequence-level knowledge distillation,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 10 817–10 834. [online]. available: https: //aclanthology.org/2023.acl-long.605 c. liang, s. zuo, q. zhang, p . he, w. chen, and t. zhao, “less is more: task-aware layer-wise distillation for lan- guage model compression,” in international conference on machine learning . pmlr, 2023, pp. 20 852–20 867. m. kwon, s. m. xie, k. bullard, and d. sadigh, “reward de- sign with language models,” in iclr . openreview.net, 2023. b. peng, c. li, p . he, m. galley, and j. gao, “instruction tuning with gpt-4,” 2023. g. li, h. a. a. k. hammoud, h. itani, d. khizbullin, and b. ghanem, “camel: communicative agents for” mind” exploration of large scale language model society,” arxiv preprint arxiv:2303.17760 , 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, “openchat: advancing open-source language models with mixed-quality data,” sep. 2023, arxiv:2309.11235 [cs]. [online]. available: http://arxiv.org/abs/2309.11235 m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” arxiv preprint arxiv:2305.18395 , 2023. h. luo, y.-s. chuang, y. gong, t. zhang, y. kim, x. wu, d. fox, h. meng, and j. glass, “sail: search-augmented in- struction learning,” arxiv preprint arxiv:2305.15225 , 2023. a. asai, z. wu, y. wang, a. sil, and h. hajishirzi, “self- rag: learning to retrieve, generate, and critique through self-reflection,” arxiv preprint arxiv:2310.11511 , 2023. s. ye, y. jo, d. kim, s. kim, h. hwang, and m. seo, “selfee: iterative self-revising llm empowered by self-feedback generation,” blog post, may 2023. [online]. available: https://kaistai.github.io/selfee/ p . wang, l. li, l. chen, f. song, b. lin, y. cao, t. liu, and z. sui, “making large language models better reasoners with alignment,” 2023. d. cheng, s. huang, and f. wei, “adapting large language models via reading comprehension,” 2023. y. zhang, z. chen, y. fang, l. cheng, y. lu, f. li, w. zhang, 33 and h. chen, “knowledgeable preference alignment for llms in domain-specific question answering,” 2023. j. scheurer, j. a. campos, t. korbak, j. s. chan, a. chen, k. cho, and e. perez, “training language models with language feedback at scale,” 2023. s. kim, s. bae, j. shin, s. kang, d. kwak, k. yoo, and m. seo, “aligning large language models through synthetic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 677–13 700. [online]. available: https://aclanthology. org/2023.emnlp-main.844 p . roit, j. ferret, l. shani, r. aharoni, g. cideron, r. dadashi, m. geist, s. girgin, l. hussenot, o. keller, n. momchev, s. ramos garea, p . stanczyk, n. vieillard, o. bachem, g. elidan, a. hassidim, o. pietquin, and i. szpektor, “factually consistent summarization via reinforcement learning with textual entailment feedback,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 6252–6272. [online]. available: https://aclanthology.org/2023.acl-long.344 y. yang, e. chern, x. qiu, g. neubig, and p . liu, “alignment for honesty,” arxiv preprint arxiv:2312.07000 , 2023. r. liu, r. yang, c. jia, g. zhang, d. zhou, a. m. dai, d. yang, and s. vosoughi, “training socially aligned lan- guage models on simulated social interactions,” 2023. t. schick, j. dwivedi-yu, r. dess `ı, r. raileanu, m. lomeli, l. zettlemoyer, n. cancedda, and t. scialom, “tool- former: language models can teach themselves to use tools,” 2023. j. zhang, “graph-toolformer: to empower llms with graph reasoning ability via prompt augmented by chatgpt,” arxiv preprint arxiv:2304.11116 , 2023. s. g. patil, t. zhang, x. wang, and j. e. gonzalez, “gorilla: large language model connected with massive apis,” 2023. q. tang, z. deng, h. lin, x. han, q. liang, b. cao, and l. sun, “toolalpaca: generalized tool learning for lan- guage models with 3000 simulated cases,” 2023. y. qin, s. liang, y. ye, k. zhu, l. yan, y. lu, y. lin, x. cong, x. tang, b. qian, s. zhao, l. hong, r. tian, r. xie, j. zhou, m. gerstein, d. li, z. liu, and m. sun, “toolllm: facilitating large language models to master 16000+ real- world apis,” 2023. l. yuan, y. chen, x. wang, y. r. fung, h. peng, and h. ji, “craft: customizing llms by creating and retrieving from specialized toolsets,” 2023. s. gao, z. shi, m. zhu, b. fang, x. xin, p . ren, z. chen, j. ma, and z. ren, “confucius: iterative tool learning from introspection feedback by easy-to-difficult curriculum,” 2023. c. wang, w. luo, q. chen, h. mai, j. guo, s. dong, xiaohua, xuan, z. li, l. ma, and s. gao, “mllm-tool: a multimodal large language model for tool agent learning,” 2024. w. shen, c. li, h. chen, m. yan, x. quan, h. chen, j. zhang, and f. huang, “small llms are weak tool learners: a multi- llm agent,” 2024.b. chen, c. shu, e. shareghi, n. collier, k. narasimhan, and s. yao, “fireact: toward language agent fine-tuning,” 2023. a. zeng, m. liu, r. lu, b. wang, x. liu, y. dong, and j. tang, “agenttuning: enabling generalized agent abilities for llms,” 2023. d. yin, f. brahman, a. ravichander, k. chandu, k.-w. chang, y. choi, and b. y. lin, “lumos: learning agents with unified data, modular design, and open-source llms,” 2023. s. qiao, n. zhang, r. fang, y. luo, w. zhou, y. e. jiang, c. lv, and h. chen, “autoact: automatic agent learning from scratch via self-planning,” 2024. y. kong, j. ruan, y. chen, b. zhang, t. bao, s. shi, g. du, x. hu, h. mao, z. li, x. zeng, and r. zhao, “tptu-v2: boosting task planning and tool usage of large language model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” model-based agents in real-world systems,” 2023. f. gilardi, m. alizadeh, and m. kubli, “chatgpt outperforms crowd workers for text-annotation tasks,” proceedings of the national academy of sciences , vol. 120, no. 30, jul. 2023. [online]. available: http: //dx.doi.org/10.1073/pnas.2305016120 z. wang, a. w. yu, o. firat, and y. cao, “towards zero-label language learning,” 2021. y. xu, r. xu, d. iter, y. liu, s. wang, c. zhu, and m. zeng, “inheritsumm: a general, versatile and compact summarizer by distilling from gpt,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 13 879–13 892. [online]. available: https: //aclanthology.org/2023.findings-emnlp.927 f. xu, w. shi, and e. choi, “recomp: improving retrieval- augmented lms with context compression and selective augmentation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=mljlvignhp s. ramnath, b. joshi, s. hallinan, x. lu, l. h. li, a. chan, j. hessel, y. choi, and x. ren, “tailoring self-rationalizers with multi-reward distillation,” 2023. s. wang, y. liu, y. xu, c. zhu, and m. zeng, “want to reduce labeling cost? gpt-3 can help,” in findings of the association for computational linguistics: emnlp 2021 , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 4195–4205. [online]. available: https: //aclanthology.org/2021.findings-emnlp.354 z. guo, p . wang, y. wang, and s. yu, “improving small language models on pubmedqa via generative data aug- mentation,” 2023. w. yang and g. nicolai, “neural machine translation data generation and augmentation using chatgpt,” 2023. k. srinivasan, k. raman, a. samanta, l. liao, l. bertelli, and m. bendersky, “quill: query intent with large language models using retrieval augmentation and multi-stage distillation,” in proceedings of the 2022 conference on empirical methods in natural language processing: industry track , y. li and a. lazaridou, eds. abu dhabi, uae: association for computational linguistics, dec. 2022, pp. 492–501. [online]. available: 34 https://aclanthology.org/2022.emnlp-industry.50 z. dai, v . y. zhao, j. ma, y. luan, j. ni, j. lu, a. bakalov, k. guu, k. b. hall, and m. chang, “promptagator: few-shot dense retrieval from 8 examples,” in the eleventh international conference on learning representations, iclr 2023, kigali, rwanda, may 1-5, 2023 , 2023. [online]. available: https://openreview.net/pdf?id=gml46ympu2j r. meng, y. liu, s. yavuz, d. agarwal, l. tu, n. yu, j. zhang, m. bhat, and y. zhou, “augtriever: unsupervised dense retrieval by scalable data augamentation,” 2023. w. sun, l. yan, x. ma, s. wang, p . ren, z. chen, d. yin, and z. ren, “is chatgpt good at search? investigating large language models as re-ranking agents,” 2023. r. pradeep, s. sharifymoghaddam, and j. lin, “rankvicuna: zero-shot listwise document reranking with open-source large language models,” 2023. ——, “rankzephyr: effective and robust zero-shot listwise reranking is a breeze!” 2023. f. ferraretto, t. laitz, r. lotufo, and r. nogueira, “exaranker: synthetic explanations improve neural rankers,” in proceedings of the 46th international acm sigir conference on research and development in information retrieval , ser. sigir ’23. new york, ny, usa: association for computing machinery, 2023, p. 2409–2414. [online]. available: https://doi.org/10.1145/3539618.3592067 s. mysore, a. mccallum, and h. zamani, “large language model augmented narrative driven recommendations,” inproceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 777–783. [online]. available: https://doi.org/10.1145/3604915.3608829 j. zhang, r. xie, y. hou, w. x. zhao, l. lin, and j.-r. wen, “recommendation as instruction following: a large language model empowered recommendation approach,” 2023. q. liu, n. chen, t. sakai, and x.-m. wu, “once: boost- ing content-based recommendation with both open- and closed-source large language models,” 2023. s. kim, j. shin, y. cho, j. jang, s. longpre, h. lee, s. yun, s. shin, s. kim, j. thorne, and m. seo, “prometheus: inducing evaluation capability in language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https: //openreview.net/forum?id=8eujatvekw w. xu, d. wang, l. pan, z. song, m. freitag, w. wang, and l. li, “instructscore: towards explainable text generation evaluation with automatic feedback,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5967–5994. [online]. available: https://aclanthology.org/2023.emnlp-main.365 d. jiang, y. li, g. zhang, w. huang, b. y. lin, and w. chen, “tigerscore: towards building explainable metric for all text generation tasks,” 2023. j. li, s. sun, w. yuan, r.-z. fan, hai zhao, and p . liu, “generative judge for evaluating alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=gtkfw6szgsb. rozi `ere, j. gehring, f. gloeckle, s. sootla, i. gat, x. e. tan, y. adi, j. liu, t. remez, j. rapin, a. kozhevnikov, i. evtimov, j. bitton, m. bhatt, c. c. ferrer, a. grattafiori, w. xiong, a. d ´efossez, j. copet, f. azhar, h. touvron, l. martin, n. usunier, t. scialom, and g. synnaeve, “code llama: open foundation models for code,” 2023. b. liu, c. chen, c. liao, z. gong, h. wang, z. lei, m. liang, d. chen, m. shen, h. zhou, h. yu, and j. li, “mftcoder: boosting code llms with multitask fine-tuning,” 2023. n. jain, t. zhang, w. chiang, j. e. gonzalez, k. sen, and i. stoica, “llm-assisted code cleaning for training accurate code generators,” corr , vol. abs/2311.14904, 2023. [online]. available: https://doi.org/10.48550/ arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up arxiv.2311.14904 h. liu, c. li, q. wu, and y. j. lee, “visual instruction tuning,” in neurips , 2023. b. zhao, b. wu, m. he, and t. huang, “svit: scaling up visual instruction tuning,” 2023. j. wang, l. meng, z. weng, b. he, z. wu, and y.-g. jiang, “to see is to believe: prompting gpt-4v for better visual instruction tuning,” 2023. k. chen, z. zhang, w. zeng, r. zhang, f. zhu, and r. zhao, “shikra: unleashing multimodal llm’s referential dialogue magic,” 2023. j. s. park, j. hessel, k. r. chandu, p . p . liang, x. lu, p . west, y. yu, q. huang, j. gao, a. farhadi, and y. choi, “localized symbolic knowledge distillation for visual commonsense models,” 2023. r. pi, j. gao, s. diao, r. pan, h. dong, j. zhang, l. yao, j. han, h. xu, l. kong, and t. zhang, “detgpt: detect what you need via reasoning,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 14 172–14 189. [online]. available: https: //aclanthology.org/2023.emnlp-main.876 l. zhao, e. yu, z. ge, j. yang, h. wei, h. zhou, j. sun, y. peng, r. dong, c. han, and x. zhang, “chatspot: bootstrapping multimodal llms via precise referring in- struction tuning,” 2023. f. liu, k. lin, l. li, j. wang, y. yacoob, and l. wang, “mitigating hallucination in large multi-modal models via robust instruction tuning,” 2023. s. wu, h. fei, l. qu, w. ji, and t.-s. chua, “next-gpt: any- to-any multimodal llm,” 2023. r. luo, z. zhao, m. yang, j. dong, d. li, p . lu, t. wang, l. hu, m. qiu, and z. wei, “valley: video assistant with large language model enhanced ability,” 2023. y. jiang, e. schoop, a. swearngin, and j. nichols, “iluvui: instruction-tuned language-vision modeling of uis from machine conversations,” 2023. y. li, c. zhang, g. yu, z. wang, b. fu, g. lin, c. shen, l. chen, and y. wei, “stablellava: enhanced visual in- struction tuning with synthesized image-dialogue data,” 2023. r. xu, x. wang, t. wang, y. chen, j. pang, and d. lin, “pointllm: empowering large language models to under- stand point clouds,” 2023. q. huang, m. tao, z. an, c. zhang, c. jiang, z. chen, z. wu, and y. feng, “lawyer llama technical report,” arxiv preprint arxiv:2305.15062 , 2023. 35 j. cui, z. li, y. yan, b. chen, and l. yuan, “chatlaw: open- source legal large language model with integrated ex- ternal knowledge bases,” arxiv preprint arxiv:2306.16092 , 2023. h. zhang, j. chen, f. jiang, f. yu, z. chen, g. chen, j. li, x. wu, z. zhiyi, q. xiao, x. wan, b. wang, and h. li, “huatuogpt, towards taming language model to be a doctor,” in findings of the association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 10 859– 10 885. [online]. available: https://aclanthology.org/ 2023.findings-emnlp.725 j. chen, x. wang, a. gao, f. jiang, s. chen, h. zhang, d. song, w. xie, c. kong, j. li, x. wan, h. li, and b. wang, “huatuogpt-ii, one-stage training for medical adaption of llms,” corr , vol. abs/2311.09774, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09774 x. zhang and q. yang, “xuanyuan 2.0: a large chinese financial chat model with hundreds of billions parameters,” in proceedings of the 32nd acm international conference on information and knowledge management, cikm 2023, birmingham, united kingdom, october 21- 25, 2023 , i. frommholz, f. hopfgartner, m. lee, m. oakes, m. lalmas, m. zhang, and r. l. t. santos, eds. acm, 2023, pp. 4435–4439. [online]. available: https://doi.org/10.1145/3583780.3615285 t. xie, y. wan, w. huang, z. yin, y. liu, s. wang, q. linghu, c. kit, c. grazian, w. zhang, i. razzak, and b. hoex, “darwin series: domain specific large language models for natural science,” corr , vol. abs/2308.13565, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2308.13565 y. dan, z. lei, y. gu, y. li, j. yin, j. lin, l. ye, z. tie, y. zhou, y. wang, a. zhou, z. zhou, q. chen, j. zhou, l. he, and x. qiu, “educhat: a large-scale language model-based chatbot system for intelligent education,” corr , vol. abs/2308.02773, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.02773 h. guo, j. yang, j. liu, l. yang, l. chai, j. bai, j. peng, x. hu, c. chen, d. zhang, x. shi, t. zheng, l. zheng, b. zhang, k. xu, and z. li, “owl: a large language model for it operations,” corr , vol. abs/2309.09298, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.09298 y. kim and a. m. rush, “sequence-level knowledge distil- lation,” arxiv preprint arxiv:1606.07947 , 2016. s. han, h. mao, and w. j. dally, “deep compression: compressing deep neural networks with pruning, trained quantization and huffman coding,” international confer- ence on learning representations (iclr) , 2016. v . gangal, s. y. feng, m. alikhani, t. mitamura, and e. hovy, “nareor: the narrative reordering problem,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 10 645–10 653. s. longpre, y. lu, z. tu, and c. dubois, “an exploration of data augmentation and sampling techniques for domain- agnostic question answering,” in proceedings of the 2nd workshop on machine reading for question answering , a. fisch, a. talmor, r. jia, m. seo, e. choi, and d. chen, eds. hong kong, china: association for computational linguistics, nov. 2019, pp. 220–227. [online]. available:https://aclanthology.org/d19-5829 p . west, c. bhagavatula, j. hessel, j. hwang, l. jiang, r. le bras, x. lu, s. welleck, and y. choi, “symbolic knowledge distillation: from general language models to commonsense models,” in proceedings of the 2022 conference of the north american chapter of the association for computational linguistics: human language technologies , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 4602–4625. [online]. available: https://aclanthology.org/2022.naacl-main.341 z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, z. li, x. xu, t. shen, c. xu, j.-c. gu, and c. tao, “leveraging large language models for nlg evaluation: a survey,” 2024. s. li, j. chen, y. shen, z. chen, x. zhang, z. li, h. wang, j. qian, b. peng, y. mao, w. chen, and x. yan, “explana- tions from large language models make small reasoners better,” 2022. n. ho, l. schmid, and s. yun, “large language models are reasoning teachers,” in acl (1) . association for computational linguistics, 2023, pp. 14 852–14 882. l. c. magister, j. mallinson, j. adamek, e. malmi, and a. severyn, “teaching small language models to reason,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 2: short papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 1773–1781. [online]. available: https://aclanthology.org/2023.acl-short.151 y. fu, h. peng, l. ou, a. sabharwal, and t. khot, “specializ- ing smaller language models towards multi-step reason- ing,” 2023. l. h. li, j. hessel, y. yu, x. ren, k.-w. chang, and y. choi, “symbolic chain-of-thought distillation: small models can also “think” step-by-step,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2665–2679. [online]. available: https://aclanthology.org/2023.acl- long.150 w. liu, g. li, k. zhang, b. du, q. chen, x. hu, h. xu, j. chen, and j. wu, “mind’s mirror: distilling self- evaluation capability and comprehensive thinking from large language models,” 2023. s. longpre, l. hou, t. vu, a. webson, h. w. chung, y. tay, d. zhou, q. v . le, b. zoph, j. wei et al. , “the flan collec- tion: designing data and methods for effective instruction tuning,” arxiv preprint arxiv:2301.13688 , 2023. y. anand, z. nussbaum, b. duderstadt, b. schmidt, and a. mulyar, “gpt4all: training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo,” github , 2023. q. si, t. wang, z. lin, x. zhang, y. cao, and w. wang, “an empirical study of instruction-tuning large language models in chinese,” in emnlp (findings) . association for computational linguistics, 2023, pp. 4086–4107. y. ji, y. deng, y. gong, y. peng, q. niu, l. zhang, b. ma, and x. li, “exploring the impact of instruction data scaling on large language models: an empirical study on real-world use cases,” 2023. m. wu, a. waheed, c. zhang, m. abdul-mageed, and a. f. 36 aji, “lamini-lm: a diverse herd of distilled models from large-scale instructions,” 2023. w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” 2023. y. yu, y. zhuang, j. zhang, y. meng, a. ratner, r. krishna, j. shen, and c. zhang, “large language model as at- tributed training data generator: a tale of diversity and bias,” 2023. f. wan, x. huang, d. cai, x. quan, w. bi, and s. shi, “knowledge fusion of large language models,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum? id=jidsk12qcz q. zhao and b. zhu, “towards the fundamental limits of knowledge transfer over finite domains,” inneurips 2023 workshop on mathematics of modern machine learning , 2023. [online]. available: https: //openreview.net/forum?id=9qxoxqxa0n c. qin, w. xia, f. jiao, and s. joty, “improving in-context learning via bidirectional alignment,” 2023. n. boizard, k. el-haddad, c. hudelot, and p . colombo, “towards cross-tokenizer distillation: the universal logit distillation loss for llms,” arxiv preprint arxiv:2402.12030 , 2024. q. zhong, l. ding, l. shen, j. liu, b. du, and d. tao, “revis- iting knowledge distillation for autoregressive language models,” 2024. m. kim, s. lee, j. lee, s. hong, d.-s. chang, w. sung, and j. choi, “token-scaled logit distillation for ternary weight generative language models,” arxiv preprint arxiv:2308.06744 , 2023. z. chen, k. zhou, w. x. zhao, j. wan, f. zhang, d. zhang, and j.-r. wen, “improving large language models via fine- grained reinforcement learning with minimum editing constraint,” 2024. g. guo, r. zhao, t. tang, x. zhao, and j.-r. wen, “beyond imitation: leveraging fine-grained quality signals for alignment,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=lnlju5c5dk z. allen-zhu and y. li, “towards understanding ensemble, knowledge distillation and self-distillation in deep learn- ing,” arxiv preprint arxiv:2012.09816 , 2020. t. zheng, s. guo, x. qu, j. guo, w. zhang, x. du, c. lin, w. huang, w. chen, j. fu et al. , “kun: answer polish- ment for chinese self-alignment with instruction back- translation,” arxiv preprint arxiv:2401.06477 , 2024. x. li, p . yu, c. zhou, t. schick, o. levy, l. zettlemoyer, j. e. weston, and m. lewis, “self-alignment with instruction backtranslation,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=1oijhjbrst b. zhao, h. hajishirzi, and q. cao, “apt: adaptive pruning and tuning pretrained language models for efficient train- ing and inference,” arxiv preprint arxiv:2401.12200 , 2024. a. singh, j. d. co-reyes, r. agarwal, a. anand, p . patil, p . j. liu, j. harrison, j. lee, k. xu, a. parisi et al. , “beyond hu- man data: scaling self-training for problem-solving with language models,” arxiv preprint arxiv:2312.06585 , 2023. w. chen, d. song, and b. li, “grath: gradual self-truthifyingfor large language models,” 2024. a. hosseini, x. yuan, n. malkin, a. courville, a. sordoni, and r. agarwal, “v-star: training verifiers for self-taught reasoners,” 2024. a. askell, y. bai, a. chen, d. drain, d. ganguli, t. henighan, a. jones, n. joseph, b. mann, n. dassarma, n. elhage, z. hatfield-dodds, d. hernandez, j. kernion, k. ndousse, c. olsson, d. amodei, t. brown, j. clark, s. mccandlish, c. olah, and j. kaplan, “a general lan- guage assistant as a laboratory for alignment,” 2021. j. huang, s. gu, l. hou, y. wu, x. wang, h. yu, and j. han, “large language models can self-improve,” in proceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 1051–1068. [online]. available: https://aclanthology.org/2023.emnlp-main.67 h. chen, x. quan, h. chen, m. yan, and j. zhang, “knowl- edge distillation for closed-source language models,” arxiv preprint arxiv:2401.07013 , 2024. i. sason and s. verd ´u, “f-divergence inequalities,” ieee transactions on information theory , vol. 62, no. 11, pp. 5973– 6006, 2016. s. sun, y. cheng, z. gan, and j. liu, “patient knowledge distillation for bert model compression,” 2019. z. sun, h. yu, x. song, r. liu, y. yang, and d. zhou, “mobilebert: a compact task-agnostic bert for resource-limited devices,” in proceedings of the 58th annual meeting of the association for computational linguistics , d. jurafsky, j. chai, n. schluter, and j. tetreault, eds. online: association for computational linguistics, jul. 2020, pp. 2158–2170. [online]. available: https://aclanthology.org/2020.acl-main.195 x. jiao, y. yin, l. shang, x. jiang, x. chen, l. li, f. wang, and q. liu, “tinybert: distilling bert for natural language understanding,” in findings of the association for computational linguistics: emnlp 2020 , t. cohn, y. he, and y. liu, eds. online: association for computational linguistics, nov. 2020, pp. 4163–4174. [online]. available: https://aclanthology.org/2020.findings-emnlp.372 l. hou, z. huang, l. shang, x. jiang, x. chen, and q. liu, “dynabert: dynamic bert with adaptive width and depth,” advances in neural information processing systems , vol. 33, pp. 9782–9793, 2020. s. zuo, q. zhang, c. liang, p . he, t. zhao, and w. chen, “moebert: from bert to mixture-of-experts via importance- guided adaptation,” arxiv preprint arxiv:2204.07675 , 2022. k. j. liang, w. hao, d. shen, y. zhou, w. chen, c. chen, and l. carin, “mixkd: towards efficient distillation of large- scale language models,” in 9th international conference on learning representations, iclr 2021, virtual event, austria, may 3-7, 2021 . openreview.net, 2021. [online]. available: https://openreview.net/forum?id=ufgeeljklu5 y. j. ma, w. liang, g. wang, d.-a. huang, o. bastani, d. ja- yaraman, y. zhu, l. fan, and a. anandkumar, “eureka: human-level reward design via coding large language models,” 2023. j.-c. pang, p . wang, k. li, x.-h. chen, j. xu, z. zhang, and y. yu, “language model self-improvement by reinforce- ment learning contemplation,” 2023. y. du, o. watkins, z. wang, c. colas, t. darrell, p . abbeel, 37 a. gupta, and j. andreas, “guiding pretraining in reinforcement learning with large language models,” inproceedings of the 40th international conference on machine learning , ser. proceedings of machine learning research, a. krause, e. brunskill, k. cho, b. engelhardt, s. sabato, and j. scarlett, eds., vol. 202. pmlr, 23–29 jul 2023, pp. 8657–8677. [online]. available: https://proceedings.mlr.press/v202/du23f.html j. schulman, f. wolski, p . dhariwal, a. radford, and o. klimov, “proximal policy optimization algorithms,” 2017. r. rafailov, a. sharma, e. mitchell, s. ermon, c. d. man- ning, and c. finn, “direct preference optimization: your language model is secretly a reward model,” 2023. f. song, b. yu, m. li, h. yu, f. huang, y. li, and h. wang, “preference ranking optimization for human alignment,” arxiv preprint arxiv:2306.17492 , 2023. z. yuan, h. yuan, c. tan, w. wang, s. huang, and f. huang, “rrhf: rank responses to align language mod- els with human feedback without tears,” arxiv preprint arxiv:2304.05302 , 2023. m. li, l. chen, j. chen, s. he, and t. zhou, “reflection-tuning: recycling data for better instruction- tuning,” in neurips 2023 workshop on instruction tuning and instruction following , 2023. [online]. available: https://openreview.net/forum?id=xaqozzqkpu m. li, l. chen, j. chen, s. he, j. gu, and t. zhou, “selective reflection-tuning: student-selected data recycling for llm instruction-tuning,” 2024. [online]. available: https: //api.semanticscholar.org/corpusid:267682220 x. geng, a. gudibande, h. liu, e. wallace, p . abbeel, s. levine, and d. song, “koala: a dialogue model for academic research,” blog post, april 2023. [online]. available: https://bair.berkeley.edu/blog/2023/04/03/ koala/ m. li, j. chen, l. chen, and t. zhou, “can llms speak for diverse people? tuning llms via debate to generate controllable controversial statements,” 2024. m. kang, s. lee, j. baek, k. kawaguchi, and s. j. hwang, “knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks,” 2023. r. yang, l. song, y. li, s. zhao, y. ge, x. li, and y. shan, “gpt4tools: teaching large language model to use tools via self-instruction,” 2023. a. yehudai, b. carmeli, y. mass, o. arviv, n. mills, a. toledo, e. shnarch, and l. choshen, “genie: achieving human parity in content-grounded datasets generation,” 2024. y. zhang, r. zhang, j. gu, y. zhou, n. lipka, d. yang, and t. sun, “llavar: enhanced visual instruction tuning for text-rich image understanding,” 2023. c. lyu, m. wu, l. wang, x. huang, b. liu, z. du, s. shi, and z. tu, “macaw-llm: multi-modal language modeling with image, audio, video, and text integration,” arxiv preprint arxiv:2306.09093 , 2023. b. li, y. zhang, l. chen, j. wang, f. pu, j. yang, c. li, and z. liu, “mimic-it: multi-modal in-context instruction tuning,” 2023. z. zhao, l. guo, t. yue, s. chen, s. shao, x. zhu, z. yuan, and j. liu, “chatbridge: bridging modalities with large language model as a language catalyst,” 2023.y. zhao, b. yu, b. hui, h. yu, f. huang, y. li, and n. l. zhang, “a preliminary study of the intrinsic relationship between complexity and alignment,” 2023. a. gudibande, e. wallace, c. snell, x. geng, h. liu, p . abbeel, s. levine, and d. song, “the false promise of imitating proprietary llms,” arxiv preprint arxiv:2305.15717 , 2023. c. zhou, p . liu, p . xu, s. iyer, j. sun, y. mao, x. ma, a. efrat, p . yu, l. yu, s. zhang, g. ghosh, m. lewis, l. zettlemoyer, and o. levy, “lima: less is more for alignment,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview.net/forum?id=kbmokmx2he m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to-strong data filtering for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and for fast instruction-tuning,” 2024. [online]. available: https://api.semanticscholar.org/corpusid:267365346 b. xu, a. yang, j. lin, q. wang, c. zhou, y. zhang, and z. mao, “expertprompting: instructing large language models to be distinguished experts,” 2023. w. liu, w. zeng, k. he, y. jiang, and j. he, “what makes good data for alignment? a comprehensive study of auto- matic data selection in instruction tuning,” 2023. r. lou, k. zhang, j. xie, y. sun, j. ahn, h. xu, y. su, and w. yin, “muffin: curating multi-faceted instructions for improving instruction-following,” 2023. t. schick, j. dwivedi-yu, z. jiang, f. petroni, p . lewis, g. izacard, q. you, c. nalmpantis, e. grave, and s. riedel, “peer: a collaborative language model,” 2022. a. madaan, n. tandon, p . gupta, s. hallinan, l. gao, s. wiegreffe, u. alon, n. dziri, s. prabhumoye, y. yang, s. gupta, b. p . majumder, k. hermann, s. welleck, a. yaz- danbakhsh, and p . clark, “self-refine: iterative refinement with self-feedback,” 2023. w. saunders, c. yeh, j. wu, s. bills, l. ouyang, j. ward, and j. leike, “self-critiquing models for assisting human evaluators,” 2022. d. m. ziegler, n. stiennon, j. wu, t. b. brown, a. radford, d. amodei, p . christiano, and g. irving, “fine-tuning language models from human preferences,” arxiv preprint arxiv:1909.08593 , 2019. n. stiennon, l. ouyang, j. wu, d. ziegler, r. lowe, c. voss, a. radford, d. amodei, and p . f. christiano, “learning to summarize with human feedback,” advances in neu- ral information processing systems , vol. 33, pp. 3008–3021, 2020. j. wu, l. ouyang, d. m. ziegler, n. stiennon, r. lowe, j. leike, and p . christiano, “recursively summarizing books with human feedback,” 2021. y. bai, a. jones, k. ndousse, a. askell, a. chen, n. das- sarma, d. drain, s. fort, d. ganguli, t. henighan et al. , “training a helpful and harmless assistant with rein- forcement learning from human feedback,” arxiv preprint arxiv:2204.05862 , 2022. a. k ¨opf, y. kilcher, d. von r ¨utte, s. anagnostidis, z.-r. tam, k. stevens, a. barhoum, n. m. duc, o. stanley, r. nagyfi, s. es, s. suri, d. glushkov, a. dantuluri, a. maguire, c. schuhmann, h. nguyen, and a. mattick, “openassis- tant conversations – democratizing large language model alignment,” 2023. g. wang, s. cheng, x. zhan, x. li, s. song, and y. liu, 38 “openchat: advancing open-source language models with mixed-quality data,” 2023. l. weidinger, j. mellor, m. rauh, c. griffin, j. uesato, p .- s. huang, m. cheng, m. glaese, b. balle, a. kasirzadeh, z. kenton, s. brown, w. hawkins, t. stepleton, c. biles, a. birhane, j. haas, l. rimell, l. a. hendricks, w. isaac, s. legassick, g. irving, and i. gabriel, “ethical and social risks of harm from language models,” 2021. j. ji, m. liu, j. dai, x. pan, c. zhang, c. bian, c. zhang, r. sun, y. wang, and y. yang, “beavertails: towards improved safety alignment of llm via a human-preference dataset,” 2023. i. solaiman and c. dennison, “process for adapting lan- guage models to society (palms) with values-targeted datasets,” advances in neural information processing sys- tems , vol. 34, pp. 5861–5873, 2021. l. qiu, y. zhao, j. li, p . lu, b. peng, j. gao, and s.-c. zhu, “valuenet: a new dataset for human value driven dialogue system,” in proceedings of the aaai conference on artificial intelligence , vol. 36, no. 10, 2022, pp. 11 183– 11 191. j. kiesel, m. alshomary, n. handke, x. cai, h. wachsmuth, and b. stein, “identifying the human values behind arguments,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4459–4471. [online]. available: https://aclanthology.org/2022.acl-long.306 r. liu, g. zhang, x. feng, and s. vosoughi, “aligning generative language models with human values,” in findings of the association for computational linguistics: naacl 2022 , m. carpuat, m.-c. de marneffe, and i. v . meza ruiz, eds. seattle, united states: association for computational linguistics, jul. 2022, pp. 241– 252. [online]. available: https://aclanthology.org/2022. findings-naacl.18 a. glaese, n. mcaleese, m. trebacz, j. aslanides, v . firoiu, t. ewalds, m. rauh, l. weidinger, m. chadwick, p . thacker et al. , “improving alignment of dialogue agents via targeted human judgements,” arxiv preprint arxiv:2209.14375 , 2022. h. sun, z. zhang, f. mi, y. wang, w. liu, j. cui, b. wang, q. liu, and m. huang, “moraldial: a framework to train and evaluate moral dialogue systems via moral discussions,” in proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 2213–2230. [online]. available: https://aclanthology.org/2023.acl-long.123 j. yao, x. yi, x. wang, j. wang, and x. xie, “from instructions to intrinsic human values – a survey of alignment goals for big models,” 2023. y. liu, y. yao, j.-f. ton, x. zhang, r. g. h. cheng, y. klochkov, m. f. taufiq, and h. li, “trustworthy llms: a survey and guideline for evaluating large language models’ alignment,” arxiv preprint arxiv:2308.05374 , 2023. j. qian, h. wang, z. li, s. li, and x. yan, “limitations of language models in arithmetic and symbolic induction,” 2022.x. she, y. liu, y. zhao, y. he, l. li, c. tantithamthavorn, z. qin, and h. wang, “pitfalls in language models for code intelligence: a taxonomy and survey,” 2023. h. manikandan, y. jiang, and j. z. kolter, “language models are weak learners,” 2023. y. liang, c. wu, t. song, w. wu, y. xia, y. liu, y. ou, s. lu, l. ji, s. mao, y. wang, l. shou, m. gong, and n. duan, “taskmatrix.ai: completing tasks by connecting foundation models with millions of apis,” 2023. g. mialon, r. dess `ı, m. lomeli, c. nalmpantis, r. pa- sunuru, r. raileanu, b. rozi `ere, t. schick, j. dwivedi- yu, a. celikyilmaz, e. grave, y. lecun, and t. scialom, “augmented language models: a survey,” 2023. a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, a. parisi, y. zhao, and n. fiedel, “talm: tool augmented language models,” 2022. r. nakano, j. hilton, s. balaji, j. wu, l. ouyang, c. kim, c. hesse, s. jain, v . kosaraju, w. saunders, x. jiang, k. cobbe, t. eloundou, g. krueger, k. button, m. knight, b. chess, and j. schulman, “webgpt: browser-assisted question-answering with human feedback,” 2022. y. qin, z. cai, d. jin, l. yan, s. liang, k. zhu, y. lin, x. han, n. ding, h. wang, r. xie, f. qi, z. liu, m. sun, and j. zhou, “webcpm: interactive web search for chinese long-form question answering,” inproceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers) , a. rogers, j. boyd-graber, and n. okazaki, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 8968–8988. [online]. available: https://aclanthology.org/2023.acl-long.499 y. song, w. xiong, d. zhu, w. wu, h. qian, m. song, h. huang, c. li, k. wang, r. yao, y. tian, and s. li, “restgpt: connecting large language models with real- world restful apis,” 2023. t. cai, x. wang, t. ma, x. chen, and d. zhou, “large language models as tool makers,” 2023. y. shen, k. song, x. tan, d. li, w. lu, and y. zhuang, “hugginggpt: solving ai tasks with chatgpt and its friends in hugging face,” 2023. s. hao, t. liu, z. wang, and z. hu, “toolkengpt: augment- ing frozen language models with massive tools via tool embeddings,” 2024. s. yuan, k. song, j. chen, x. tan, y. shen, r. kan, d. li, and d. yang, “easytool: enhancing llm-based agents with concise tool instruction,” 2024. s. zhang, s. roller, n. goyal, m. artetxe, m. chen, s. chen, c. dewan, m. diab, x. li, x. v . lin, t. mihaylov, m. ott, s. shleifer, k. shuster, d. simig, p . s. koura, a. sridhar, t. wang, and l. zettlemoyer, “opt: open pre-trained transformer language models,” 2022. t. brown, b. mann, n. ryder, m. subbiah, j. d. ka- plan, p . dhariwal, a. neelakantan, p . shyam, g. sastry, a. askell et al. , “language models are few-shot learners,” advances in neural information processing systems , vol. 33, pp. 1877–1901, 2020. w. huang, p . abbeel, d. pathak, and i. mordatch, “lan- guage models as zero-shot planners: extracting actionable knowledge for embodied agents,” in international confer- ence on machine learning . pmlr, 2022, pp. 9118–9147. i. singh, v . blukis, a. mousavian, a. goyal, d. xu, j. trem- blay, d. fox, j. thomason, and a. garg, “progprompt: 39 generating situated robot task plans using large language models,” 2022. d. zhou, n. sch ¨arli, l. hou, j. wei, n. scales, x. wang, d. schuurmans, c. cui, o. bousquet, q. le, and e. chi, “least-to-most prompting enables complex reasoning in large language models,” 2023. c. h. song, j. wu, c. washington, b. m. sadler, w.-l. chao, and y. su, “llm-planner: few-shot grounded planning for embodied agents with large language models,” in proceed- ings of the ieee/cvf international conference on computer vision , 2023, pp. 2998–3009. z. wang, s. cai, a. liu, x. ma, and y. liang, “describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents,” arxiv preprint arxiv:2302.01560 , 2023. s. yao, d. yu, j. zhao, i. shafran, t. l. griffiths, y. cao, and k. narasimhan, “tree of thoughts: deliberate prob- lem solving with large language models,” arxiv preprint arxiv:2305.10601 , 2023. b. liu, y. jiang, x. zhang, q. liu, s. zhang, j. biswas, and p . stone, “llm+ p: empowering large language mod- els with optimal planning proficiency,” arxiv preprint arxiv:2304.11477 , 2023. s. hao, y. gu, h. ma, j. j. hong, z. wang, d. z. wang, and z. hu, “reasoning with language model is planning with world model,” arxiv preprint arxiv:2305.14992 , 2023. m. hu, y. mu, x. yu, m. ding, s. wu, w. shao, q. chen, b. wang, y. qiao, and p . luo, “tree-planner: efficient close-loop task planning with large language models,” arxiv preprint arxiv:2310.08582 , 2023. b. y. lin, c. huang, q. liu, w. gu, s. sommerer, and x. ren, “on grounded planning for embodied tasks with language models,” in proceedings of the aaai conference on artificial intelligence , vol. 37, no. 11, 2023, pp. 13 192– 13 200. k. valmeekam, m. marquez, s. sreedharan, and s. kambhampati, “on the planning abilities of large language models - a critical investigation,” in thirty-seventh conference on neural informa- tion processing systems , 2023. [online]. available: https://openreview.net/forum?id=x6deqxisew t. sumers, k. marino, a. ahuja, r. fergus, and i. dasgupta, “distilling internet-scale vision-language models into em- bodied agents,” in proceedings of the 40th international conference on machine learning , ser. icml’23. jmlr.org, 2023. y. yang, t. zhou, k. li, d. tao, l. li, l. shen, x. he, j. jiang, and y. shi, “embodied multi-modal agent trained by an llm from a parallel textworld,” 2023. a. vaswani, n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez, ł. kaiser, and i. polosukhin, “attention is all you need,” advances in neural information processing systems , vol. 30, 2017. y. liu, m. ott, n. goyal, j. du, m. joshi, d. chen, o. levy, m. lewis, l. zettlemoyer, and v . stoyanov, “roberta: a robustly optimized bert pretraining approach,” 2019. j. li, l. gui, y. zhou, d. west, c. aloisi, and y. he, “dis- tilling chatgpt for explainable automated student answer assessment,” in emnlp (findings) . association for com- putational linguistics, 2023, pp. 6007–6026. r. tang, x. han, x. jiang, and x. hu, “does syntheticdata generation of llms help clinical text mining?” arxiv preprint arxiv:2303.04360 , 2023. x. he, i. nassar, j. kiros, g. haffari, and m. norouzi, “generate, annotate, and learn: nlp with synthetic text,” trans. assoc. comput. linguistics , vol. 10, pp. 826–842, 2022. [online]. available: https://transacl.org/ojs/index. php/tacl/article/view/3811 y. meng, j. huang, y. zhang, and j. han, “generating training data with language models: towards zero-shot language understanding,” in advances in neural information processing systems 35: annual conference on neural information processing systems 2022, neurips 2022, new orleans, la, usa, november 28 - december 9, 2022 , 2022. [online]. available: http://papers.nips.cc/paper files/ paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms paper/2022/hash/0346c148ba1c21c6b4780a961ea141dc- abstract-conference.html j. wang, z. yao, a. mitra, s. osebe, z. yang, and h. yu, “umass bionlp at mediqa-chat 2023: can llms generate high-quality synthetic note-oriented doctor- patient conversations?” in proceedings of the 5th clinical natural language processing workshop , t. naumann, a. ben abacha, s. bethard, k. roberts, and a. rumshisky, eds. toronto, canada: association for computational linguistics, jul. 2023, pp. 460–471. [online]. available: https://aclanthology.org/2023.clinicalnlp-1.49 z. yang, s. cherian, and s. vucetic, “data augmentation for radiology report simplification,” in findings of the association for computational linguistics: eacl 2023 , a. vlachos and i. augenstein, eds. dubrovnik, croatia: association for computational linguistics, may 2023, pp. 1922–1932. [online]. available: https: //aclanthology.org/2023.findings-eacl.144 z. cai, c. tao, t. shen, c. xu, x. geng, x. a. lin, l. he, and d. jiang, “hyper: multitask hyper-prompted training en- ables large-scale retrieval generalization,” in the eleventh international conference on learning representations , 2022. c. liu, c. tao, x. geng, t. shen, d. zhao, c. xu, b. jiao, and d. jiang, “adam: dense retrieval distillation with adaptive dark examples,” arxiv preprint arxiv:2212.10192 , 2022. j. feng, c. tao, x. geng, t. shen, c. xu, g. long, d. zhao, and d. jiang, “knowledge refinement via interaction be- tween search engines and large language models,” arxiv preprint arxiv:2305.07402 , 2023. t. shen, g. long, x. geng, c. tao, t. zhou, and d. jiang, “large language models are strong zero-shot retriever,” arxiv preprint arxiv:2304.14233 , 2023. x. ma, x. zhang, r. pradeep, and j. lin, “zero-shot listwise document reranking with a large language model,” 2023. z. qin, r. jagerman, k. hui, h. zhuang, j. wu, j. shen, t. liu, j. liu, d. metzler, x. wang, and m. bendersky, “large language models are effective text rankers with pairwise ranking prompting,” 2023. x. ma, y. gong, p . he, h. zhao, and n. duan, “query rewriting in retrieval-augmented large language models,” inproceedings of the 2023 conference on empirical methods in natural language processing , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 5303–5315. [online]. available: https://aclanthology.org/2023.emnlp-main.322 40 d. sachan, m. lewis, m. joshi, a. aghajanyan, w.- t. yih, j. pineau, and l. zettlemoyer, “improving passage retrieval with zero-shot question generation,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 3781–3797. [online]. available: https://aclanthology.org/2022.emnlp-main.249 d. s. sachan, m. lewis, d. yogatama, l. zettlemoyer, j. pineau, and m. zaheer, “questions are all you need to train a dense passage retriever,” transactions of the association for computational linguistics , vol. 11, pp. 600–616, 2023. [online]. available: https://aclanthology. org/2023.tacl-1.35 t. schick and h. sch ¨utze, “generating datasets with pretrained language models,” in proceedings of the 2021 conference on empirical methods in natural language processing , m.-f. moens, x. huang, l. specia, and s. w.-t. yih, eds. online and punta cana, dominican republic: association for computational linguistics, nov. 2021, pp. 6943–6951. [online]. available: https: //aclanthology.org/2021.emnlp-main.555 z. peng, x. wu, and y. fang, “soft prompt tuning for augmenting dense retrieval with large language models,” arxiv preprint arxiv:2307.08303 , 2023. j. saad-falcon, o. khattab, k. santhanam, r. florian, m. franz, s. roukos, a. sil, m. a. sultan, and c. potts, “udapdr: unsupervised domain adaptation via llm prompting and distillation of rerankers,” in proceedings of the 2023 conference on empirical methods in natural language processing, emnlp 2023, singapore, december 6-10, 2023 , 2023, pp. 11 265–11 279. [online]. available: https://aclanthology.org/2023.emnlp-main.693 v . jeronymo, l. bonifacio, h. abonizio, m. fadaee, r. lotufo, j. zavrel, and r. nogueira, “inpars-v2: large language models as efficient dataset generators for infor- mation retrieval,” arxiv preprint arxiv:2301.01820 , 2023. w. sun, z. chen, x. ma, l. yan, s. wang, p . ren, z. chen, d. yin, and z. ren, “instruction distillation makes large language models efficient zero-shot rankers,” 2023. c. raffel, n. shazeer, a. roberts, k. lee, s. narang, m. matena, y. zhou, w. li, and p . j. liu, “exploring the limits of transfer learning with a unified text-to-text transformer,” j. mach. learn. res. , vol. 21, no. 1, jan 2020. s. bruch, x. wang, m. bendersky, and m. najork, “an analysis of the softmax cross entropy loss for learning- to-rank with binary relevance,” in proceedings of the 2019 acm sigir international conference on theory of information retrieval, ictir 2019, santa clara, ca, usa, october 2-5, 2019 , 2019, pp. 75–78. [online]. available: https://doi.org/10.1145/3341981.3344221 c. burges, t. shaked, e. renshaw, a. lazier, m. deeds, n. hamilton, and g. hullender, “learning to rank using gradient descent,” in proceedings of the 22nd international conference on machine learning , ser. icml ’05. new york, ny, usa: association for computing machinery, 2005, p. 89–96. [online]. available: https: //doi.org/10.1145/1102351.1102363 x. wang, c. li, n. golbandi, m. bendersky, and m. najork, “the lambdaloss framework for ranking metricoptimization,” in proceedings of the 27th acm international conference on information and knowledge management , ser. cikm ’18. new york, ny, usa: association for computing machinery, 2018, p. 1313–1322. [online]. available: https://doi.org/10.1145/3269206.3271784 w. wang, x. lin, f. feng, x. he, and t.-s. chua, “generative recommendation: towards next-generation recommender paradigm,” 2023. s. dai, n. shao, h. zhao, w. yu, z. si, c. xu, z. sun, x. zhang, and j. xu, “uncovering chatgpt’s capabilities in recommender systems,” in proceedings of the 17th acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 acm conference on recommender systems , ser. recsys ’23. new york, ny, usa: association for computing machinery, 2023, p. 1126–1132. [online]. available: https://doi.org/10.1145/3604915.3610646 y. xi, w. liu, j. lin, x. cai, h. zhu, j. zhu, b. chen, r. tang, w. zhang, r. zhang, and y. yu, “towards open- world recommendation with knowledge augmentation from large language models,” 2023. x. ren, w. wei, l. xia, l. su, s. cheng, j. wang, d. yin, and c. huang, “representation learning with large language models for recommendation,” 2023. w. wei, x. ren, j. tang, q. wang, l. su, s. cheng, j. wang, d. yin, and c. huang, “llmrec: large language models with graph augmentation for recommendation,” 2024. l. wang, s. zhang, y. wang, e.-p . lim, and y. wang, “llm4vis: explainable visualization recommendation using chatgpt,” in proceedings of the 2023 conference on empirical methods in natural language processing: industry track , m. wang and i. zitouni, eds. singapore: association for computational linguistics, dec. 2023, pp. 675–692. [online]. available: https://aclanthology.org/ 2023.emnlp-industry.64 z. cui, j. ma, c. zhou, j. zhou, and h. yang, “m6-rec: generative pretrained language models are open-ended recommender systems,” 2022. p . liu, l. zhang, and j. a. gulla, “pre-train, prompt and recommendation: a comprehensive survey of language modelling paradigm adaptations in recommender sys- tems,” 2023. k. papineni, s. roukos, t. ward, and w.-j. zhu, “bleu: a method for automatic evaluation of machine translation,” inproceedings of the 40th annual meeting on association for computational linguistics , ser. acl ’02. usa: association for computational linguistics, 2002, p. 311–318. [online]. available: https://doi.org/10.3115/1073083.1073135 c.-y. lin, “rouge: a package for automatic evaluation of summaries,” in text summarization branches out . barcelona, spain: association for computational linguistics, jul. 2004, pp. 74–81. [online]. available: https://aclanthology.org/w04-1013 c. su and c. mcmillan, “distilled gpt for source code summarization,” corr , vol. abs/2308.14731, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308. 14731 w. guo, j. yang, k. yang, x. li, z. rao, y. xu, and d. niu, “instruction fusion: advancing prompt evolution through hybridization,” corr , vol. abs/2312.15692, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.15692 o. sener and s. savarese, “active learning for convolutional neural networks: a core-set approach,” in 6th international 41 conference on learning representations, iclr 2018, vancouver, bc, canada, april 30 - may 3, 2018, conference track proceedings , 2018. [online]. available: https://openreview.net/forum?id=h1aiuk-rw h. liu, c. li, y. li, and y. j. lee, “improved baselines with visual instruction tuning,” 2023. s. zhang, p . sun, s. chen, m. xiao, w. shao, w. zhang, y. liu, k. chen, and p . luo, “gpt4roi: instruction tuning large language model on region-of-interest,” 2023. openai, “gpt-4v(ision) system card,” 2023. [online]. available: https://api.semanticscholar.org/corpusid: 263218031 b. a. plummer, l. wang, c. m. cervantes, j. c. caicedo, j. hockenmaier, and s. lazebnik, “flickr30k entities: collecting region-to-phrase correspondences for richer image-to-sentence models,” in proceedings of the ieee in- ternational conference on computer vision , 2015, pp. 2641– 2649. l. li, z. xie, m. li, s. chen, p . wang, l. chen, y. yang, b. wang, and l. kong, “silkie: preference distilla- tion for large visual language models,” arxiv preprint arxiv:2312.10665 , 2023. h. ha, p . florence, and s. song, “scaling up and distilling down: language-guided robot skill acquisition,” in con- ference on robot learning . pmlr, 2023, pp. 3766–3777. s. wu, z. liu, z. zhang, z. chen, w. deng, w. zhang, j. yang, z. yao, y. lyu, x. xin, s. gao, p . ren, z. ren, and z. chen, “fuzi.mingcha,” https://github.com/irlab- sdu/fuzi.mingcha, 2023. h. xiong, s. wang, y. zhu, z. zhao, y. liu, q. wang, and d. shen, “doctorglm: fine-tuning your chinese doctor is not a herculean task,” arxiv preprint arxiv:2304.01097 , 2023. x. zhang, c. tian, x. yang, l. chen, z. li, and l. r. pet- zold, “alpacare: instruction-tuned large language models for medical application,” arxiv preprint arxiv:2310.14558 , 2023. y. li, z. li, k. zhang, r. dan, s. jiang, and y. zhang, “chatdoctor: a medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge,” cureus , vol. 15, no. 6, 2023. t. han, l. c. adams, j. papaioannou, p . grundmann, t. oberhauser, a. l ¨oser, d. truhn, and k. k. bressem, “medalpaca - an open-source collection of medical conversational ai models and training data,” corr , vol. abs/2304.08247, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.08247 c. wu, w. lin, x. zhang, y. zhang, y. wang, and w. xie, “pmc-llama: towards building open-source language models for medicine,” arxiv preprint arxiv:2305.10415 , vol. 6, 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consultation,” corr , vol. abs/2308.14346, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.14346 z. gou, z. shao, y. gong, yelong shen, y. yang, m. huang, n. duan, and w. chen, “tora: a tool- integrated reasoning agent for mathematical problem solving,” in the twelfth international conference on learning representations , 2024. [online]. available: https://openreview.net/forum?id=ep0ttjvoap e. perkowski, r. pan, t. d. nguyen, y. ting, s. kruk, t. zhang, c. o’neill, m. jablonska, z. sun, m. j. smith, h. liu, k. schawinski, k. iyer, i. ciuca, and universetbd, “astrollama-chat: scaling astrollama with conversational and diverse datasets,” corr , vol. abs/2401.01916, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2401.01916 j. gao, r. pi, j. zhang, j. ye, w. zhong, y. wang, l. hong, j. han, h. xu, z. li, and l. kong, “g-llava: solving geometric problem with multi-modal large language model,” corr , vol. abs/2312.11370, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.11370 h. zhao, s. liu, c. ma, h. xu, j. fu, z.-h. deng, l. kong, and q. liu, “gimlet: a unified graph-text model for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv for instruction-based molecule zero-shot learning,” in thirty-seventh conference on neural information processing systems , 2023. [online]. available: https://openreview. net/forum?id=tt6drrcgjv a. n. rubungo, c. arnold, b. p . rand, and a. b. dieng, “llm-prop: predicting physical and electronic properties of crystalline solids from their text descriptions,” corr , vol. abs/2310.14029, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.14029 h. cao, z. liu, x. lu, y. yao, and y. li, “instructmol: multi-modal integration for building a versatile and reliable molecular assistant in drug discovery,” corr , vol. abs/2311.16208, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2311.16208 h. abdine, m. chatzianastasis, c. bouyioukos, and m. vazirgiannis, “prot2text: multimodal protein’s function generation with gnns and transform- ers,” in deep generative models for health workshop neurips 2023 , 2023. [online]. available: https://openreview.net/forum?id=ej7yngwyfj y. luo, j. zhang, s. fan, k. yang, y. wu, m. qiao, and z. nie, “biomedgpt: open multimodal generative pre-trained transformer for biomedicine,” arxiv preprint arxiv:2308.09442 , 2023. b. chen, x. cheng, p . li, y. geng, j. gong, s. li, z. bei, x. tan, b. wang, x. zeng, c. liu, a. zeng, y. dong, j. tang, and l. song, “xtrimopglm: unified 100b-scale pre-trained transformer for deciphering the language of protein,” corr , vol. abs/2401.06199, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.06199 c. deng, t. zhang, z. he, y. xu, q. chen, y. shi, l. fu, w. zhang, x. wang, c. zhou, z. lin, and j. he, “k2: a foundation language model for geoscience knowledge understanding and utilization,” 2023. z. bi, n. zhang, y. xue, y. ou, d. ji, g. zheng, and h. chen, “oceangpt: a large language model for ocean science tasks,” corr , vol. abs/2310.02031, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.02031 z. zheng, j. zhang, t. vu, s. diao, y. h. w. tim, and s. yeung, “marinegpt: unlocking secrets of ocean to the public,” corr , vol. abs/2310.13596, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.13596 z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: 42 a scientific large language model in geoscience,” corr , vol. abs/2401.00434, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.00434 d. zhang, a. petrova, d. trautmann, and f. schilder, “un- leashing the power of large language models for legal applications,” in proceedings of the 32nd acm international conference on information and knowledge management , 2023, pp. 5257–5258. z. sun, “a short survey of viewing large language models in legal aspect,” arxiv preprint arxiv:2303.09136 , 2023. j. lai, w. gan, j. wu, z. qi, and p . s. yu, “large language models in law: a survey,” arxiv preprint arxiv:2312.03718 , 2023. s. yue, w. chen, s. wang, b. li, c. shen, s. liu, y. zhou, y. xiao, s. yun, w. lin et al. , “disc-lawllm: fine-tuning large language models for intelligent legal services,” arxiv preprint arxiv:2309.11325 , 2023. h. zhong, c. xiao, c. tu, t. zhang, z. liu, and m. sun, “jec-qa: a legal-domain question answering dataset,” in proceedings of the aaai conference on artificial intelligence , vol. 34, no. 05, 2020, pp. 9701–9708. k. singhal, t. tu, j. gottweis, r. sayres, e. wulczyn, l. hou, k. clark, s. pfohl, h. cole-lewis, d. neal, m. schaekermann, a. wang, m. amin, s. lachgar, p . a. mansfield, s. prakash, b. green, e. dominowska, b. a. y arcas, n. tomasev, y. liu, r. wong, c. semturs, s. s. mahdavi, j. k. barral, d. r. webster, g. s. corrado, y. matias, s. azizi, a. karthikesalingam, and v . natarajan, “towards expert-level medical question answering with large language models,” corr , vol. abs/2305.09617, 2023. [online]. available: https://doi. org/10.48550/arxiv.2305.09617 x. yang, j. gao, w. xue, and e. alexandersson, “pllama: an open-source large language model for plant science,” corr , vol. abs/2401.01600, 2024. [online]. available: https://doi.org/10.48550/arxiv.2401.01600 x. wang, g. h. chen, d. song, z. zhang, z. chen, q. xiao, f. jiang, j. li, x. wan, b. wang et al. , “cmb: a compre- hensive medical benchmark in chinese,” arxiv preprint arxiv:2308.08833 , 2023. w. zhu, x. wang, h. zheng, m. chen, and b. tang, “promptcblue: a chinese prompt tuning benchmark for the medical domain,” arxiv preprint arxiv:2310.14151 , 2023. z. bao, w. chen, s. xiao, k. ren, j. wu, c. zhong, j. peng, x. huang, and z. wei, “disc-medllm: bridging general large language models and real-world medical consulta- tion,” arxiv preprint arxiv:2308.14346 , 2023. c. wu, x. zhang, y. zhang, y. wang, and w. xie, “pmc- llama: further finetuning llama on medical papers,” corr , vol. abs/2304.14454, 2023. [online]. available: https://doi.org/10.48550/arxiv.2304.14454 s. xue, f. zhou, y. xu, h. zhao, s. xie, q. dai, c. jiang, j. zhang, j. zhou, d. xiu, and h. mei, “weaverbird: empowering financial decision-making with large language model, knowledge base, and search engine,” corr , vol. abs/2308.05361, 2023. [online]. available: https://doi.org/10.48550/arxiv.2308.05361 s. wu, o. irsoy, s. lu, v . dabravolski, m. dredze, s. gehrmann, p . kambadur, d. s. rosenberg, and g. mann, “bloomberggpt: a large language modelfor finance,” corr , vol. abs/2303.17564, 2023. [online]. available: https://doi.org/10.48550/arxiv.2303.17564 d. lu, h. wu, j. liang, y. xu, q. he, y. geng, m. han, y. xin, and y. xiao, “bbt-fin: comprehensive construction of chinese financial domain pre-trained language model, corpus and benchmark,” corr , vol. abs/2302.09432, 2023. [online]. available: https://doi.org/10.48550/arxiv.2302. 09432 y. yang, y. tang, and k. y. tam, “investlm: a large language model for investment using financial domain instruction tuning,” corr , vol. abs/2309.13064, 2023. [online]. available: https://doi.org/10.48550/arxiv.2309.13064 q. xie, w. han, x. zhang, y. lai, m. peng, a. lopez- lira, and j. huang, “pixiu: a large language model, instruction data and evaluation benchmark for finance,” corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction corr , vol. abs/2306.05443, 2023. [online]. available: https://doi.org/10.48550/arxiv.2306.05443 n. wang, h. yang, and c. d. wang, “fingpt: instruction tuning benchmark for open-source large language models in financial datasets,” corr , vol. abs/2310.04793, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310. 04793 r. taylor, m. kardas, g. cucurull, t. scialom, a. hartshorn, e. saravia, a. poulton, v . kerkez, and r. stojnic, “galactica: a large language model for science,” corr , vol. abs/2211.09085, 2022. [online]. available: https://doi.org/10.48550/arxiv.2211.09085 j. yin, s. dash, f. wang, and m. shankar, “forge: pre-training open foundation models for science,” inproceedings of the international conference for high performance computing, networking, storage and analysis, sc 2023, denver, co, usa, november 12-17, 2023 , d. arnold, r. m. badia, and k. m. mohror, eds. acm, 2023, pp. 81:1–81:13. [online]. available: https: //doi.org/10.1145/3581784.3613215 z. azerbayev, h. schoelkopf, k. paster, m. d. santos, s. mcaleer, a. q. jiang, j. deng, s. biderman, and s. welleck, “llemma: an open language model for mathematics,” corr , vol. abs/2310.10631, 2023. [online]. available: https://doi.org/10.48550/arxiv.2310.10631 f. yu, a. gao, and b. wang, “outcome-supervised verifiers for planning in mathematical reasoning,” corr , vol. abs/2311.09724, 2023. [online]. available: https://doi.org/10.48550/arxiv.2311.09724 t. d. nguyen, y. ting, i. ciuca, c. o’neill, z. sun, m. jablonska, s. kruk, e. perkowski, j. w. miller, j. li, j. peek, k. iyer, t. r ´ozanski, p . khetarpal, s. zaman, d. brodrick, s. j. r. m ´endez, t. bui, a. goodman, a. accomazzi, j. p . naiman, j. cranney, k. schawinski, and universetbd, “astrollama: towards specialized foundation models in astronomy,” corr , vol. abs/2309.06126, 2023. [online]. available: https: //doi.org/10.48550/arxiv.2309.06126 j. roberts, t. l ¨uddecke, s. das, k. han, and s. albanie, “gpt4geo: how a language model sees the world’s ge- ography,” 2023. z. lin, c. deng, l. zhou, t. zhang, y. xu, y. xu, z. he, y. shi, b. dai, y. song, b. zeng, q. chen, t. shi, t. huang, y. xu, s. wang, l. fu, w. zhang, j. he, c. ma, y. zhu, x. wang, and c. zhou, “geogalactica: a scientific large language model in geoscience,” 2023. 43 c. wang, d. engler, x. li, j. hou, d. j. wald, k. jaiswal, and s. xu, “near-real-time earthquake-induced fatality estimation using crowdsourced data and large-language models,” 2023. l. chen, s. li, j. yan, h. wang, k. gunaratna, v . yadav, z. tang, v . srinivasan, t. zhou, h. huang, and h. jin, “alpagasus: training a better alpaca with fewer data,” 2023. y. cao, y. kang, and l. sun, “instruction mining: high- quality instruction data selection for large language mod- els,” 2023. m. li, y. zhang, z. li, j. chen, l. chen, n. cheng, j. wang, t. zhou, and j. xiao, “from quantity to quality: boosting llm performance with self-guided data selection for instruction tuning,” arxiv , vol. abs/2308.12032, 2023. [online]. available: https://api.semanticscholar. org/corpusid:261076515 q. du, c. zong, and j. zhang, “mods: model-oriented data selection for instruction tuning,” 2023. y. li, b. hui, x. xia, j. yang, m. yang, l. zhang, s. si, j. liu, t. liu, f. huang, and y. li, “one shot learning as instruction data prospector for large language models,” 2023. e. frantar, s. p . singh, and d. alistarh, “optimal brain com- pression: a framework for accurate post-training quanti- zation and pruning,” 2023. t. dettmers, m. lewis, y. belkada, and l. zettlemoyer, “gpt3.int8(): 8-bit matrix multiplication for transformers at scale,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https://openreview.net/ forum?id=dxigwqboxad y. j. kim, r. henry, r. fahim, and h. h. awadalla, “finequant: unlocking efficiency with fine-grained weight-only quantization for llms,” 2023. c. tao, l. hou, w. zhang, l. shang, x. jiang, q. liu, p . luo, and n. wong, “compression of generative pre-trained language models via quantization,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 4821–4836. [online]. available: https://aclanthology.org/2022.acl- long.331 z. yao, r. yazdani aminabadi, m. zhang, x. wu, c. li, and y. he, “zeroquant: efficient and affordable post-training quantization for large-scale transformers,” advances in neural information processing systems , vol. 35, pp. 27 168– 27 183, 2022. g. xiao, j. lin, m. seznec, h. wu, j. demouth, and s. han, “smoothquant: accurate and efficient post-training quan- tization for large language models,” 2023. x. ma, g. fang, and x. wang, “llm-pruner: on the struc- tural pruning of large language models,” 2023. m. zhang, h. chen, c. shen, z. yang, l. ou, x. yu, and b. zhuang, “loraprune: pruning meets low-rank parameter-efficient fine-tuning,” 2023. e. frantar and d. alistarh, “sparsegpt: massive language models can be accurately pruned in one-shot,” 2023. m. xu, y. l. xu, and d. p . mandic, “tensorgpt: efficient compression of the embedding layer in llms based on thetensor-train decomposition,” 2023. y. li, y. yu, q. zhang, c. liang, p . he, w. chen, and t. zhao, “losparse: structured compression of large lan- guage models based on low-rank and sparse approxima- tion,” 2023. z. hu, l. wang, y. lan, w. xu, e.-p . lim, l. bing, x. xu, s. poria, and r. k.-w. lee, “llm-adapters: an adapter family for parameter-efficient fine-tuning of large lan- guage models,” 2023. h. liu, d. tam, m. mohammed, j. mohta, t. huang, m. bansal, and c. raffel, “few-shot parameter- efficient fine-tuning is better and cheaper than in- context learning,” in advances in neural information processing systems , a. h. oh, a. agarwal, d. belgrave, and k. cho, eds., 2022. [online]. available: https: //openreview.net/forum?id=rbcvmg-jspd y. wang, s. agarwal, s. mukherjee, x. liu, j. gao, a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical a. h. awadallah, and j. gao, “adamix: mixture- of-adaptations for parameter-efficient model tuning,” inproceedings of the 2022 conference on empirical methods in natural language processing , y. goldberg, z. kozareva, and y. zhang, eds. abu dhabi, united arab emirates: association for computational linguistics, dec. 2022, pp. 5744–5760. [online]. available: https://aclanthology.org/2022.emnlp-main.388 e. j. hu, y. shen, p . wallis, z. allen-zhu, y. li, s. wang, l. wang, and w. chen, “lora: low-rank adaptation of large language models,” 2021. x. l. li and p . liang, “prefix-tuning: optimizing continuous prompts for generation,” in proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 1: long papers) , c. zong, f. xia, w. li, and r. navigli, eds. online: association for computational linguistics, aug. 2021, pp. 4582–4597. [online]. available: https://aclanthology.org/2021.acl- long.353 x. liu, k. ji, y. fu, w. tam, z. du, z. yang, and j. tang, “p- tuning: prompt tuning can be comparable to fine-tuning across scales and tasks,” in proceedings of the 60th annual meeting of the association for computational linguistics (volume 2: short papers) , s. muresan, p . nakov, and a. villavicencio, eds. dublin, ireland: association for computational linguistics, may 2022, pp. 61–68. [online]. available: https://aclanthology.org/2022.acl-short.8 t. dettmers, a. pagnoni, a. holtzman, and l. zettlemoyer, “qlora: efficient finetuning of quantized llms,” 2023. j. kim, j. h. lee, s. kim, j. park, k. m. yoo, s. j. kwon, and d. lee, “memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization,” 2023. s. malladi, t. gao, e. nichani, a. damian, j. d. lee, d. chen, and s. arora, “fine-tuning language models with just forward passes,” 2024. z. wan, x. wang, c. liu, s. alam, y. zheng, j. liu, z. qu, s. yan, y. zhu, q. zhang, m. chowdhury, and m. zhang, “efficient large language models: a survey,” 2024. y.-s. lee, m. sultan, y. el-kurdi, t. naseem, a. munawar, r. florian, s. roukos, and r. astudillo, “ensemble- instruct: instruction tuning data generation with a heterogeneous mixture of lms,” in findings of the 44 association for computational linguistics: emnlp 2023 , h. bouamor, j. pino, and k. bali, eds. singapore: association for computational linguistics, dec. 2023, pp. 12 561–12 571. [online]. available: https://aclanthology. org/2023.findings-emnlp.836 w. chen, y. zhou, n. du, y. huang, j. laudon, z. chen, and c. cui, “lifelong language pretraining with distribution- specialized experts,” in international conference on machine learning . pmlr, 2023, pp. 5383–5395. s. kotha, j. m. springer, and a. raghunathan, “under- standing catastrophic forgetting in language models via implicit inference,” arxiv preprint arxiv:2309.10105 , 2023. b. koloski, b. ˇskrlj, m. robnik- ˇsikonja, and s. pollak, “mea- suring catastrophic forgetting in cross-lingual transfer paradigms: exploring tuning strategies,” arxiv preprint arxiv:2309.06089 , 2023. t. wu, l. luo, y.-f. li, s. pan, t.-t. vu, and g. haffari, “continual learning for large language models: a sur- vey,” arxiv preprint arxiv:2402.01364 , 2024. y. luo, z. yang, f. meng, y. li, j. zhou, and y. zhang, “an empirical study of catastrophic forgetting in large language models during continual fine-tuning,” arxiv preprint arxiv:2308.08747 , 2023. j. kirkpatrick, r. pascanu, n. rabinowitz, j. veness, g. des- jardins, a. a. rusu, k. milan, j. quan, t. ramalho, a. grabska-barwinska et al. , “overcoming catastrophic forgetting in neural networks,” proceedings of the national academy of sciences , vol. 114, no. 13, pp. 3521–3526, 2017. m. rostami, s. kolouri, and p . k. pilly, “complementary learning for overcoming catastrophic forgetting using ex- perience replay,” arxiv preprint arxiv:1903.04566 , 2019. d. rolnick, a. ahuja, j. schwarz, t. lillicrap, and g. wayne, “experience replay for continual learning,” advances in neural information processing systems , vol. 32, 2019. s.-w. lee, j.-h. kim, j. jun, j.-w. ha, and b.-t. zhang, “overcoming catastrophic forgetting by incremental mo- ment matching,” advances in neural information processing systems , vol. 30, 2017. a. mallya, d. davis, and s. lazebnik, “piggyback: adapting a single network to multiple tasks by learning to mask weights,” in proceedings of the european conference on com- puter vision (eccv) , 2018, pp. 67–82. z. wang, z. zhang, c.-y. lee, h. zhang, r. sun, x. ren, g. su, v . perot, j. dy, and t. pfister, “learning to prompt for continual learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2022, pp. 139–149. z. hu, y. li, j. lyu, d. gao, and n. vasconcelos, “dense network expansion for class incremental learning,” in proceedings of the ieee/cvf conference on computer vision and pattern recognition , 2023, pp. 11 858–11 867. x. li, l. lin, s. wang, and c. qian, “unlock the power: competitive distillation for multi-modal large language models,” arxiv preprint arxiv:2311.08213 , 2023. m. zeng, w. xue, q. liu, and y. guo, “continual learning with dirichlet generative-based rehearsal,” arxiv preprint arxiv:2309.06917 , 2023. z. zhang, m. fang, l. chen, and m.-r. namazi-rad, “citb: a benchmark for continual instruction tuning,” arxiv preprint arxiv:2310.14510 , 2023. c. burns, p . izmailov, j. h. kirchner, b. baker, l. gao,l. aschenbrenner, y. chen, a. ecoffet, m. joglekar, j. leike, i. sutskever, and j. wu, “weak-to-strong generalization: eliciting strong capabilities with weak supervision,” corr , vol. abs/2312.09390, 2023. [online]. available: https://doi.org/10.48550/arxiv.2312.09390 m. li, y. zhang, s. he, z. li, h. zhao, j. wang, n. cheng, and t. zhou, “superfiltering: weak-to- strong data filtering for fast instruction-tuning,” corr , vol. abs/2402.00530, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.00530 j. ji, b. chen, h. lou, d. hong, b. zhang, x. pan, j. dai, and y. yang, “aligner: achieving efficient alignment through weak-to-strong correction,” corr , vol. abs/2402.02416, 2024. [online]. available: https: //doi.org/10.48550/arxiv.2402.02416'}]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are an AI assistant skilled at translating complex AI research into clear, exciting summaries for business leaders who are curious about AI but may not have deep technical knowledge. Your summaries are known for their ability to convey the business potential of cutting-edge AI research in simple, relatable terms.\"}, \n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Create an engaging, business-friendly summary of the following academic paper on AI technology. Your summary should:\n",
    "\n",
    "Start with a brief, exciting explanation (2-3 sentences) of the paper's core concept and why it matters to businesses. Use simple language and avoid technical jargon.\n",
    "Present 2-3 key takeaways as short paragraphs. For each:\n",
    "\n",
    "Explain the idea in simple terms, as if you're talking to someone with no AI background.\n",
    "Provide a concrete, relatable example of how this could impact or improve a common business operation.\n",
    "Briefly mention how this overcomes an existing business challenge.\n",
    "\n",
    "\n",
    "Include one short, impactful quote from the paper. Explain its significance for businesses in plain language.\n",
    "Throughout the summary, subtly connect these ideas to current business trends or concerns.\n",
    "Conclude with 1-2 sentences that excite the reader about the near-future potential of this technology for businesses.\n",
    "\n",
    "Aim for a tone that's enthusiastic and accessible. Focus on the practical business applications and benefits, avoiding deep technical explanations. Your goal is to leave business leaders thinking, \"This could really help my company - I want to learn more!\"\n",
    "Keep the entire summary under 300 words to maintain engagement.\n",
    "Now, here's the full paper:\n",
    "{processed_content}\n",
    "\"\"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_2 = [{\"role\": \"system\", \"content\": \"You are an AI assistant skilled at distilling complex AI research papers into compelling, holistic summaries for data scientists, software engineers, and machine learning engineers. Your summaries are known for capturing the essence of groundbreaking research and sparking curiosity in technical minds.\"}, \n",
    "            {\"role\": \"user\", \"content\": f\"\"\"Create an engaging, technical summary of the following AI research paper tailored for ML engineers and AI professionals. Your summary should:\n",
    "\n",
    "Opening:\n",
    "\n",
    "Begin with a powerful statement that captures the paper's most innovative or impactful idea.\n",
    "Follow with a brief explanation of the paper's main contribution to the field of AI/ML and why it matters.\n",
    "Use language that would intrigue technical practitioners and immediately convey the significance of the research.\n",
    "\n",
    "\n",
    "Key Takeaways:\n",
    "Present 2-3 key takeaways. For each:\n",
    "\n",
    "Explain the main idea in clear terms, avoiding excessive technical jargon.\n",
    "Mention its potential impact on current ML practices or future research.\n",
    "Ensure clear linkages between concepts, showing how ideas connect or build upon each other.\n",
    "Include enough detail to convey the importance and novelty of each takeaway, while maintaining overall brevity.\n",
    "\n",
    "Impactful Quote:\n",
    "Include one short, impactful quote from the paper. This should be the only instance of using the paper's exact words. Explain its significance succinctly, focusing on its potential implications for AI/ML development.\n",
    "Conclusion:\n",
    "Highlight the potential future impact of this research and why it's exciting for the field, drawing on the ideas presented in the takeaways.\n",
    "\n",
    "Throughout the summary:\n",
    "\n",
    "Use your own words to convey the paper's ideas. Do not copy exact sentences or substantial phrases from the paper, except for the single quote in section 3.\n",
    "Aim for a tone that balances technical insight with accessibility and enthusiasm.\n",
    "Focus on sparking curiosity and conveying the potential importance of the research.\n",
    "Ensure clear connections between different concepts and ideas presented.\n",
    "                         \n",
    "Aim for a tone that balances technical precision with enthusiasm for the research's potential. Focus on aspects that would most interest ML practitioners and researchers, always ensuring clear explanations of how concepts interact or build upon each other. Your goal is to leave technical readers thinking, \"\"This approach could significantly advance our current methods  - I need to explore the full paper.\"\n",
    "Keep the entire summary under 300 words to maintain engagement, remembering that detailed section-by-section summaries will follow.\n",
    "Now, here's the full paper:\n",
    "{processed_content}\n",
    "\"\"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Unlocking the Power of AI: How Knowledge Distillation Can Transform Your Business**\n",
      "\n",
      "Imagine having the advanced capabilities of cutting-edge AI models like GPT-4 at your fingertips, but without the hefty price tag or the need for extensive computational resources. This is the promise of Knowledge Distillation (KD), a process that transfers the intelligence of these powerful models into more accessible, cost-effective versions. For businesses, this means harnessing top-tier AI performance to drive innovation, efficiency, and competitive advantage.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "1. **Enhanced AI Capabilities at Lower Costs:**\n",
      "   KD allows smaller, open-source models to learn from the best proprietary models. Think of it as a master-apprentice relationship where the apprentice (smaller model) learns the tricks of the trade from the master (larger model). For instance, a small retail business could use a distilled model to implement sophisticated customer service chatbots that provide personalized, accurate responses, similar to those from high-end AI systems, but at a fraction of the cost. This overcomes the challenge of high operational costs associated with proprietary AI models.\n",
      "\n",
      "2. **Tailored Solutions for Specific Needs:**\n",
      "   KD can be customized to enhance specific skills or domains. For example, a healthcare provider could use KD to train models specifically for medical diagnosis and patient interaction, ensuring the AI understands and processes medical terminology accurately. This tailored approach addresses the challenge of generic AI models that may not meet niche industry requirements, providing more precise and relevant AI applications.\n",
      "\n",
      "3. **Improved Efficiency and Sustainability:**\n",
      "   By compressing large models into smaller, more efficient ones without significant performance loss, KD not only reduces computational demands but also enhances environmental sustainability. A logistics company, for example, could deploy these efficient models to optimize route planning and supply chain management, reducing fuel consumption and carbon footprint. This approach tackles the dual challenges of high energy consumption and the need for sustainable business practices.\n",
      "\n",
      "**Quote from the Paper:**\n",
      "\"Knowledge distillation fosters a more accessible and equitable AI landscape, where smaller entities and individual researchers gain access to state-of-the-art capabilities, encouraging wider participation and diversity in AI advancements.\"\n",
      "\n",
      "**Significance for Businesses:**\n",
      "This quote underscores the democratizing power of KD, making advanced AI accessible to businesses of all sizes. It highlights the potential for broader innovation and participation, driving diverse applications across industries.\n",
      "\n",
      "**Exciting Future Potential:**\n",
      "As KD techniques continue to evolve, businesses can look forward to even more powerful, efficient, and specialized AI solutions. This technology not only levels the playing field but also paves the way for groundbreaking advancements in various sectors, from healthcare to finance to customer service.\n",
      "\n",
      "In summary, Knowledge Distillation is a game-changer, offering businesses the opportunity to leverage top-tier AI capabilities in a cost-effective, efficient, and sustainable manner. This could be the key to unlocking new levels of innovation and competitiveness in your industry.\n"
     ]
    }
   ],
   "source": [
    "test_b_summary = generate_completion(client_val, \"gpt-4o\", messages)\n",
    "print(test_b_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Opening:**\n",
      "\n",
      "In the rapidly evolving landscape of AI, knowledge distillation (KD) has emerged as a pivotal technique for transferring the advanced capabilities of proprietary large language models (LLMs) like GPT-4 to more accessible, open-source counterparts such as LLaMA and Mistral. This paper presents a comprehensive survey of KD's role in enhancing the performance of LLMs, focusing on its applications in model compression, self-improvement, and skill enhancement. By leveraging data augmentation (DA) to generate context-rich training data, KD transcends traditional boundaries, enabling open-source models to approximate the nuanced understanding and ethical alignment of their proprietary counterparts.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "1. **Algorithmic Foundations of KD:**\n",
      "   - **Main Idea:** The paper categorizes KD mechanisms into algorithmic strategies, skill enhancement, and verticalization, providing a structured overview of how KD can be systematically applied to LLMs.\n",
      "   - **Impact:** This categorization helps practitioners understand the diverse methodologies available for KD, enabling more targeted and effective application in various AI projects. It also sets a foundation for future research to build upon these structured approaches.\n",
      "\n",
      "2. **Data Augmentation Synergy:**\n",
      "   - **Main Idea:** The integration of DA within the KD framework is highlighted as a powerful paradigm. By generating skill-specific training data, DA enhances the performance of distilled models.\n",
      "   - **Impact:** This approach significantly improves the quality and diversity of training data, leading to more robust and versatile LLMs. It also opens new avenues for creating high-quality datasets that can be used to train smaller models more effectively.\n",
      "\n",
      "3. **Self-Improvement through KD:**\n",
      "   - **Main Idea:** The paper discusses the emerging trend of using open-source LLMs as teachers for their own self-improvement, employing techniques like self-instruction and self-rewarding.\n",
      "   - **Impact:** This self-improvement strategy not only enhances the capabilities of open-source models but also reduces dependency on proprietary models, fostering a more equitable AI ecosystem. It demonstrates the potential for models to iteratively refine their performance autonomously.\n",
      "\n",
      "**Impactful Quote:**\n",
      "\n",
      "\"By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts.\"\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The future impact of this research is profound, as it paves the way for more accessible, efficient, and powerful AI solutions. By bridging the gap between proprietary and open-source LLMs, KD fosters a more inclusive AI research environment. The integration of DA and self-improvement techniques within the KD framework not only enhances model performance but also democratizes access to advanced AI capabilities. This research is exciting for the field as it highlights the potential for continuous innovation and improvement in AI, driven by collaborative and open-source efforts.\n"
     ]
    }
   ],
   "source": [
    "test_b1_summary = generate_completion(client_val, \"gpt-4o\", messages_2)\n",
    "print(test_b1_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_paper_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
